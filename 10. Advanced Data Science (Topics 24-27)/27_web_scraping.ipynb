{
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# Topic 30: Web Scraping for Data Collection\n",
"\n",
"## Overview\n",
"Web scraping is the process of automatically extracting data from websites[7][23]. Python offers powerful libraries like Requests and Beautiful Soup that make web scraping accessible and efficient for data collection.\n",
"\n",
"### What You'll Learn:\n",
"- Web scraping fundamentals and ethics\n",
"- HTTP requests with the Requests library\n",
"- HTML parsing with Beautiful Soup\n",
"- Handling different data formats (JSON, XML)\n",
"- Managing sessions, cookies, and authentication\n",
"- Best practices and common challenges\n",
"\n",
"---"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 1. Web Scraping Fundamentals\n",
"\n",
"Understanding the basics of web scraping and HTTP:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Web Scraping Fundamentals\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport json\nimport time\nimport re\nfrom urllib.parse import urljoin, urlparse\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Web Scraping Fundamentals:\")\nprint(\"=\" * 26)\n\nprint(\"1. What is Web Scraping?\")\nweb_scraping_steps = [\n    \"1. Send HTTP request to target website\",\n    \"2. Download HTML content from server\", \n    \"3. Parse HTML to extract desired information\",\n    \"4. Clean and structure the extracted data\",\n    \"5. Store data in desired format (CSV, JSON, database)\"\n]\n\nfor step in web_scraping_steps:\n    print(f\"   {step}\")\n\nprint(f\"\\n2. Web Scraping Ethics and Legality:\")\nethical_guidelines = {\n    \"âœ“ Check robots.txt\": \"Always check website's robots.txt file\",\n    \"âœ“ Respect rate limits\": \"Don't overload servers with requests\", \n    \"âœ“ Read Terms of Service\": \"Understand website's usage policies\",\n    \"âœ“ Public data only\": \"Only scrape publicly available information\",\n    \"âœ“ Give attribution\": \"Credit data sources appropriately\",\n    \"âš ï¸  User-Agent headers\": \"Identify your scraper appropriately\",\n    \"âš ï¸  Handle errors gracefully\": \"Don't break on failed requests\"\n}\n\nfor guideline, description in ethical_guidelines.items():\n    print(f\"   {guideline}: {description}\")\n\nprint(f\"\\n3. HTTP Basics for Web Scraping:\")\nhttp_concepts = {\n    \"GET Request\": \"Retrieve data from server\",\n    \"POST Request\": \"Send data to server\", \n    \"Headers\": \"Metadata about the request/response\",\n    \"Status Codes\": \"200 (OK), 404 (Not Found), 403 (Forbidden), 500 (Server Error)\",\n    \"Cookies\": \"Store session information\",\n    \"User-Agent\": \"Identifies the client making the request\"\n}\n\nfor concept, description in http_concepts.items():\n    print(f\"   {concept}: {description}\")\n\n# Basic HTTP request example\nprint(f\"\\n4. Making basic HTTP requests:\")\n\n# Example: Get a simple webpage (using httpbin for testing)\ntest_url = \"https://httpbin.org/html\"\n\ntry:\n    response = requests.get(test_url)\n    print(f\"   Request to {test_url}:\")\n    print(f\"   Status Code: {response.status_code}\")\n    print(f\"   Content Type: {response.headers.get('content-type')}\")\n    print(f\"   Response Length: {len(response.text)} characters\")\n    print(f\"   First 200 characters: {response.text[:200]}...\")\nexcept requests.RequestException as e:\n    print(f\"   Error making request: {e}\")\n\n# Headers examination\nprint(f\"\\n5. HTTP Headers:\")\nprint(f\"   Request Headers (what we send):\")\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en-US,en;q=0.5',\n    'Accept-Encoding': 'gzip, deflate',\n    'Connection': 'keep-alive',\n}\n\nfor header, value in headers.items():\n    print(f\"     {header}: {value[:50]}{'...' if len(value) > 50 else ''}\")\n\ntry:\n    response_with_headers = requests.get(test_url, headers=headers)\n    print(f\"\\n   Response Headers (what we receive):\")\n    for header, value in list(response_with_headers.headers.items())[:5]:\n        print(f\"     {header}: {value}\")\nexcept:\n    print(f\"   Could not fetch headers\")\n\nprint(f\"\\n6. Common Web Scraping Libraries:\")\nlibraries = {\n    \"requests\": \"HTTP library for making web requests\",\n    \"urllib\": \"Built-in Python URL handling\",\n    \"BeautifulSoup\": \"HTML/XML parsing and navigation\", \n    \"lxml\": \"Fast XML/HTML parser\",\n    \"selenium\": \"Browser automation for JavaScript-heavy sites\",\n    \"scrapy\": \"Framework for large-scale web scraping\",\n    \"pandas\": \"Data manipulation and CSV/JSON handling\"\n}\n\nfor library, description in libraries.items():\n    print(f\"   {library}: {description}\")\n\nprint(f\"\\n7. Web Scraping Challenges:\")\nchallenges = [\n    \"JavaScript-rendered content\",\n    \"Anti-bot measures (CAPTCHAs, rate limiting)\", \n    \"Dynamic content and AJAX calls\",\n    \"Session management and authentication\",\n    \"Handling different encodings\",\n    \"Dealing with malformed HTML\",\n    \"Respecting robots.txt and rate limits\",\n    \"Legal and ethical considerations\"\n]\n\nfor i, challenge in enumerate(challenges, 1):\n    print(f\"   {i}. {challenge}\")\n\nprint(f\"\\n8. When NOT to use web scraping:\")\nalternatives = [\n    \"âœ“ Official APIs are available\",\n    \"âœ“ Data is available in structured formats (CSV, JSON)\", \n    \"âœ“ Website prohibits scraping in Terms of Service\",\n    \"âœ“ Real-time data feeds exist\",\n    \"âœ“ Partner or commercial data sources available\"\n]\n\nfor alternative in alternatives:\n    print(f\"   {alternative}\")\n\nprint(f\"\\n   Always prefer official APIs when available!\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 2. HTML Parsing with Beautiful Soup\n",
"\n",
"Extracting data from HTML using Beautiful Soup:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# HTML Parsing with Beautiful Soup\nprint(\"HTML Parsing with Beautiful Soup:\")\nprint(\"=\" * 33)\n\n# Sample HTML for demonstration\nsample_html = \"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Sample E-commerce Page</title>\n    <meta charset=\"UTF-8\">\n</head>\n<body>\n    <header>\n        <h1>Online Store</h1>\n        <nav>\n            <ul>\n                <li><a href=\"/home\">Home</a></li>\n                <li><a href=\"/products\">Products</a></li>\n                <li><a href=\"/about\">About</a></li>\n            </ul>\n        </nav>\n    </header>\n    \n    <main>\n        <div class=\"products\">\n            <div class=\"product\" data-id=\"1\">\n                <h2 class=\"product-title\">Laptop Computer</h2>\n                <p class=\"price\">$999.99</p>\n                <p class=\"description\">High-performance laptop for professionals</p>\n                <span class=\"category\">Electronics</span>\n                <div class=\"rating\">\n                    <span class=\"stars\">â˜…â˜…â˜…â˜…â˜†</span>\n                    <span class=\"review-count\">(245 reviews)</span>\n                </div>\n                <img src=\"laptop.jpg\" alt=\"Laptop Computer\">\n            </div>\n            \n            <div class=\"product\" data-id=\"2\">\n                <h2 class=\"product-title\">Wireless Headphones</h2>\n                <p class=\"price\">$149.99</p>\n                <p class=\"description\">Premium noise-cancelling headphones</p>\n                <span class=\"category\">Electronics</span>\n                <div class=\"rating\">\n                    <span class=\"stars\">â˜…â˜…â˜…â˜…â˜…</span>\n                    <span class=\"review-count\">(89 reviews)</span>\n                </div>\n                <img src=\"headphones.jpg\" alt=\"Wireless Headphones\">\n            </div>\n            \n            <div class=\"product\" data-id=\"3\">\n                <h2 class=\"product-title\">Coffee Maker</h2>\n                <p class=\"price\">$79.99</p>\n                <p class=\"description\">Automatic drip coffee maker</p>\n                <span class=\"category\">Appliances</span>\n                <div class=\"rating\">\n                    <span class=\"stars\">â˜…â˜…â˜…â˜†â˜†</span>\n                    <span class=\"review-count\">(156 reviews)</span>\n                </div>\n                <img src=\"coffee-maker.jpg\" alt=\"Coffee Maker\">\n            </div>\n        </div>\n        \n        <aside>\n            <div class=\"contact-info\">\n                <h3>Contact Us</h3>\n                <p>Email: info@store.com</p>\n                <p>Phone: (555) 123-4567</p>\n                <address>\n                    123 Main Street<br>\n                    City, State 12345\n                </address>\n            </div>\n        </aside>\n    </main>\n    \n    <footer>\n        <p>&copy; 2024 Online Store. All rights reserved.</p>\n    </footer>\n</body>\n</html>\n\"\"\"\n\nprint(\"1. Creating BeautifulSoup object:\")\n\n# Parse HTML with Beautiful Soup\nsoup = BeautifulSoup(sample_html, 'html.parser')\nprint(f\"   Parsed HTML document\")\nprint(f\"   Title: {soup.title.string}\")\nprint(f\"   Document type: {type(soup)}\")\n\nprint(f\"\\n2. Basic element selection:\")\n\n# Find elements by tag\nprint(f\"   Finding by tag:\")\nprint(f\"   First h1: {soup.find('h1').string}\")\nprint(f\"   All h2 tags: {[h2.string for h2 in soup.find_all('h2')]}\")\n\n# Find by class\nprint(f\"\\n   Finding by class:\")\nprices = soup.find_all(class_='price')\nprint(f\"   All prices: {[price.string for price in prices]}\")\n\n# Find by ID (if existed)\nproduct_by_id = soup.find('div', {'data-id': '1'})\nprint(f\"   Product with data-id='1': {product_by_id.find(class_='product-title').string}\")\n\nprint(f\"\\n3. CSS Selectors:\")\n\n# CSS selector examples\nprint(f\"   Using CSS selectors:\")\n\n# Select by class\nproduct_titles = soup.select('.product-title')\nprint(f\"   Product titles: {[title.string for title in product_titles]}\")\n\n# Select by descendant\ncategories = soup.select('.product .category')\nprint(f\"   Categories: {[cat.string for cat in categories]}\")\n\n# Select by attribute\nelectronics = soup.select('.product:has(.category:contains(\"Electronics\"))')\nprint(f\"   Electronics products found: {len(electronics)} (using CSS selectors)\")\n\n# More practical approach for filtering\nelectronics_products = []\nfor product in soup.find_all('div', class_='product'):\n    category = product.find(class_='category')\n    if category and 'Electronics' in category.string:\n        title = product.find(class_='product-title').string\n        electronics_products.append(title)\n\nprint(f\"   Electronics products: {electronics_products}\")\n\nprint(f\"\\n4. Extracting structured data:\")\n\n# Extract all products into a structured format\nproducts_data = []\n\nfor product in soup.find_all('div', class_='product'):\n    # Extract basic information\n    title = product.find(class_='product-title').string\n    price_text = product.find(class_='price').string\n    price = float(price_text.replace('$', '').replace(',', ''))\n    description = product.find(class_='description').string\n    category = product.find(class_='category').string\n    \n    # Extract rating information\n    stars_element = product.find(class_='stars')\n    stars_count = len([s for s in stars_element.string if s == 'â˜…']) if stars_element else 0\n    \n    review_count_element = product.find(class_='review-count')\n    review_count_text = review_count_element.string if review_count_element else '(0 reviews)'\n    review_count = int(re.search(r'\\((\\d+)', review_count_text).group(1))\n    \n    # Extract image info\n    img = product.find('img')\n    image_src = img['src'] if img else None\n    image_alt = img['alt'] if img else None\n    \n    # Get product ID\n    product_id = product.get('data-id')\n    \n    products_data.append({\n        'id': product_id,\n        'title': title,\n        'price': price,\n        'description': description,\n        'category': category,\n        'stars': stars_count,\n        'review_count': review_count,\n        'image_src': image_src,\n        'image_alt': image_alt\n    })\n\n# Display extracted data\nprint(f\"   Extracted {len(products_data)} products:\")\nfor product in products_data:\n    print(f\"   - {product['title']}: ${product['price']} ({product['stars']} stars, {product['review_count']} reviews)\")\n\n# Convert to DataFrame\nproducts_df = pd.DataFrame(products_data)\nprint(f\"\\n   DataFrame shape: {products_df.shape}\")\nprint(f\"{products_df}\")\n\nprint(f\"\\n5. Advanced parsing techniques:\")\n\n# Handle missing elements gracefully\nprint(f\"   Handling missing elements:\")\n\ndef safe_extract(element, selector, attribute=None):\n    \"\"\"Safely extract data from HTML elements\"\"\"\n    try:\n        found = element.select_one(selector)\n        if found:\n            if attribute:\n                return found.get(attribute)\n            else:\n                return found.get_text(strip=True)\n        return None\n    except:\n        return None\n\n# Example of safe extraction\nfor product in soup.find_all('div', class_='product')[:1]:  # Just first product\n    safe_title = safe_extract(product, '.product-title')\n    safe_price = safe_extract(product, '.price')\n    safe_missing = safe_extract(product, '.nonexistent-class')\n    \n    print(f\"   Safe extraction example:\")\n    print(f\"     Title: {safe_title}\")\n    print(f\"     Price: {safe_price}\")\n    print(f\"     Missing element: {safe_missing}\")\n\n# Extract all links\nprint(f\"\\n6. Extracting links and navigation:\")\n\nlinks = soup.find_all('a')\nprint(f\"   Found {len(links)} links:\")\nfor link in links:\n    href = link.get('href')\n    text = link.get_text(strip=True)\n    print(f\"     {text}: {href}\")\n\n# Extract contact information\nprint(f\"\\n7. Extracting contact information:\")\ncontact_section = soup.find('div', class_='contact-info')\nif contact_section:\n    email = contact_section.find('p').get_text(strip=True)\n    phone = contact_section.find_all('p')[1].get_text(strip=True)\n    address = contact_section.find('address').get_text(strip=True)\n    \n    contact_info = {\n        'email': email,\n        'phone': phone, \n        'address': address.replace('\\n', ' ').strip()\n    }\n    \n    print(f\"   Contact Information:\")\n    for key, value in contact_info.items():\n        print(f\"     {key.title()}: {value}\")\n\nprint(f\"\\n8. Saving extracted data:\")\n\n# Save to CSV\nproducts_df.to_csv('scraped_products.csv', index=False)\nprint(f\"   âœ“ Saved products to 'scraped_products.csv'\")\n\n# Save to JSON\nwith open('scraped_products.json', 'w') as f:\n    json.dump(products_data, f, indent=2)\nprint(f\"   âœ“ Saved products to 'scraped_products.json'\")\n\nprint(f\"\\n9. Beautiful Soup best practices:\")\nprint(f\"   âœ“ Always specify a parser ('html.parser', 'lxml', 'html5lib')\")\nprint(f\"   âœ“ Handle missing elements gracefully with try/except\")\nprint(f\"   âœ“ Use CSS selectors for complex selections\")\nprint(f\"   âœ“ Extract data into structured formats (dicts, DataFrames)\")\nprint(f\"   âœ“ Clean and validate extracted data\")\nprint(f\"   âœ“ Use get_text(strip=True) to clean whitespace\")\nprint(f\"   âœ“ Check for None values before accessing attributes\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 3. Real-World Scraping Examples\n",
"\n",
"Practical web scraping scenarios and techniques:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Real-World Scraping Examples\nprint(\"Real-World Web Scraping Examples:\")\nprint(\"=\" * 34)\n\nprint(\"1. Scraping public APIs and JSON data:\")\n\n# Example: Using a public API that returns JSON\n# JSONPlaceholder - a free fake API for testing\napi_url = \"https://jsonplaceholder.typicode.com/posts\"\n\ntry:\n    api_response = requests.get(api_url)\n    \n    if api_response.status_code == 200:\n        posts_data = api_response.json()  # Parse JSON directly\n        print(f\"   Successfully fetched {len(posts_data)} posts from API\")\n        print(f\"   First post: {posts_data[0]['title']}\")\n        \n        # Convert to DataFrame\n        posts_df = pd.DataFrame(posts_data)\n        print(f\"   DataFrame shape: {posts_df.shape}\")\n        print(f\"   Columns: {list(posts_df.columns)}\")\n        \n        # Show sample data\n        print(f\"\\n   Sample posts:\")\n        for post in posts_data[:3]:\n            print(f\"     ID {post['id']}: {post['title'][:50]}...\")\n    else:\n        print(f\"   API request failed with status: {api_response.status_code}\")\n        \nexcept requests.RequestException as e:\n    print(f\"   Error accessing API: {e}\")\n\nprint(f\"\\n2. Handling different content types:\")\n\n# Example of handling different response types\ntest_endpoints = {\n    \"JSON\": \"https://jsonplaceholder.typicode.com/users/1\",\n    \"XML\": \"https://httpbin.org/xml\",\n    \"HTML\": \"https://httpbin.org/html\"\n}\n\nfor content_type, url in test_endpoints.items():\n    try:\n        response = requests.get(url, timeout=10)\n        content_type_header = response.headers.get('content-type', 'unknown')\n        \n        print(f\"   {content_type} Content:\")\n        print(f\"     Status: {response.status_code}\")\n        print(f\"     Content-Type: {content_type_header}\")\n        print(f\"     Length: {len(response.text)} characters\")\n        \n        if content_type == \"JSON\" and response.status_code == 200:\n            try:\n                json_data = response.json()\n                print(f\"     Parsed JSON keys: {list(json_data.keys()) if isinstance(json_data, dict) else 'List data'}\")\n            except json.JSONDecodeError:\n                print(f\"     Failed to parse JSON\")\n                \n    except requests.RequestException as e:\n        print(f\"   Error with {content_type}: {e}\")\n\nprint(f\"\\n3. Session management and cookies:\")\n\n# Using sessions for persistent connections\nsession = requests.Session()\n\n# Set default headers for the session\nsession.headers.update({\n    'User-Agent': 'Python Web Scraper 1.0',\n    'Accept': 'text/html,application/json'\n})\n\nprint(f\"   Created session with default headers\")\nprint(f\"   Session headers: {dict(session.headers)}\")\n\n# Example of making multiple requests with same session\ntry:\n    # First request\n    response1 = session.get(\"https://httpbin.org/cookies/set/session_id/12345\")\n    print(f\"   First request status: {response1.status_code}\")\n    \n    # Second request - cookies should be maintained\n    response2 = session.get(\"https://httpbin.org/cookies\")\n    if response2.status_code == 200:\n        cookies_data = response2.json()\n        print(f\"   Cookies maintained: {cookies_data.get('cookies', {})}\")\n        \nexcept requests.RequestException as e:\n    print(f\"   Session example error: {e}\")\n\nprint(f\"\\n4. Handling forms and POST requests:\")\n\n# Example of submitting form data\nform_data = {\n    'name': 'John Doe',\n    'email': 'john@example.com',\n    'message': 'Hello from Python web scraper!'\n}\n\ntry:\n    # POST request with form data\n    post_response = requests.post(\"https://httpbin.org/post\", data=form_data)\n    \n    if post_response.status_code == 200:\n        response_data = post_response.json()\n        submitted_form = response_data.get('form', {})\n        print(f\"   Form submission successful\")\n        print(f\"   Submitted data: {submitted_form}\")\n        \nexcept requests.RequestException as e:\n    print(f\"   Form submission error: {e}\")\n\nprint(f\"\\n5. Rate limiting and respectful scraping:\")\n\nclass RespectfulScraper:\n    \"\"\"A scraper that implements rate limiting and retry logic\"\"\"\n    \n    def __init__(self, delay=1.0, max_retries=3):\n        self.delay = delay\n        self.max_retries = max_retries\n        self.session = requests.Session()\n        self.session.headers.update({\n            'User-Agent': 'Educational Python Scraper 1.0'\n        })\n        \n    def get_page(self, url):\n        \"\"\"Get a page with rate limiting and retry logic\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                # Rate limiting\n                time.sleep(self.delay)\n                \n                response = self.session.get(url, timeout=10)\n                \n                if response.status_code == 200:\n                    return response\n                elif response.status_code == 429:  # Too Many Requests\n                    print(f\"     Rate limited, waiting longer...\")\n                    time.sleep(self.delay * 3)\n                    continue\n                else:\n                    print(f\"     HTTP {response.status_code} on attempt {attempt + 1}\")\n                    \n            except requests.RequestException as e:\n                print(f\"     Request failed on attempt {attempt + 1}: {e}\")\n                if attempt < self.max_retries - 1:\n                    time.sleep(self.delay * 2)\n                    \n        return None\n    \n    def scrape_multiple_pages(self, urls):\n        \"\"\"Scrape multiple pages respectfully\"\"\"\n        results = []\n        \n        for i, url in enumerate(urls):\n            print(f\"   Scraping page {i+1}/{len(urls)}: {url}\")\n            \n            response = self.get_page(url)\n            if response:\n                results.append({\n                    'url': url,\n                    'status_code': response.status_code,\n                    'content_length': len(response.text),\n                    'title': self.extract_title(response.text)\n                })\n            else:\n                results.append({\n                    'url': url,\n                    'status_code': None,\n                    'content_length': 0,\n                    'title': 'Failed to fetch'\n                })\n                \n        return results\n    \n    def extract_title(self, html_content):\n        \"\"\"Extract page title from HTML\"\"\"\n        try:\n            soup = BeautifulSoup(html_content, 'html.parser')\n            title_tag = soup.find('title')\n            return title_tag.string.strip() if title_tag else 'No title found'\n        except:\n            return 'Failed to parse title'\n\n# Demonstrate respectful scraping\nscraper = RespectfulScraper(delay=0.5, max_retries=2)\n\n# Test URLs\ntest_urls = [\n    \"https://httpbin.org/html\",\n    \"https://httpbin.org/delay/1\",\n    \"https://httpbin.org/status/200\"\n]\n\nprint(f\"   Testing respectful scraper:\")\nscraping_results = scraper.scrape_multiple_pages(test_urls[:2])  # Limit for demo\n\nfor result in scraping_results:\n    print(f\"     {result['url']}: {result['status_code']} - {result['title']}\")\n\nprint(f\"\\n6. Error handling and robustness:\")\n\ndef robust_scrape(url, max_attempts=3):\n    \"\"\"Demonstrate robust error handling\"\"\"\n    \n    for attempt in range(max_attempts):\n        try:\n            response = requests.get(url, timeout=5)\n            \n            # Check status code\n            if response.status_code != 200:\n                raise requests.HTTPError(f\"HTTP {response.status_code}\")\n            \n            # Check content type\n            content_type = response.headers.get('content-type', '')\n            if 'text/html' not in content_type and 'application/json' not in content_type:\n                raise ValueError(f\"Unexpected content type: {content_type}\")\n            \n            # Try to parse\n            soup = BeautifulSoup(response.text, 'html.parser')\n            \n            return {\n                'success': True,\n                'title': soup.title.string if soup.title else 'No title',\n                'text_length': len(response.text),\n                'attempt': attempt + 1\n            }\n            \n        except requests.Timeout:\n            print(f\"     Attempt {attempt + 1}: Timeout\")\n        except requests.ConnectionError:\n            print(f\"     Attempt {attempt + 1}: Connection error\")\n        except requests.HTTPError as e:\n            print(f\"     Attempt {attempt + 1}: HTTP error - {e}\")\n        except ValueError as e:\n            print(f\"     Attempt {attempt + 1}: Content error - {e}\")\n        except Exception as e:\n            print(f\"     Attempt {attempt + 1}: Unexpected error - {e}\")\n        \n        if attempt < max_attempts - 1:\n            time.sleep(1)\n    \n    return {\n        'success': False,\n        'error': 'All attempts failed',\n        'attempts': max_attempts\n    }\n\n# Test robust scraping\nprint(f\"   Testing robust error handling:\")\ntest_result = robust_scrape(\"https://httpbin.org/html\")\nprint(f\"   Result: {test_result}\")\n\nprint(f\"\\n7. Data export and storage:\")\n\n# Create sample scraped data\nscraped_data = {\n    'pages_scraped': len(scraping_results),\n    'timestamp': pd.Timestamp.now().isoformat(),\n    'results': scraping_results\n}\n\n# Save in multiple formats\nprint(f\"   Saving scraped data in multiple formats:\")\n\n# JSON export\nwith open('scraping_results.json', 'w') as f:\n    json.dump(scraped_data, f, indent=2, default=str)\nprint(f\"     âœ“ Saved to scraping_results.json\")\n\n# CSV export (flattened data)\nif scraping_results:\n    results_df = pd.DataFrame(scraping_results)\n    results_df.to_csv('scraping_results.csv', index=False)\n    print(f\"     âœ“ Saved to scraping_results.csv\")\n\nprint(f\"\\n8. Web scraping best practices summary:\")\nbest_practices = [\n    \"âœ“ Always check robots.txt before scraping\",\n    \"âœ“ Use appropriate delays between requests\", \n    \"âœ“ Handle errors gracefully with try/except blocks\",\n    \"âœ“ Use sessions for multiple requests to same site\",\n    \"âœ“ Set proper User-Agent headers\",\n    \"âœ“ Respect rate limits and avoid overwhelming servers\",\n    \"âœ“ Cache responses when possible to reduce requests\",\n    \"âœ“ Validate and clean extracted data\",\n    \"âœ“ Store data in structured formats\",\n    \"âœ“ Monitor for website changes that break scrapers\"\n]\n\nfor practice in best_practices:\n    print(f\"   {practice}\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Summary\n",
"\n",
"In this notebook, you learned about:\n",
"\n",
"âœ… **Web Scraping Fundamentals**: HTTP requests, HTML structure, ethics[7][23]  \n",
"âœ… **Requests Library**: Making HTTP requests, handling responses, sessions  \n",
"âœ… **Beautiful Soup**: HTML parsing, element selection, data extraction[24][27]  \n",
"âœ… **Data Collection**: Structured data extraction and storage formats  \n",
"âœ… **Best Practices**: Rate limiting, error handling, respectful scraping[7][10]  \n",
"âœ… **Real-World Examples**: APIs, forms, multiple content types  \n",
"\n",
"### Key Takeaways:\n",
"1. Web scraping is powerful but must be done ethically and legally[7][23]\n",
"2. Always prefer official APIs when available over scraping\n",
"3. Requests and Beautiful Soup provide excellent tools for most scraping needs[24]\n",
"4. Rate limiting and error handling are crucial for robust scrapers[10]\n",
"5. Structure extracted data for easy analysis and storage\n",
"6. Test scrapers thoroughly and handle edge cases gracefully\n",
"\n",
"## ðŸŽ‰ **Complete Python Journey Achieved!**\n",
"\n",
"You've now mastered **30 comprehensive topics** covering:\n",
"- **Python Fundamentals** (1-19): Core language, OOP, file handling\n",
"- **Advanced Python** (20-24): Comprehensions, decorators, generators, regex  \n",
"- **Data Science Stack** (25-27): NumPy, Pandas, Matplotlib\n",
"- **Specialized Skills** (28-30): Seaborn, Machine Learning, Web Scraping\n",
"\n",
"You're now equipped with production-ready Python skills for data science, web development, automation, and beyond! ðŸðŸš€"
]
}
],
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.8.5"
}
},
"nbformat": 4,
"nbformat_minor": 4
}
