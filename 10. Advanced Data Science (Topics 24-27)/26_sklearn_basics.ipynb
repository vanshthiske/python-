{
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# Topic 29: Machine Learning Basics with Scikit-Learn\n",
"\n",
"## Overview\n",
"Scikit-learn is Python's most popular machine learning library, providing simple and efficient tools for data mining and analysis[9][25]. It offers a consistent API for various ML algorithms and preprocessing tools.\n",
"\n",
"### What You'll Learn:\n",
"- Machine learning workflow and concepts\n",
"- Supervised learning: classification and regression\n",
"- Unsupervised learning: clustering and dimensionality reduction\n",
"- Model evaluation and validation\n",
"- Data preprocessing and feature engineering\n",
"- Pipeline creation and model selection\n",
"\n",
"---"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 1. Machine Learning Fundamentals\n",
"\n",
"Understanding ML concepts and the scikit-learn API:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Machine Learning Fundamentals with Scikit-Learn\nfrom sklearn import datasets, model_selection, preprocessing, metrics\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nprint(\"Machine Learning Fundamentals with Scikit-Learn:\")\nprint(\"=\" * 47)\n\n# Machine Learning Problem Types\nprint(\"1. Machine Learning Problem Types:\")\nml_types = {\n    'Supervised Learning': {\n        'Classification': 'Predict categories (spam/not spam, disease/healthy)',\n        'Regression': 'Predict continuous values (price, temperature, sales)'\n    },\n    'Unsupervised Learning': {\n        'Clustering': 'Group similar data points (customer segments)',\n        'Dimensionality Reduction': 'Reduce feature space (visualization, compression)'\n    },\n    'Semi-supervised Learning': {\n        'Mixed': 'Uses both labeled and unlabeled data'\n    },\n    'Reinforcement Learning': {\n        'Sequential': 'Learn through rewards and penalties (games, robotics)'\n    }\n}\n\nfor category, problems in ml_types.items():\n    print(f\"   {category}:\")\n    for problem_type, description in problems.items():\n        print(f\"     {problem_type}: {description}\")\n\n# Load sample datasets\nprint(\"\\n2. Loading sample datasets:\")\n\n# Classification dataset (Iris)\niris = datasets.load_iris()\nprint(f\"   Iris dataset: {iris.data.shape} features, {len(iris.target_names)} classes\")\nprint(f\"   Features: {iris.feature_names}\")\nprint(f\"   Classes: {iris.target_names}\")\n\n# Regression dataset (Boston Housing - if available, otherwise California Housing)\ntry:\n    housing = datasets.load_boston()\n    print(f\"   Boston Housing: {housing.data.shape} features, continuous target\")\nexcept:\n    housing = datasets.fetch_california_housing()\n    print(f\"   California Housing: {housing.data.shape} features, continuous target\")\n\n# Convert to DataFrames for easier handling\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\niris_df['target'] = iris.target\niris_df['species'] = [iris.target_names[i] for i in iris.target]\n\nhousing_df = pd.DataFrame(housing.data, columns=housing.feature_names)\nhousing_df['target'] = housing.target\n\nprint(f\"\\n3. Exploring the datasets:\")\nprint(f\"   Iris dataset shape: {iris_df.shape}\")\nprint(f\"{iris_df.head()}\")\n\nprint(f\"\\n   Housing dataset (first 5 columns): {housing_df.iloc[:, :5].shape}\")\nprint(f\"{housing_df.iloc[:, :5].head()}\")\n\n# Basic statistics\nprint(f\"\\n4. Dataset statistics:\")\nprint(f\"   Iris target distribution:\")\nprint(f\"{iris_df['species'].value_counts()}\")\nprint(f\"\\n   Housing target statistics:\")\nprint(f\"     Mean price: ${housing_df['target'].mean():.2f}\")\nprint(f\"     Std deviation: ${housing_df['target'].std():.2f}\")\nprint(f\"     Price range: ${housing_df['target'].min():.2f} - ${housing_df['target'].max():.2f}\")\n\n# Scikit-learn API pattern\nprint(f\"\\n5. Scikit-learn API pattern:\")\nprint(f\"   1. Import the model class\")\nprint(f\"   2. Instantiate the model: model = ModelClass(parameters)\")\nprint(f\"   3. Fit the model: model.fit(X_train, y_train)\")\nprint(f\"   4. Make predictions: predictions = model.predict(X_test)\")\nprint(f\"   5. Evaluate: score = model.score(X_test, y_test)\")\n\n# Demonstrate basic workflow\nprint(f\"\\n6. Basic ML workflow demonstration:\")\n\n# Split the iris data\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint(f\"   Dataset split:\")\nprint(f\"   Training set: {X_train.shape}\")\nprint(f\"   Test set: {X_test.shape}\")\n\n# Train a simple classifier\nclf = LogisticRegression(random_state=42, max_iter=200)\nclf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = clf.predict(X_test)\naccuracy = clf.score(X_test, y_test)\n\nprint(f\"   Model trained and evaluated:\")\nprint(f\"   Accuracy: {accuracy:.3f}\")\nprint(f\"   Predictions: {y_pred[:10]}\")\nprint(f\"   Actual: {y_test[:10]}\")\n\n# Visualize results\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Feature visualization\nsns.scatterplot(data=iris_df, x='sepal length (cm)', y='sepal width (cm)', \n               hue='species', ax=axes[0])\naxes[0].set_title('Iris Dataset: Sepal Dimensions')\n\n# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n           xticklabels=iris.target_names, \n           yticklabels=iris.target_names, ax=axes[1])\naxes[1].set_title('Confusion Matrix')\naxes[1].set_ylabel('Actual')\naxes[1].set_xlabel('Predicted')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"   ✓ Successfully completed basic ML workflow\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 2. Supervised Learning: Classification\n",
"\n",
"Building and comparing classification models:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Supervised Learning: Classification\nprint(\"Supervised Learning: Classification:\")\nprint(\"=\" * 34)\n\n# Prepare data\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Scale features (important for some algorithms)\nscaler = preprocessing.StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"1. Multiple classification algorithms:\")\n\n# Define classifiers\nclassifiers = {\n    'Logistic Regression': LogisticRegression(random_state=42, max_iter=200),\n    'Decision Tree': DecisionTreeClassifier(random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'SVM (RBF)': SVC(kernel='rbf', random_state=42),\n    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n}\n\n# Train and evaluate each classifier\nresults = []\n\nfor name, clf in classifiers.items():\n    # Use scaled data for SVM and KNN, original for tree-based methods\n    if name in ['SVM (RBF)', 'K-Nearest Neighbors', 'Logistic Regression']:\n        clf.fit(X_train_scaled, y_train)\n        y_pred = clf.predict(X_test_scaled)\n        y_pred_proba = clf.predict_proba(X_test_scaled) if hasattr(clf, 'predict_proba') else None\n    else:\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        y_pred_proba = clf.predict_proba(X_test) if hasattr(clf, 'predict_proba') else None\n    \n    # Calculate metrics\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    precision = metrics.precision_score(y_test, y_pred, average='weighted')\n    recall = metrics.recall_score(y_test, y_pred, average='weighted')\n    f1 = metrics.f1_score(y_test, y_pred, average='weighted')\n    \n    results.append({\n        'Algorithm': name,\n        'Accuracy': accuracy,\n        'Precision': precision,\n        'Recall': recall,\n        'F1-Score': f1\n    })\n    \n    print(f\"   {name}:\")\n    print(f\"     Accuracy: {accuracy:.3f}\")\n    print(f\"     Precision: {precision:.3f}\")\n    print(f\"     Recall: {recall:.3f}\")\n    print(f\"     F1-Score: {f1:.3f}\")\n\n# Convert results to DataFrame for visualization\nresults_df = pd.DataFrame(results)\n\n# Visualize algorithm comparison\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Classification Algorithm Comparison', fontsize=16, fontweight='bold')\n\n# Metrics comparison\nmetrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\nfor i, metric in enumerate(metrics_to_plot):\n    row, col = i // 2, i % 2\n    sns.barplot(data=results_df, x='Algorithm', y=metric, ax=axes[row, col])\n    axes[row, col].set_title(f'{metric} Comparison')\n    axes[row, col].set_xticklabels(axes[row, col].get_xticklabels(), rotation=45)\n    axes[row, col].set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n2. Detailed classification report:\")\n\n# Use the best performing classifier for detailed analysis\nbest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nbest_clf.fit(X_train, y_train)\ny_pred_best = best_clf.predict(X_test)\n\n# Classification report\nprint(f\"   Classification Report (Random Forest):\")\nprint(metrics.classification_report(y_test, y_pred_best, \n                                  target_names=iris.target_names))\n\n# Feature importance\nfeature_importance = best_clf.feature_importances_\nfeature_names = iris.feature_names\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Feature importance plot\nfeature_df = pd.DataFrame({\n    'feature': feature_names,\n    'importance': feature_importance\n}).sort_values('importance', ascending=True)\n\nsns.barplot(data=feature_df, x='importance', y='feature', ax=ax1)\nax1.set_title('Feature Importance (Random Forest)')\nax1.set_xlabel('Importance Score')\n\n# Confusion matrix heatmap\ncm = confusion_matrix(y_test, y_pred_best)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n           xticklabels=iris.target_names,\n           yticklabels=iris.target_names, ax=ax2)\nax2.set_title('Confusion Matrix')\nax2.set_ylabel('Actual')\nax2.set_xlabel('Predicted')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"   ✓ Feature importance analysis completed\")\n\nprint(f\"\\n3. Cross-validation for robust evaluation:\")\n\n# Perform cross-validation\ncv_scores = cross_val_score(best_clf, X, y, cv=5, scoring='accuracy')\n\nprint(f\"   5-Fold Cross-Validation Results:\")\nprint(f\"   Individual fold scores: {cv_scores}\")\nprint(f\"   Mean accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\nprint(f\"   This gives us confidence in model performance\")\n\nprint(f\"\\n4. Hyperparameter tuning with Grid Search:\")\n\n# Define parameter grid\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 5, 7, None],\n    'min_samples_split': [2, 5, 10]\n}\n\n# Perform grid search\ngrid_search = GridSearchCV(RandomForestClassifier(random_state=42), \n                          param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\n\nprint(f\"   Best parameters: {grid_search.best_params_}\")\nprint(f\"   Best cross-validation score: {grid_search.best_score_:.3f}\")\nprint(f\"   Test set accuracy: {grid_search.score(X_test, y_test):.3f}\")\n\nprint(f\"\\n5. Model interpretation and predictions:\")\n\n# Make predictions on new data\nnew_samples = np.array([[5.1, 3.5, 1.4, 0.2],  # Likely Setosa\n                       [6.2, 2.8, 4.8, 1.8],  # Likely Versicolor \n                       [7.2, 3.0, 5.8, 2.2]]) # Likely Virginica\n\npredictions = best_clf.predict(new_samples)\nprediction_proba = best_clf.predict_proba(new_samples)\n\nprint(f\"   New sample predictions:\")\nfor i, (sample, pred, proba) in enumerate(zip(new_samples, predictions, prediction_proba)):\n    species_name = iris.target_names[pred]\n    confidence = max(proba)\n    print(f\"   Sample {i+1}: {sample}\")\n    print(f\"     Predicted: {species_name} (confidence: {confidence:.3f})\")\n    print(f\"     Class probabilities: {dict(zip(iris.target_names, proba.round(3)))}\")\n\nprint(f\"\\n6. Classification best practices:\")\nprint(f\"   ✓ Always split data into train/validation/test sets\")\nprint(f\"   ✓ Scale features for distance-based algorithms\")\nprint(f\"   ✓ Use cross-validation for robust evaluation\")\nprint(f\"   ✓ Consider class imbalance in real datasets\")\nprint(f\"   ✓ Interpret feature importance and model decisions\")\nprint(f\"   ✓ Tune hyperparameters systematically\")\nprint(f\"   ✓ Evaluate using multiple metrics (precision, recall, F1)\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 3. Supervised Learning: Regression\n",
"\n",
"Building regression models for continuous predictions:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Supervised Learning: Regression\nprint(\"Supervised Learning: Regression:\")\nprint(\"=\" * 30)\n\n# Prepare regression data\nX_reg = housing.data\ny_reg = housing.target\n\n# Split the data\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n    X_reg, y_reg, test_size=0.3, random_state=42)\n\n# Scale features\nscaler_reg = preprocessing.StandardScaler()\nX_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\nX_test_reg_scaled = scaler_reg.transform(X_test_reg)\n\nprint(f\"1. Regression dataset overview:\")\nprint(f\"   Features: {X_reg.shape[1]}\")\nprint(f\"   Samples: {X_reg.shape[0]}\")\nprint(f\"   Target range: {y_reg.min():.2f} - {y_reg.max():.2f}\")\nprint(f\"   Training set: {X_train_reg.shape}\")\nprint(f\"   Test set: {X_test_reg.shape}\")\n\nprint(f\"\\n2. Multiple regression algorithms:\")\n\n# Define regressors\nregressors = {\n    'Linear Regression': LinearRegression(),\n    'Decision Tree': DecisionTreeRegressor(random_state=42),\n    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n    'SVM (RBF)': SVR(kernel='rbf'),\n}\n\n# Train and evaluate each regressor\nregression_results = []\n\nfor name, reg in regressors.items():\n    # Use scaled data for SVM, original for tree-based methods\n    if name in ['SVM (RBF)', 'Linear Regression']:\n        reg.fit(X_train_reg_scaled, y_train_reg)\n        y_pred_reg = reg.predict(X_test_reg_scaled)\n    else:\n        reg.fit(X_train_reg, y_train_reg)\n        y_pred_reg = reg.predict(X_test_reg)\n    \n    # Calculate regression metrics\n    mse = metrics.mean_squared_error(y_test_reg, y_pred_reg)\n    rmse = np.sqrt(mse)\n    mae = metrics.mean_absolute_error(y_test_reg, y_pred_reg)\n    r2 = metrics.r2_score(y_test_reg, y_pred_reg)\n    \n    regression_results.append({\n        'Algorithm': name,\n        'MSE': mse,\n        'RMSE': rmse,\n        'MAE': mae,\n        'R²': r2\n    })\n    \n    print(f\"   {name}:\")\n    print(f\"     MSE: {mse:.3f}\")\n    print(f\"     RMSE: {rmse:.3f}\")\n    print(f\"     MAE: {mae:.3f}\")\n    print(f\"     R² Score: {r2:.3f}\")\n\n# Convert results to DataFrame\nregression_results_df = pd.DataFrame(regression_results)\n\n# Visualize regression results\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Regression Algorithm Comparison', fontsize=16, fontweight='bold')\n\n# Metrics comparison\nregression_metrics = ['MSE', 'RMSE', 'MAE', 'R²']\nfor i, metric in enumerate(regression_metrics):\n    row, col = i // 2, i % 2\n    sns.barplot(data=regression_results_df, x='Algorithm', y=metric, ax=axes[row, col])\n    axes[row, col].set_title(f'{metric} Comparison')\n    axes[row, col].set_xticklabels(axes[row, col].get_xticklabels(), rotation=45)\n    \n    # Set y-limit for R² to show differences better\n    if metric == 'R²':\n        axes[row, col].set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n3. Detailed analysis of best model:\")\n\n# Use Random Forest as the best model (typically performs well)\nbest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nbest_reg.fit(X_train_reg, y_train_reg)\ny_pred_best_reg = best_reg.predict(X_test_reg)\n\n# Feature importance for regression\nfeature_importance_reg = best_reg.feature_importances_\nfeature_names_reg = housing.feature_names\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfig.suptitle('Regression Model Analysis', fontsize=16, fontweight='bold')\n\n# Feature importance\nfeature_df_reg = pd.DataFrame({\n    'feature': feature_names_reg,\n    'importance': feature_importance_reg\n}).sort_values('importance', ascending=True)\n\nsns.barplot(data=feature_df_reg.tail(10), x='importance', y='feature', ax=axes[0,0])\naxes[0,0].set_title('Top 10 Feature Importance')\naxes[0,0].set_xlabel('Importance Score')\n\n# Predictions vs Actual\naxes[0,1].scatter(y_test_reg, y_pred_best_reg, alpha=0.5)\naxes[0,1].plot([y_test_reg.min(), y_test_reg.max()], \n              [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)\naxes[0,1].set_xlabel('Actual Values')\naxes[0,1].set_ylabel('Predicted Values')\naxes[0,1].set_title('Predictions vs Actual Values')\n\n# Residual plot\nresiduals = y_test_reg - y_pred_best_reg\naxes[1,0].scatter(y_pred_best_reg, residuals, alpha=0.5)\naxes[1,0].axhline(y=0, color='r', linestyle='--')\naxes[1,0].set_xlabel('Predicted Values')\naxes[1,0].set_ylabel('Residuals')\naxes[1,0].set_title('Residual Plot')\n\n# Residual distribution\naxes[1,1].hist(residuals, bins=30, alpha=0.7)\naxes[1,1].set_xlabel('Residuals')\naxes[1,1].set_ylabel('Frequency')\naxes[1,1].set_title('Residual Distribution')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"   ✓ Feature importance and residual analysis completed\")\n\nprint(f\"\\n4. Model validation and learning curves:\")\n\n# Learning curve analysis\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, val_scores = learning_curve(\n    best_reg, X_train_reg, y_train_reg, \n    train_sizes=np.linspace(0.1, 1.0, 10),\n    cv=5, scoring='r2'\n)\n\n# Plot learning curves\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Learning curve\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\nval_mean = np.mean(val_scores, axis=1)\nval_std = np.std(val_scores, axis=1)\n\nax1.plot(train_sizes, train_mean, 'o-', label='Training Score', color='blue')\nax1.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\nax1.plot(train_sizes, val_mean, 'o-', label='Validation Score', color='red')\nax1.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2, color='red')\nax1.set_xlabel('Training Set Size')\nax1.set_ylabel('R² Score')\nax1.set_title('Learning Curve')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Cross-validation scores\ncv_scores_reg = cross_val_score(best_reg, X_reg, y_reg, cv=5, scoring='r2')\nax2.bar(range(1, 6), cv_scores_reg)\nax2.axhline(y=cv_scores_reg.mean(), color='red', linestyle='--', \n           label=f'Mean: {cv_scores_reg.mean():.3f}')\nax2.set_xlabel('Fold')\nax2.set_ylabel('R² Score')\nax2.set_title('5-Fold Cross-Validation Scores')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"   Cross-validation R² scores: {cv_scores_reg}\")\nprint(f\"   Mean CV R² score: {cv_scores_reg.mean():.3f} (+/- {cv_scores_reg.std() * 2:.3f})\")\n\nprint(f\"\\n5. Making predictions on new data:\")\n\n# Example predictions\n# Create some example house features (scaled appropriately)\nnew_houses = np.array([\n    X_reg[0],  # Use first house as example\n    X_reg[100], # Use 100th house as example\n    np.mean(X_reg, axis=0)  # Average house\n])\n\npredictions_reg = best_reg.predict(new_houses)\n\nprint(f\"   Example house predictions:\")\nfor i, (house_features, predicted_price) in enumerate(zip(new_houses, predictions_reg)):\n    print(f\"   House {i+1}:\")\n    print(f\"     Predicted price: ${predicted_price:.2f}\")\n    print(f\"     Features: {house_features[:3].round(2)}... (showing first 3)\")\n\nprint(f\"\\n6. Regression model interpretation:\")\n\n# For Linear Regression, show coefficients\nlinear_reg = LinearRegression()\nlinear_reg.fit(X_train_reg_scaled, y_train_reg)\n\ncoefficients = pd.DataFrame({\n    'Feature': feature_names_reg,\n    'Coefficient': linear_reg.coef_\n}).sort_values('Coefficient', key=abs, ascending=False)\n\nprint(f\"   Linear Regression Coefficients (top 5):\")\nfor _, row in coefficients.head().iterrows():\n    print(f\"     {row['Feature']}: {row['Coefficient']:.3f}\")\n\nprint(f\"\\n7. Regression best practices:\")\nprint(f\"   ✓ Check for linear relationships with scatter plots\")\nprint(f\"   ✓ Handle outliers that can skew results\")\nprint(f\"   ✓ Scale features for linear models\")\nprint(f\"   ✓ Use multiple evaluation metrics (R², RMSE, MAE)\")\nprint(f\"   ✓ Analyze residuals for model diagnostics\")\nprint(f\"   ✓ Use cross-validation for robust evaluation\")\nprint(f\"   ✓ Consider feature engineering and polynomial terms\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Summary\n",
"\n",
"In this notebook, you learned about:\n",
"\n",
"✅ **ML Fundamentals**: Problem types, supervised vs unsupervised learning[9][22]  \n",
"✅ **Scikit-learn API**: Consistent interface for all algorithms[25]  \n",
"✅ **Classification**: Multiple algorithms, evaluation metrics, hyperparameter tuning[12]  \n",
"✅ **Regression**: Continuous prediction, residual analysis, feature importance  \n",
"✅ **Model Validation**: Cross-validation, learning curves, robust evaluation  \n",
"✅ **Best Practices**: Data splitting, preprocessing, interpretation[22]  \n",
"\n",
"### Key Takeaways:\n",
"1. Scikit-learn provides a unified API for diverse ML algorithms[9][25]\n",
"2. Always split data and use cross-validation for reliable results[12]\n",
"3. Feature scaling is crucial for distance-based algorithms\n",
"4. Multiple metrics provide better model understanding than single scores\n",
"5. Hyperparameter tuning can significantly improve performance[22]\n",
"6. Model interpretation is as important as performance metrics\n",
"\n",
"### Next Topic: 30_web_scraping.ipynb\n",
"Learn how to collect data from websites using Python scraping libraries."
]
}
],
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.8.5"
}
},
"nbformat": 4,
"nbformat_minor": 4
}
