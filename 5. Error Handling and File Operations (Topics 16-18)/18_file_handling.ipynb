{
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# Topic 18: File Handling and I/O Operations\n",
"\n",
"## Overview\n",
"File handling is essential for data persistence and processing. Learn to read, write, and manipulate files effectively in Python.\n",
"\n",
"### What You'll Learn:\n",
"- Opening and closing files\n",
"- Reading and writing text and binary files\n",
"- File modes and encoding\n",
"- Context managers and the 'with' statement\n",
"- File system operations\n",
"- Working with different file formats\n",
"\n",
"---"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 1. Basic File Operations\n",
"\n",
"Opening, reading, writing, and closing files:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Basic file operations\nprint(\"Basic File Operations:\")\nprint(\"=\" * 22)\n\n# Writing to a file\nprint(\"1. Writing to a file:\")\nfile_content = \"\"\"Hello, World!\nThis is a sample text file.\nIt contains multiple lines.\nPython file handling is powerful!\n\nNumbers: 1, 2, 3, 4, 5\nSpecial characters: @#$%^&*()\n\"\"\"\n\n# Method 1: Traditional file handling (not recommended)\nprint(\"   Method 1 - Traditional (not recommended):\")\nfile = open('sample.txt', 'w')\nfile.write(file_content)\nfile.close()\nprint(\"   File written and closed manually\")\n\n# Method 2: Using with statement (recommended)\nprint(\"   Method 2 - With statement (recommended):\")\nwith open('sample_with.txt', 'w') as file:\n    file.write(file_content)\nprint(\"   File written using with statement (auto-closed)\")\n\n# Reading from a file\nprint(\"\\n2. Reading from a file:\")\n\n# Read entire file\nwith open('sample.txt', 'r') as file:\n    content = file.read()\nprint(f\"   Entire file content ({len(content)} characters):\")\nprint(f\"   First 50 chars: {repr(content[:50])}...\")\n\n# Read line by line\nprint(\"\\n   Reading line by line:\")\nwith open('sample.txt', 'r') as file:\n    line_number = 1\n    for line in file:\n        print(f\"     Line {line_number}: {repr(line)}\")\n        line_number += 1\n        if line_number > 3:  # Show only first 3 lines\n            print(\"     ...\")\n            break\n\n# Read all lines into a list\nwith open('sample.txt', 'r') as file:\n    all_lines = file.readlines()\nprint(f\"\\n   Total lines read: {len(all_lines)}\")\nprint(f\"   Last line: {repr(all_lines[-1])}\")\n\n# Read specific number of characters\nwith open('sample.txt', 'r') as file:\n    first_20_chars = file.read(20)\n    next_20_chars = file.read(20)\nprint(f\"\\n   First 20 chars: {repr(first_20_chars)}\")\nprint(f\"   Next 20 chars: {repr(next_20_chars)}\")\n\n# File position and seeking\nprint(\"\\n3. File position and seeking:\")\nwith open('sample.txt', 'r') as file:\n    print(f\"   Initial position: {file.tell()}\")\n    data = file.read(10)\n    print(f\"   After reading 10 chars: {file.tell()}\")\n    file.seek(0)  # Go back to beginning\n    print(f\"   After seek(0): {file.tell()}\")\n    file.seek(5)  # Go to position 5\n    remaining = file.read(15)\n    print(f\"   Reading from position 5: {repr(remaining)}\")\n\n# Appending to a file\nprint(\"\\n4. Appending to a file:\")\nappend_content = \"\\nThis line was appended later.\\nAnother appended line.\\n\"\nwith open('sample.txt', 'a') as file:\n    file.write(append_content)\nprint(\"   Content appended successfully\")\n\n# Verify the append\nwith open('sample.txt', 'r') as file:\n    lines = file.readlines()\nprint(f\"   Total lines after append: {len(lines)}\")\nprint(f\"   Last two lines: {[line.strip() for line in lines[-2:]]}\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 2. File Modes and Encoding\n",
"\n",
"Understanding different file modes and character encoding:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# File modes and encoding\nprint(\"File Modes and Encoding:\")\nprint(\"=\" * 25)\n\n# Different file modes\nprint(\"1. File modes:\")\nfile_modes = {\n    'r': 'Read only (default)',\n    'w': 'Write only (truncates existing file)',\n    'a': 'Append only',\n    'x': 'Create new file (fails if exists)',\n    'r+': 'Read and write',\n    'w+': 'Read and write (truncates)',\n    'a+': 'Read and append',\n    'rb': 'Read binary',\n    'wb': 'Write binary',\n    'ab': 'Append binary'\n}\n\nfor mode, description in file_modes.items():\n    print(f\"   '{mode}': {description}\")\n\n# Demonstrate different modes\nprint(\"\\n2. Mode demonstrations:\")\n\n# Write mode (creates new or overwrites)\nwith open('mode_test.txt', 'w') as file:\n    file.write(\"Original content\\n\")\nprint(\"   'w' mode: File created with original content\")\n\n# Append mode\nwith open('mode_test.txt', 'a') as file:\n    file.write(\"Appended content\\n\")\nprint(\"   'a' mode: Content appended\")\n\n# Read mode\nwith open('mode_test.txt', 'r') as file:\n    content = file.read()\nprint(f\"   'r' mode: Read content:\\n{repr(content)}\")\n\n# r+ mode (read and write)\nwith open('mode_test.txt', 'r+') as file:\n    original = file.read()\n    file.seek(0, 2)  # Go to end of file\n    file.write(\"Added with r+\\n\")\n    file.seek(0)  # Go to beginning\n    modified = file.read()\nprint(f\"   'r+' mode: Added content at end\")\n\n# x mode (exclusive creation)\ntry:\n    with open('exclusive_test.txt', 'x') as file:\n        file.write(\"Created exclusively\\n\")\n    print(\"   'x' mode: File created exclusively\")\nexcept FileExistsError:\n    print(\"   'x' mode: File already exists\")\n\n# Try x mode again (should fail)\ntry:\n    with open('exclusive_test.txt', 'x') as file:\n        file.write(\"This won't work\\n\")\nexcept FileExistsError:\n    print(\"   'x' mode: Failed as expected (file exists)\")\n\n# Encoding examples\nprint(\"\\n3. Character encoding:\")\n\n# UTF-8 encoding (default in Python 3)\ntext_with_unicode = \"Hello! üêç Python supports Unicode: Œ±Œ≤Œ≥Œ¥Œµ ‰∏≠Êñá ÿßŸÑÿπÿ±ÿ®Ÿäÿ© üåç\"\n\nwith open('unicode_test.txt', 'w', encoding='utf-8') as file:\n    file.write(text_with_unicode)\nprint(\"   UTF-8: Unicode text written\")\n\n# Read with UTF-8\nwith open('unicode_test.txt', 'r', encoding='utf-8') as file:\n    read_unicode = file.read()\nprint(f\"   UTF-8: Read back: {read_unicode}\")\n\n# Different encodings\nencodings_to_test = ['utf-8', 'latin-1', 'ascii']\ntest_text = \"Hello, caf√©!\"\n\nfor encoding in encodings_to_test:\n    filename = f'encoding_{encoding.replace(\"-\", \"_\")}.txt'\n    try:\n        with open(filename, 'w', encoding=encoding) as file:\n            file.write(test_text)\n        with open(filename, 'r', encoding=encoding) as file:\n            read_back = file.read()\n        print(f\"   {encoding}: Success - {read_back}\")\n    except UnicodeEncodeError as e:\n        print(f\"   {encoding}: Encode error - {e}\")\n    except UnicodeDecodeError as e:\n        print(f\"   {encoding}: Decode error - {e}\")\n\n# Handle encoding errors\nprint(\"\\n4. Encoding error handling:\")\nproblematic_text = \"Caf√© na√Øve r√©sum√© ‰∏≠Êñá\"\n\n# Strict (default) - raises exception\ntry:\n    with open('ascii_test.txt', 'w', encoding='ascii') as file:\n        file.write(problematic_text)\nexcept UnicodeEncodeError as e:\n    print(f\"   Strict mode: {e}\")\n\n# Ignore errors\nwith open('ascii_ignore.txt', 'w', encoding='ascii', errors='ignore') as file:\n    file.write(problematic_text)\nwith open('ascii_ignore.txt', 'r', encoding='ascii') as file:\n    ignored_result = file.read()\nprint(f\"   Ignore errors: '{ignored_result}'\")\n\n# Replace errors\nwith open('ascii_replace.txt', 'w', encoding='ascii', errors='replace') as file:\n    file.write(problematic_text)\nwith open('ascii_replace.txt', 'r', encoding='ascii') as file:\n    replaced_result = file.read()\nprint(f\"   Replace errors: '{replaced_result}'\")\n\n# Check file encoding\nprint(\"\\n5. Detecting file encoding:\")\nimport locale\nprint(f\"   System default encoding: {locale.getpreferredencoding()}\")\nprint(f\"   Python default encoding: utf-8 (since Python 3.0)\")\n\n# File object properties\nwith open('unicode_test.txt', 'r', encoding='utf-8') as file:\n    print(f\"   File encoding: {file.encoding}\")\n    print(f\"   File mode: {file.mode}\")\n    print(f\"   File name: {file.name}\")\n    print(f\"   File closed: {file.closed}\")\n\nprint(f\"   File closed after context: {file.closed}\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 3. Binary Files and Advanced Operations\n",
"\n",
"Working with binary data and advanced file operations:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Binary files and advanced operations\nprint(\"Binary Files and Advanced Operations:\")\nprint(\"=\" * 37)\n\n# Working with binary files\nprint(\"1. Binary file operations:\")\n\n# Create some binary data\nbinary_data = bytes([0, 1, 2, 3, 255, 254, 253, 65, 66, 67])  # Mix of values and ASCII\nprint(f\"   Binary data: {binary_data}\")\nprint(f\"   As list: {list(binary_data)}\")\n\n# Write binary data\nwith open('binary_test.bin', 'wb') as file:\n    file.write(binary_data)\nprint(\"   Binary data written to file\")\n\n# Read binary data\nwith open('binary_test.bin', 'rb') as file:\n    read_binary = file.read()\nprint(f\"   Binary data read back: {read_binary}\")\nprint(f\"   Data matches: {binary_data == read_binary}\")\n\n# Working with different binary data types\nprint(\"\\n2. Different binary data types:\")\n\n# Bytes from string\ntext = \"Hello, World!\"\ntext_bytes = text.encode('utf-8')\nprint(f\"   Text: '{text}'\")\nprint(f\"   As bytes: {text_bytes}\")\nprint(f\"   Back to text: '{text_bytes.decode('utf-8')}'\")\n\n# Bytearray (mutable bytes)\nba = bytearray(b\"mutable\")\nprint(f\"   Bytearray: {ba}\")\nba[0] = ord('M')  # Change first character to 'M'\nprint(f\"   Modified: {ba}\")\nprint(f\"   As string: '{ba.decode()}'\")\n\n# Working with integers and bytes\nprint(\"\\n3. Integer and bytes conversion:\")\nnumber = 1000\nnumber_bytes = number.to_bytes(4, byteorder='big')\nprint(f\"   Number: {number}\")\nprint(f\"   As bytes (big-endian): {number_bytes}\")\nprint(f\"   Back to int: {int.from_bytes(number_bytes, byteorder='big')}\")\n\n# Little-endian\nnumber_bytes_little = number.to_bytes(4, byteorder='little')\nprint(f\"   As bytes (little-endian): {number_bytes_little}\")\nprint(f\"   Back to int: {int.from_bytes(number_bytes_little, byteorder='little')}\")\n\n# File copying (binary)\nprint(\"\\n4. File copying:\")\n\n# Create a test file with mixed content\ntest_content = \"Mixed content: text and binary data\\n\" + \"\\x00\\x01\\x02\\xff\\xfe\\xfd\"\nwith open('source.txt', 'w', encoding='utf-8') as file:\n    # Note: This won't work for pure binary, but demonstrates concept\n    file.write(\"Mixed content: text and binary markers\")\n\n# Copy file byte by byte\ndef copy_file(source, destination):\n    \"\"\"Copy file using binary mode\"\"\"\n    with open(source, 'rb') as src:\n        with open(destination, 'wb') as dst:\n            while True:\n                chunk = src.read(1024)  # Read in chunks\n                if not chunk:\n                    break\n                dst.write(chunk)\n    print(f\"   Copied {source} to {destination}\")\n\ncopy_file('source.txt', 'destination.txt')\n\n# Verify copy\nwith open('source.txt', 'rb') as f1, open('destination.txt', 'rb') as f2:\n    original = f1.read()\n    copied = f2.read()\n    print(f\"   Copy successful: {original == copied}\")\n\n# Working with file chunks\nprint(\"\\n5. Processing large files in chunks:\")\n\n# Create a larger test file\nlarge_content = \"Line {}\\n\".format(i) * 1000 for i in range(1000)\nwith open('large_file.txt', 'w') as file:\n    for i in range(1000):\n        file.write(f\"Line {i+1}: This is line number {i+1} with some content.\\n\")\nprint(\"   Large file created (1000 lines)\")\n\n# Process file in chunks\ndef process_file_in_chunks(filename, chunk_size=1024):\n    \"\"\"Process file in chunks to handle large files\"\"\"\n    line_count = 0\n    char_count = 0\n    \n    with open(filename, 'r') as file:\n        while True:\n            chunk = file.read(chunk_size)\n            if not chunk:\n                break\n            char_count += len(chunk)\n            line_count += chunk.count('\\n')\n    \n    return line_count, char_count\n\nlines, chars = process_file_in_chunks('large_file.txt')\nprint(f\"   Processed: {lines} lines, {chars} characters\")\n\n# File seeking and random access\nprint(\"\\n6. Random file access:\")\n\n# Create a file with numbered records\nrecord_size = 20  # Fixed record size\nwith open('records.txt', 'w') as file:\n    for i in range(10):\n        record = f\"Record {i:02d}:{' '*8}\\n\"  # Pad to fixed size\n        file.write(record)\n\nprint(f\"   Created file with {10} fixed-size records\")\n\n# Random access to specific record\ndef read_record(filename, record_number, record_size):\n    \"\"\"Read specific record by number\"\"\"\n    with open(filename, 'r') as file:\n        file.seek(record_number * record_size)\n        return file.read(record_size).strip()\n\nfor record_num in [0, 5, 9]:\n    record_data = read_record('records.txt', record_num, record_size)\n    print(f\"   Record {record_num}: '{record_data}'\")\n\n# Memory-mapped files (for very large files)\nprint(\"\\n7. Memory-mapped files:\")\nimport mmap\n\n# Create a test file\nwith open('mmap_test.txt', 'w') as file:\n    file.write(\"0123456789\" * 100)  # 1000 characters\n\n# Use memory mapping\nwith open('mmap_test.txt', 'r+b') as file:\n    with mmap.mmap(file.fileno(), 0) as mmapped_file:\n        print(f\"   File size: {len(mmapped_file)} bytes\")\n        print(f\"   First 20 bytes: {mmapped_file[:20]}\")\n        \n        # Modify through memory map\n        mmapped_file[0:5] = b'HELLO'\n        mmapped_file.flush()\n        \n        print(f\"   After modification: {mmapped_file[:20]}\")\n\n# Verify modification\nwith open('mmap_test.txt', 'r') as file:\n    content = file.read(20)\n    print(f\"   File content after mmap: '{content}'\")\n\nprint(\"\\nBinary file operations completed!\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 4. File System Operations\n",
"\n",
"Working with directories, paths, and file metadata:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# File system operations\nimport os\nimport pathlib\nimport glob\nimport shutil\nimport stat\nimport time\nfrom datetime import datetime\n\nprint(\"File System Operations:\")\nprint(\"=\" * 23)\n\n# Working with paths\nprint(\"1. Path operations:\")\n\n# Using os.path (traditional)\ncurrent_dir = os.getcwd()\nparent_dir = os.path.dirname(current_dir)\nprint(f\"   Current directory: {os.path.basename(current_dir)}\")\nprint(f\"   Parent directory: {os.path.basename(parent_dir)}\")\n\n# Join paths safely\ntest_path = os.path.join(current_dir, 'test_folder', 'subdir', 'file.txt')\nprint(f\"   Joined path: {test_path}\")\n\n# Path components\nprint(f\"   Directory: {os.path.dirname(test_path)}\")\nprint(f\"   Filename: {os.path.basename(test_path)}\")\nprint(f\"   Extension: {os.path.splitext(test_path)[1]}\")\nprint(f\"   Without extension: {os.path.splitext(test_path)[0]}\")\n\n# Using pathlib (modern approach)\nprint(\"\\n2. Pathlib (modern approach):\")\npath = pathlib.Path.cwd()\nprint(f\"   Current path: {path.name}\")\nprint(f\"   Parent: {path.parent.name}\")\nprint(f\"   Home directory: {pathlib.Path.home()}\")\n\n# Path operations with pathlib\ntest_pathlib = pathlib.Path('test_folder') / 'subdir' / 'file.txt'\nprint(f\"   Pathlib joined: {test_pathlib}\")\nprint(f\"   Parts: {test_pathlib.parts}\")\nprint(f\"   Suffix: {test_pathlib.suffix}\")\nprint(f\"   Stem: {test_pathlib.stem}\")\nprint(f\"   Is absolute: {test_pathlib.is_absolute()}\")\n\n# Directory operations\nprint(\"\\n3. Directory operations:\")\n\n# Create directory structure\ntest_dir = pathlib.Path('test_directory_structure')\nsubdirs = ['subdir1', 'subdir2', 'subdir1/nested']\n\nfor subdir in subdirs:\n    full_path = test_dir / subdir\n    full_path.mkdir(parents=True, exist_ok=True)\n    print(f\"   Created: {full_path}\")\n\n# Create some test files\ntest_files = [\n    test_dir / 'file1.txt',\n    test_dir / 'file2.py',\n    test_dir / 'subdir1' / 'nested_file.txt',\n    test_dir / 'subdir2' / 'another_file.py'\n]\n\nfor file_path in test_files:\n    file_path.write_text(f\"Content of {file_path.name}\")\n    print(f\"   Created file: {file_path}\")\n\n# List directory contents\nprint(\"\\n4. Directory listing:\")\n\n# Using os.listdir\ncontents = os.listdir(test_dir)\nprint(f\"   os.listdir: {contents}\")\n\n# Using pathlib\nprint(\"   pathlib contents:\")\nfor item in test_dir.iterdir():\n    item_type = \"DIR\" if item.is_dir() else \"FILE\"\n    print(f\"     {item_type}: {item.name}\")\n\n# Recursive directory walking\nprint(\"\\n5. Recursive directory walking:\")\n\n# Using os.walk\nprint(\"   os.walk:\")\nfor root, dirs, files in os.walk(test_dir):\n    level = root.replace(str(test_dir), '').count(os.sep)\n    indent = '  ' * (level + 1)\n    print(f\"{indent}{os.path.basename(root)}/\")\n    subindent = '  ' * (level + 2)\n    for file in files:\n        print(f\"{subindent}{file}\")\n\n# Using pathlib recursively\nprint(\"\\n   pathlib recursive:\")\nfor item in test_dir.rglob('*'):\n    if item.is_file():\n        relative = item.relative_to(test_dir)\n        print(f\"     FILE: {relative}\")\n\n# Pattern matching with glob\nprint(\"\\n6. Pattern matching:\")\n\n# Find all .py files\npy_files = list(test_dir.glob('**/*.py'))\nprint(f\"   Python files: {[f.name for f in py_files]}\")\n\n# Find all .txt files\ntxt_files = list(test_dir.rglob('*.txt'))\nprint(f\"   Text files: {[f.name for f in txt_files]}\")\n\n# Using glob module\nimport glob as glob_module\nall_py_files = glob_module.glob('**/*.py', recursive=True)\nprint(f\"   All .py files in current area: {len(all_py_files)} files\")\n\n# File metadata and properties\nprint(\"\\n7. File metadata:\")\n\ntest_file = test_dir / 'file1.txt'\nif test_file.exists():\n    # Using pathlib\n    stat_info = test_file.stat()\n    print(f\"   File: {test_file.name}\")\n    print(f\"   Size: {stat_info.st_size} bytes\")\n    print(f\"   Modified: {datetime.fromtimestamp(stat_info.st_mtime)}\")\n    print(f\"   Is file: {test_file.is_file()}\")\n    print(f\"   Is directory: {test_file.is_dir()}\")\n    print(f\"   Permissions: {oct(stat_info.st_mode)}\")\n    \n    # Using os.path\n    print(f\"   Exists (os.path): {os.path.exists(test_file)}\")\n    print(f\"   Size (os.path): {os.path.getsize(test_file)} bytes\")\n    print(f\"   Modified (os.path): {datetime.fromtimestamp(os.path.getmtime(test_file))}\")\n\n# File permissions\nprint(\"\\n8. File permissions:\")\nif test_file.exists():\n    # Get current permissions\n    current_mode = test_file.stat().st_mode\n    print(f\"   Current permissions: {oct(current_mode)}\")\n    \n    # Check specific permissions\n    print(f\"   Is readable: {os.access(test_file, os.R_OK)}\")\n    print(f\"   Is writable: {os.access(test_file, os.W_OK)}\")\n    print(f\"   Is executable: {os.access(test_file, os.X_OK)}\")\n    \n    # Change permissions (be careful!)\n    try:\n        test_file.chmod(0o644)  # rw-r--r--\n        print(f\"   Permissions changed to 644\")\n    except PermissionError:\n        print(f\"   Permission denied changing file permissions\")\n\n# File operations\nprint(\"\\n9. File operations:\")\n\n# Copy files\nsource_file = test_dir / 'file1.txt'\ndest_file = test_dir / 'file1_copy.txt'\n\n# Using shutil\nshutil.copy2(source_file, dest_file)  # copy2 preserves metadata\nprint(f\"   Copied {source_file.name} to {dest_file.name}\")\n\n# Move/rename files\nold_name = dest_file\nnew_name = test_dir / 'renamed_file.txt'\nshutil.move(old_name, new_name)\nprint(f\"   Moved {old_name.name} to {new_name.name}\")\n\n# Delete files and directories\nprint(\"\\n10. Cleanup operations:\")\n\n# Delete a file\nif new_name.exists():\n    new_name.unlink()  # pathlib way\n    print(f\"   Deleted file: {new_name.name}\")\n\n# Delete directory and contents\nif test_dir.exists():\n    shutil.rmtree(test_dir)  # Recursive delete\n    print(f\"   Deleted directory tree: {test_dir.name}\")\n\n# Temporary files and directories\nprint(\"\\n11. Temporary files:\")\nimport tempfile\n\n# Temporary file\nwith tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as tmp:\n    tmp.write(\"Temporary content\")\n    tmp_name = tmp.name\nprint(f\"   Created temporary file: {os.path.basename(tmp_name)}\")\n\n# Read from temporary file\nwith open(tmp_name, 'r') as tmp:\n    content = tmp.read()\nprint(f\"   Temporary file content: '{content}'\")\n\n# Clean up temporary file\nos.unlink(tmp_name)\nprint(f\"   Temporary file deleted\")\n\n# Temporary directory\nwith tempfile.TemporaryDirectory() as tmp_dir:\n    tmp_path = pathlib.Path(tmp_dir)\n    test_file = tmp_path / 'test.txt'\n    test_file.write_text('Test content')\n    print(f\"   Temporary directory: {tmp_path.name}\")\n    print(f\"   Contents: {list(tmp_path.iterdir())}\")\n\nprint(f\"   Temporary directory automatically deleted\")\n\nprint(\"\\nFile system operations completed!\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 5. Working with Different File Formats\n",
"\n",
"Handling CSV, JSON, and other common file formats:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Working with different file formats\nimport csv\nimport json\nimport configparser\nimport pickle\nfrom datetime import datetime, date\n\nprint(\"Working with Different File Formats:\")\nprint(\"=\" * 36)\n\n# 1. CSV Files\nprint(\"1. CSV (Comma-Separated Values):\")\n\n# Sample data\nemployees = [\n    ['Name', 'Age', 'Department', 'Salary'],\n    ['Alice Johnson', 28, 'Engineering', 75000],\n    ['Bob Smith', 35, 'Marketing', 65000],\n    ['Charlie Brown', 42, 'Sales', 70000],\n    ['Diana Wilson', 31, 'Engineering', 80000],\n    ['Eve Davis', 29, 'HR', 60000]\n]\n\n# Write CSV file\nwith open('employees.csv', 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerows(employees)\nprint(\"   CSV file written with employee data\")\n\n# Read CSV file\nwith open('employees.csv', 'r') as csvfile:\n    reader = csv.reader(csvfile)\n    csv_data = list(reader)\n    print(f\"   Read {len(csv_data)} rows from CSV\")\n    print(f\"   Header: {csv_data[0]}\")\n    print(f\"   First employee: {csv_data[1]}\")\n\n# Working with CSV DictReader/DictWriter\nprint(\"\\n   Using DictReader/DictWriter:\")\n\n# Write using DictWriter\nemployee_dicts = [\n    {'name': 'Frank Miller', 'age': 33, 'dept': 'IT', 'salary': 72000},\n    {'name': 'Grace Lee', 'age': 27, 'dept': 'Design', 'salary': 68000},\n    {'name': 'Henry Kim', 'age': 39, 'dept': 'Finance', 'salary': 75000}\n]\n\nwith open('employees_dict.csv', 'w', newline='') as csvfile:\n    fieldnames = ['name', 'age', 'dept', 'salary']\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n    writer.writeheader()\n    writer.writerows(employee_dicts)\nprint(\"   Dictionary-based CSV written\")\n\n# Read using DictReader\nwith open('employees_dict.csv', 'r') as csvfile:\n    reader = csv.DictReader(csvfile)\n    for row in reader:\n        print(f\"     {row['name']}: {row['age']} years, {row['dept']}, ${row['salary']}\")\n\n# 2. JSON Files\nprint(\"\\n2. JSON (JavaScript Object Notation):\")\n\n# Sample data structure\ncompany_data = {\n    'company': 'Tech Corp',\n    'founded': 2010,\n    'employees': [\n        {\n            'id': 1,\n            'name': 'Alice Johnson',\n            'position': 'Senior Developer',\n            'skills': ['Python', 'JavaScript', 'SQL'],\n            'active': True,\n            'hire_date': '2020-01-15'\n        },\n        {\n            'id': 2,\n            'name': 'Bob Smith',\n            'position': 'Product Manager',\n            'skills': ['Project Management', 'Analytics'],\n            'active': True,\n            'hire_date': '2019-03-22'\n        }\n    ],\n    'locations': {\n        'headquarters': 'San Francisco',\n        'offices': ['New York', 'London', 'Tokyo']\n    }\n}\n\n# Write JSON file\nwith open('company.json', 'w') as jsonfile:\n    json.dump(company_data, jsonfile, indent=2)\nprint(\"   JSON file written with company data\")\n\n# Read JSON file\nwith open('company.json', 'r') as jsonfile:\n    loaded_data = json.load(jsonfile)\n    print(f\"   Company: {loaded_data['company']}\")\n    print(f\"   Number of employees: {len(loaded_data['employees'])}\")\n    print(f\"   First employee: {loaded_data['employees'][0]['name']}\")\n\n# JSON with custom encoder (for dates)\nclass DateTimeEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, (datetime, date)):\n            return obj.isoformat()\n        return super().default(obj)\n\ndata_with_dates = {\n    'timestamp': datetime.now(),\n    'date': date.today(),\n    'info': 'Data with dates'\n}\n\nwith open('data_with_dates.json', 'w') as jsonfile:\n    json.dump(data_with_dates, jsonfile, cls=DateTimeEncoder, indent=2)\nprint(\"   JSON with custom date encoding written\")\n\n# Pretty print JSON\nprint(\"   Pretty-printed JSON sample:\")\nprint(json.dumps(loaded_data['employees'][0], indent=2))\n\n# 3. Configuration Files (INI format)\nprint(\"\\n3. Configuration Files (INI format):\")\n\n# Create configuration\nconfig = configparser.ConfigParser()\nconfig['DEFAULT'] = {\n    'debug': 'False',\n    'timeout': '30'\n}\nconfig['database'] = {\n    'host': 'localhost',\n    'port': '5432',\n    'name': 'myapp',\n    'user': 'admin'\n}\nconfig['api'] = {\n    'base_url': 'https://api.example.com',\n    'version': 'v2',\n    'timeout': '60'  # Overrides DEFAULT\n}\n\n# Write configuration file\nwith open('config.ini', 'w') as configfile:\n    config.write(configfile)\nprint(\"   Configuration file written\")\n\n# Read configuration file\nconfig_read = configparser.ConfigParser()\nconfig_read.read('config.ini')\n\nprint(f\"   Database host: {config_read['database']['host']}\")\nprint(f\"   API timeout: {config_read['api']['timeout']}\")\nprint(f\"   Default timeout: {config_read['DEFAULT']['timeout']}\")\n\n# List all sections and options\nprint(f\"   Sections: {config_read.sections()}\")\nfor section in config_read.sections():\n    print(f\"   {section} options: {list(config_read[section].keys())}\")\n\n# 4. Pickle (Python object serialization)\nprint(\"\\n4. Pickle (Python object serialization):\")\n\n# Complex Python object\nclass Person:\n    def __init__(self, name, age, hobbies):\n        self.name = name\n        self.age = age\n        self.hobbies = hobbies\n    \n    def __repr__(self):\n        return f\"Person('{self.name}', {self.age}, {self.hobbies})\"\n\ncomplex_data = {\n    'people': [\n        Person('Alice', 28, ['reading', 'coding']),\n        Person('Bob', 35, ['music', 'sports'])\n    ],\n    'numbers': [1, 2, 3, 4, 5],\n    'nested': {'inner': {'deep': 'value'}},\n    'timestamp': datetime.now()\n}\n\n# Write pickle file\nwith open('complex_data.pkl', 'wb') as picklefile:\n    pickle.dump(complex_data, picklefile)\nprint(\"   Complex data pickled\")\n\n# Read pickle file\nwith open('complex_data.pkl', 'rb') as picklefile:\n    loaded_complex = pickle.load(picklefile)\n    print(f\"   Loaded people: {loaded_complex['people']}\")\n    print(f\"   Loaded timestamp: {loaded_complex['timestamp']}\")\n    print(f\"   Object types preserved: {type(loaded_complex['people'][0])}\")\n\nprint(\"   ‚ö†Ô∏è  Warning: Only pickle files from trusted sources!\")\n\n# 5. Working with large files efficiently\nprint(\"\\n5. Efficient handling of large files:\")\n\n# Create a large CSV for demonstration\nwith open('large_data.csv', 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['id', 'value', 'category'])\n    for i in range(10000):\n        writer.writerow([i, f'value_{i}', f'cat_{i % 10}'])\nprint(\"   Created large CSV file (10,000 rows)\")\n\n# Process large file in chunks\ndef process_large_csv(filename, chunk_size=1000):\n    \"\"\"Process large CSV file in chunks\"\"\"\n    total_rows = 0\n    categories = set()\n    \n    with open(filename, 'r') as csvfile:\n        reader = csv.DictReader(csvfile)\n        chunk = []\n        \n        for row in reader:\n            chunk.append(row)\n            categories.add(row['category'])\n            \n            if len(chunk) >= chunk_size:\n                # Process chunk\n                total_rows += len(chunk)\n                chunk = []\n        \n        # Process remaining chunk\n        if chunk:\n            total_rows += len(chunk)\n    \n    return total_rows, len(categories)\n\nrows, unique_cats = process_large_csv('large_data.csv')\nprint(f\"   Processed {rows} rows with {unique_cats} unique categories\")\n\n# File format summary\nprint(\"\\nFile Format Summary:\")\nformats = {\n    'CSV': 'Tabular data, widely supported, human-readable',\n    'JSON': 'Structured data, web APIs, human-readable',\n    'INI': 'Configuration files, simple key-value pairs',\n    'Pickle': 'Python objects, not human-readable, Python-specific',\n    'Binary': 'Raw data, images, executables, most efficient',\n    'Text': 'Plain text, logs, documents, human-readable'\n}\n\nfor fmt, description in formats.items():\n    print(f\"   {fmt}: {description}\")\n\nprint(\"\\nFile format operations completed!\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Summary\n",
"\n",
"In this notebook, you learned about:\n",
"\n",
"‚úÖ **Basic File Operations**: Opening, reading, writing, and closing files  \n",
"‚úÖ **File Modes**: Different modes and their purposes, encoding handling  \n",
"‚úÖ **Binary Files**: Working with binary data and advanced operations  \n",
"‚úÖ **File System**: Directory operations, paths, and metadata  \n",
"‚úÖ **File Formats**: CSV, JSON, INI, Pickle, and format-specific operations  \n",
"‚úÖ **Best Practices**: Context managers, error handling, and efficiency  \n",
"\n",
"### Key Takeaways:\n",
"1. Always use context managers (with statement) for file operations\n",
"2. Specify encoding explicitly when working with text files\n",
"3. Use pathlib for modern path operations\n",
"4. Process large files in chunks to manage memory\n",
"5. Choose appropriate file formats for your data\n",
"6. Handle file operations errors gracefully\n",
"\n",
"### Next Topic: 19_oop_basics.ipynb\n",
"Learn about Object-Oriented Programming fundamentals in Python."
]
}
],
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.8.5"
}
},
"nbformat": 4,
"nbformat_minor": 4
}
