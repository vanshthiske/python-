{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 18: File Handling and I/O Operations\n",
    "\n",
    "## Overview\n",
    "File handling is essential for data persistence and processing. Learn to read, write, and manipulate files effectively in Python.\n",
    "\n",
    "### What You'll Learn:\n",
    "- Opening and closing files\n",
    "- Reading and writing text and binary files\n",
    "- File modes and encoding\n",
    "- Context managers and the 'with' statement\n",
    "- File system operations\n",
    "- Working with different file formats\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic File Operations\n",
    "\n",
    "Opening, reading, writing, and closing files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic File Operations:\n",
      "======================\n",
      "1. Writing to a file:\n",
      "   Method 1 - Traditional (not recommended):\n",
      "   File written and closed manually\n",
      "   Method 2 - With statement (recommended):\n",
      "   File written using with statement (auto-closed)\n",
      "\n",
      "2. Reading from a file:\n",
      "   Entire file content (158 characters):\n",
      "   First 50 chars: 'Hello, World!\\nThis is a sample text file.\\nIt conta'...\n",
      "\n",
      "   Reading line by line:\n",
      "     Line 1: 'Hello, World!\\n'\n",
      "     Line 2: 'This is a sample text file.\\n'\n",
      "     Line 3: 'It contains multiple lines.\\n'\n",
      "     ...\n",
      "\n",
      "   Total lines read: 7\n",
      "   Last line: 'Special characters: @#$%^&*()\\n'\n",
      "\n",
      "   First 20 chars: 'Hello, World!\\nThis i'\n",
      "   Next 20 chars: 's a sample text file'\n",
      "\n",
      "3. File position and seeking:\n",
      "   Initial position: 0\n",
      "   After reading 10 chars: 10\n",
      "   After seek(0): 0\n",
      "   Reading from position 5: ', World!\\nThis i'\n",
      "\n",
      "4. Appending to a file:\n",
      "   Content appended successfully\n",
      "   Total lines after append: 10\n",
      "   Last two lines: ['This line was appended later.', 'Another appended line.']\n"
     ]
    }
   ],
   "source": [
    "# Basic file operations\n",
    "print(\"Basic File Operations:\")\n",
    "print(\"=\" * 22)\n",
    "\n",
    "# Writing to a file\n",
    "print(\"1. Writing to a file:\")\n",
    "file_content = \"\"\"Hello, World!\n",
    "This is a sample text file.\n",
    "It contains multiple lines.\n",
    "Python file handling is powerful!\n",
    "\n",
    "Numbers: 1, 2, 3, 4, 5\n",
    "Special characters: @#$%^&*()\n",
    "\"\"\"\n",
    "\n",
    "# Method 1: Traditional file handling (not recommended)\n",
    "print(\"   Method 1 - Traditional (not recommended):\")\n",
    "file = open('sample.txt', 'w')\n",
    "file.write(file_content)\n",
    "file.close()\n",
    "print(\"   File written and closed manually\")\n",
    "\n",
    "# Method 2: Using with statement (recommended)\n",
    "print(\"   Method 2 - With statement (recommended):\")\n",
    "with open('sample_with.txt', 'w') as file:\n",
    "    file.write(file_content)\n",
    "print(\"   File written using with statement (auto-closed)\")\n",
    "\n",
    "# Reading from a file\n",
    "print(\"\\n2. Reading from a file:\")\n",
    "\n",
    "# Read entire file\n",
    "with open('sample.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "print(f\"   Entire file content ({len(content)} characters):\")\n",
    "print(f\"   First 50 chars: {repr(content[:50])}...\")\n",
    "\n",
    "# Read line by line\n",
    "print(\"\\n   Reading line by line:\")\n",
    "with open('sample.txt', 'r') as file:\n",
    "    line_number = 1\n",
    "    for line in file:\n",
    "        print(f\"     Line {line_number}: {repr(line)}\")\n",
    "        line_number += 1\n",
    "        if line_number > 3:  # Show only first 3 lines\n",
    "            print(\"     ...\")\n",
    "            break\n",
    "\n",
    "# Read all lines into a list\n",
    "with open('sample.txt', 'r') as file:\n",
    "    all_lines = file.readlines()\n",
    "print(f\"\\n   Total lines read: {len(all_lines)}\")\n",
    "print(f\"   Last line: {repr(all_lines[-1])}\")\n",
    "\n",
    "# Read specific number of characters\n",
    "with open('sample.txt', 'r') as file:\n",
    "    first_20_chars = file.read(20)\n",
    "    next_20_chars = file.read(20)\n",
    "print(f\"\\n   First 20 chars: {repr(first_20_chars)}\")\n",
    "print(f\"   Next 20 chars: {repr(next_20_chars)}\")\n",
    "\n",
    "# File position and seeking\n",
    "print(\"\\n3. File position and seeking:\")\n",
    "with open('sample.txt', 'r') as file:\n",
    "    print(f\"   Initial position: {file.tell()}\")\n",
    "    data = file.read(10)\n",
    "    print(f\"   After reading 10 chars: {file.tell()}\")\n",
    "    file.seek(0)  # Go back to beginning\n",
    "    print(f\"   After seek(0): {file.tell()}\")\n",
    "    file.seek(5)  # Go to position 5\n",
    "    remaining = file.read(15)\n",
    "    print(f\"   Reading from position 5: {repr(remaining)}\")\n",
    "\n",
    "# Appending to a file\n",
    "print(\"\\n4. Appending to a file:\")\n",
    "append_content = \"\\nThis line was appended later.\\nAnother appended line.\\n\"\n",
    "with open('sample.txt', 'a') as file:\n",
    "    file.write(append_content)\n",
    "print(\"   Content appended successfully\")\n",
    "\n",
    "# Verify the append\n",
    "with open('sample.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "print(f\"   Total lines after append: {len(lines)}\")\n",
    "print(f\"   Last two lines: {[line.strip() for line in lines[-2:]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File Modes and Encoding\n",
    "\n",
    "Understanding different file modes and character encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Modes and Encoding:\n",
      "=========================\n",
      "1. File modes:\n",
      "   'r': Read only (default)\n",
      "   'w': Write only (truncates existing file)\n",
      "   'a': Append only\n",
      "   'x': Create new file (fails if exists)\n",
      "   'r+': Read and write\n",
      "   'w+': Read and write (truncates)\n",
      "   'a+': Read and append\n",
      "   'rb': Read binary\n",
      "   'wb': Write binary\n",
      "   'ab': Append binary\n",
      "\n",
      "2. Mode demonstrations:\n",
      "   'w' mode: File created with original content\n",
      "   'a' mode: Content appended\n",
      "   'r' mode: Read content:\n",
      "'Original content\\nAppended content\\n'\n",
      "   'r+' mode: Added content at end\n",
      "   'x' mode: File already exists\n",
      "   'x' mode: Failed as expected (file exists)\n",
      "\n",
      "3. Character encoding:\n",
      "   UTF-8: Unicode text written\n",
      "   UTF-8: Read back: Hello! üêç Python supports Unicode: Œ±Œ≤Œ≥Œ¥Œµ ‰∏≠Êñá ÿßŸÑÿπÿ±ÿ®Ÿäÿ© üåç\n",
      "   utf-8: Success - Hello, caf√©!\n",
      "   latin-1: Success - Hello, caf√©!\n",
      "   ascii: Encode error - 'ascii' codec can't encode character '\\xe9' in position 10: ordinal not in range(128)\n",
      "\n",
      "4. Encoding error handling:\n",
      "   Strict mode: 'ascii' codec can't encode character '\\xe9' in position 3: ordinal not in range(128)\n",
      "   Ignore errors: 'Caf nave rsum '\n",
      "   Replace errors: 'Caf? na?ve r?sum? ??'\n",
      "\n",
      "5. Detecting file encoding:\n",
      "   System default encoding: cp1252\n",
      "   Python default encoding: utf-8 (since Python 3.0)\n",
      "   File encoding: utf-8\n",
      "   File mode: r\n",
      "   File name: unicode_test.txt\n",
      "   File closed: False\n",
      "   File closed after context: True\n"
     ]
    }
   ],
   "source": [
    "# File modes and encoding\n",
    "print(\"File Modes and Encoding:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Different file modes\n",
    "print(\"1. File modes:\")\n",
    "file_modes = {\n",
    "    'r': 'Read only (default)',\n",
    "    'w': 'Write only (truncates existing file)',\n",
    "    'a': 'Append only',\n",
    "    'x': 'Create new file (fails if exists)',\n",
    "    'r+': 'Read and write',\n",
    "    'w+': 'Read and write (truncates)',\n",
    "    'a+': 'Read and append',\n",
    "    'rb': 'Read binary',\n",
    "    'wb': 'Write binary',\n",
    "    'ab': 'Append binary'\n",
    "}\n",
    "\n",
    "for mode, description in file_modes.items():\n",
    "    print(f\"   '{mode}': {description}\")\n",
    "\n",
    "# Demonstrate different modes\n",
    "print(\"\\n2. Mode demonstrations:\")\n",
    "\n",
    "# Write mode (creates new or overwrites)\n",
    "with open('mode_test.txt', 'w') as file:\n",
    "    file.write(\"Original content\\n\")\n",
    "print(\"   'w' mode: File created with original content\")\n",
    "\n",
    "# Append mode\n",
    "with open('mode_test.txt', 'a') as file:\n",
    "    file.write(\"Appended content\\n\")\n",
    "print(\"   'a' mode: Content appended\")\n",
    "\n",
    "# Read mode\n",
    "with open('mode_test.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "print(f\"   'r' mode: Read content:\\n{repr(content)}\")\n",
    "\n",
    "# r+ mode (read and write)\n",
    "with open('mode_test.txt', 'r+') as file:\n",
    "    original = file.read()\n",
    "    file.seek(0, 2)  # Go to end of file\n",
    "    file.write(\"Added with r+\\n\")\n",
    "    file.seek(0)  # Go to beginning\n",
    "    modified = file.read()\n",
    "print(f\"   'r+' mode: Added content at end\")\n",
    "\n",
    "# x mode (exclusive creation)\n",
    "try:\n",
    "    with open('exclusive_test.txt', 'x') as file:\n",
    "        file.write(\"Created exclusively\\n\")\n",
    "    print(\"   'x' mode: File created exclusively\")\n",
    "except FileExistsError:\n",
    "    print(\"   'x' mode: File already exists\")\n",
    "\n",
    "# Try x mode again (should fail)\n",
    "try:\n",
    "    with open('exclusive_test.txt', 'x') as file:\n",
    "        file.write(\"This won't work\\n\")\n",
    "except FileExistsError:\n",
    "    print(\"   'x' mode: Failed as expected (file exists)\")\n",
    "\n",
    "# Encoding examples\n",
    "print(\"\\n3. Character encoding:\")\n",
    "\n",
    "# UTF-8 encoding (default in Python 3)\n",
    "text_with_unicode = \"Hello! üêç Python supports Unicode: Œ±Œ≤Œ≥Œ¥Œµ ‰∏≠Êñá ÿßŸÑÿπÿ±ÿ®Ÿäÿ© üåç\"\n",
    "\n",
    "with open('unicode_test.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(text_with_unicode)\n",
    "print(\"   UTF-8: Unicode text written\")\n",
    "\n",
    "# Read with UTF-8\n",
    "with open('unicode_test.txt', 'r', encoding='utf-8') as file:\n",
    "    read_unicode = file.read()\n",
    "print(f\"   UTF-8: Read back: {read_unicode}\")\n",
    "\n",
    "# Different encodings\n",
    "encodings_to_test = ['utf-8', 'latin-1', 'ascii']\n",
    "test_text = \"Hello, caf√©!\"\n",
    "\n",
    "for encoding in encodings_to_test:\n",
    "    filename = f'encoding_{encoding.replace(\"-\", \"_\")}.txt'\n",
    "    try:\n",
    "        with open(filename, 'w', encoding=encoding) as file:\n",
    "            file.write(test_text)\n",
    "        with open(filename, 'r', encoding=encoding) as file:\n",
    "            read_back = file.read()\n",
    "        print(f\"   {encoding}: Success - {read_back}\")\n",
    "    except UnicodeEncodeError as e:\n",
    "        print(f\"   {encoding}: Encode error - {e}\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"   {encoding}: Decode error - {e}\")\n",
    "\n",
    "# Handle encoding errors\n",
    "print(\"\\n4. Encoding error handling:\")\n",
    "problematic_text = \"Caf√© na√Øve r√©sum√© ‰∏≠Êñá\"\n",
    "\n",
    "# Strict (default) - raises exception\n",
    "try:\n",
    "    with open('ascii_test.txt', 'w', encoding='ascii') as file:\n",
    "        file.write(problematic_text)\n",
    "except UnicodeEncodeError as e:\n",
    "    print(f\"   Strict mode: {e}\")\n",
    "\n",
    "# Ignore errors\n",
    "with open('ascii_ignore.txt', 'w', encoding='ascii', errors='ignore') as file:\n",
    "    file.write(problematic_text)\n",
    "with open('ascii_ignore.txt', 'r', encoding='ascii') as file:\n",
    "    ignored_result = file.read()\n",
    "print(f\"   Ignore errors: '{ignored_result}'\")\n",
    "\n",
    "# Replace errors\n",
    "with open('ascii_replace.txt', 'w', encoding='ascii', errors='replace') as file:\n",
    "    file.write(problematic_text)\n",
    "with open('ascii_replace.txt', 'r', encoding='ascii') as file:\n",
    "    replaced_result = file.read()\n",
    "print(f\"   Replace errors: '{replaced_result}'\")\n",
    "\n",
    "# Check file encoding\n",
    "print(\"\\n5. Detecting file encoding:\")\n",
    "import locale\n",
    "print(f\"   System default encoding: {locale.getpreferredencoding()}\")\n",
    "print(f\"   Python default encoding: utf-8 (since Python 3.0)\")\n",
    "\n",
    "# File object properties\n",
    "with open('unicode_test.txt', 'r', encoding='utf-8') as file:\n",
    "    print(f\"   File encoding: {file.encoding}\")\n",
    "    print(f\"   File mode: {file.mode}\")\n",
    "    print(f\"   File name: {file.name}\")\n",
    "    print(f\"   File closed: {file.closed}\")\n",
    "\n",
    "print(f\"   File closed after context: {file.closed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Binary Files and Advanced Operations\n",
    "\n",
    "Working with binary data and advanced file operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Files and Advanced Operations:\n",
      "=====================================\n",
      "1. Binary file operations:\n",
      "   Binary data: b'\\x00\\x01\\x02\\x03\\xff\\xfe\\xfdABC'\n",
      "   As list: [0, 1, 2, 3, 255, 254, 253, 65, 66, 67]\n",
      "   Binary data written to file\n",
      "   Binary data read back: b'\\x00\\x01\\x02\\x03\\xff\\xfe\\xfdABC'\n",
      "   Data matches: True\n",
      "\n",
      "2. Different binary data types:\n",
      "   Text: 'Hello, World!'\n",
      "   As bytes: b'Hello, World!'\n",
      "   Back to text: 'Hello, World!'\n",
      "   Bytearray: bytearray(b'mutable')\n",
      "   Modified: bytearray(b'Mutable')\n",
      "   As string: 'Mutable'\n",
      "\n",
      "3. Integer and bytes conversion:\n",
      "   Number: 1000\n",
      "   As bytes (big-endian): b'\\x00\\x00\\x03\\xe8'\n",
      "   Back to int: 1000\n",
      "   As bytes (little-endian): b'\\xe8\\x03\\x00\\x00'\n",
      "   Back to int: 1000\n",
      "\n",
      "4. File copying:\n",
      "   Copied source.txt to destination.txt\n",
      "   Copy successful: True\n",
      "\n",
      "5. Processing large files in chunks:\n",
      "   Large file created (1000 lines)\n",
      "   Processed: 1000 lines, 52786 characters\n",
      "\n",
      "6. Random file access:\n",
      "   Created file with 10 fixed-size records\n",
      "   Record 0: 'Record 00:        \n",
      "R'\n",
      "   Record 5: 'Record 05:        \n",
      "R'\n",
      "   Record 9: 'Record 09:'\n",
      "\n",
      "7. Memory-mapped files:\n",
      "   File size: 1000 bytes\n",
      "   First 20 bytes: b'01234567890123456789'\n",
      "   After modification: b'HELLO567890123456789'\n",
      "   File content after mmap: 'HELLO567890123456789'\n",
      "\n",
      "Binary file operations completed!\n"
     ]
    }
   ],
   "source": [
    "# Binary files and advanced operations\n",
    "print(\"Binary Files and Advanced Operations:\")\n",
    "print(\"=\" * 37)\n",
    "\n",
    "# Working with binary files\n",
    "print(\"1. Binary file operations:\")\n",
    "\n",
    "# Create some binary data\n",
    "binary_data = bytes([0, 1, 2, 3, 255, 254, 253, 65, 66, 67])  # Mix of values and ASCII\n",
    "print(f\"   Binary data: {binary_data}\")\n",
    "print(f\"   As list: {list(binary_data)}\")\n",
    "\n",
    "# Write binary data\n",
    "with open('binary_test.bin', 'wb') as file:\n",
    "    file.write(binary_data)\n",
    "print(\"   Binary data written to file\")\n",
    "\n",
    "# Read binary data\n",
    "with open('binary_test.bin', 'rb') as file:\n",
    "    read_binary = file.read()\n",
    "print(f\"   Binary data read back: {read_binary}\")\n",
    "print(f\"   Data matches: {binary_data == read_binary}\")\n",
    "\n",
    "# Working with different binary data types\n",
    "print(\"\\n2. Different binary data types:\")\n",
    "\n",
    "# Bytes from string\n",
    "text = \"Hello, World!\"\n",
    "text_bytes = text.encode('utf-8')\n",
    "print(f\"   Text: '{text}'\")\n",
    "print(f\"   As bytes: {text_bytes}\")\n",
    "print(f\"   Back to text: '{text_bytes.decode('utf-8')}'\")\n",
    "\n",
    "# Bytearray (mutable bytes)\n",
    "ba = bytearray(b\"mutable\")\n",
    "print(f\"   Bytearray: {ba}\")\n",
    "ba[0] = ord('M')  # Change first character to 'M'\n",
    "print(f\"   Modified: {ba}\")\n",
    "print(f\"   As string: '{ba.decode()}'\")\n",
    "\n",
    "# Working with integers and bytes\n",
    "print(\"\\n3. Integer and bytes conversion:\")\n",
    "number = 1000\n",
    "number_bytes = number.to_bytes(4, byteorder='big')\n",
    "print(f\"   Number: {number}\")\n",
    "print(f\"   As bytes (big-endian): {number_bytes}\")\n",
    "print(f\"   Back to int: {int.from_bytes(number_bytes, byteorder='big')}\")\n",
    "\n",
    "# Little-endian\n",
    "number_bytes_little = number.to_bytes(4, byteorder='little')\n",
    "print(f\"   As bytes (little-endian): {number_bytes_little}\")\n",
    "print(f\"   Back to int: {int.from_bytes(number_bytes_little, byteorder='little')}\")\n",
    "\n",
    "# File copying (binary)\n",
    "print(\"\\n4. File copying:\")\n",
    "\n",
    "# Create a test file with mixed content\n",
    "test_content = \"Mixed content: text and binary data\\n\" + \"\\x00\\x01\\x02\\xff\\xfe\\xfd\"\n",
    "with open('source.txt', 'w', encoding='utf-8') as file:\n",
    "    # Note: This won't work for pure binary, but demonstrates concept\n",
    "    file.write(\"Mixed content: text and binary markers\")\n",
    "\n",
    "# Copy file byte by byte\n",
    "def copy_file(source, destination):\n",
    "    \"\"\"Copy file using binary mode\"\"\"\n",
    "    with open(source, 'rb') as src:\n",
    "        with open(destination, 'wb') as dst:\n",
    "            while True:\n",
    "                chunk = src.read(1024)  # Read in chunks\n",
    "                if not chunk:\n",
    "                    break\n",
    "                dst.write(chunk)\n",
    "    print(f\"   Copied {source} to {destination}\")\n",
    "\n",
    "copy_file('source.txt', 'destination.txt')\n",
    "\n",
    "# Verify copy\n",
    "with open('source.txt', 'rb') as f1, open('destination.txt', 'rb') as f2:\n",
    "    original = f1.read()\n",
    "    copied = f2.read()\n",
    "    print(f\"   Copy successful: {original == copied}\")\n",
    "\n",
    "# Working with file chunks\n",
    "print(\"\\n5. Processing large files in chunks:\")\n",
    "\n",
    "# Create a larger test file\n",
    "large_content = ''.join(\"Line {}\\n\".format(i) for i in range(1000))\n",
    "with open('large_file.txt', 'w') as file:\n",
    "    for i in range(1000):\n",
    "        file.write(f\"Line {i+1}: This is line number {i+1} with some content.\\n\")\n",
    "print(\"   Large file created (1000 lines)\")\n",
    "\n",
    "# Process file in chunks\n",
    "def process_file_in_chunks(filename, chunk_size=1024):\n",
    "    \"\"\"Process file in chunks to handle large files\"\"\"\n",
    "    line_count = 0\n",
    "    char_count = 0\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        while True:\n",
    "            chunk = file.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            char_count += len(chunk)\n",
    "            line_count += chunk.count('\\n')\n",
    "    \n",
    "    return line_count, char_count\n",
    "\n",
    "lines, chars = process_file_in_chunks('large_file.txt')\n",
    "print(f\"   Processed: {lines} lines, {chars} characters\")\n",
    "\n",
    "# File seeking and random access\n",
    "print(\"\\n6. Random file access:\")\n",
    "\n",
    "# Create a file with numbered records\n",
    "record_size = 20  # Fixed record size\n",
    "with open('records.txt', 'w') as file:\n",
    "    for i in range(10):\n",
    "        record = f\"Record {i:02d}:{' '*8}\\n\"  # Pad to fixed size\n",
    "        file.write(record)\n",
    "\n",
    "print(f\"   Created file with {10} fixed-size records\")\n",
    "\n",
    "# Random access to specific record\n",
    "def read_record(filename, record_number, record_size):\n",
    "    \"\"\"Read specific record by number\"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        file.seek(record_number * record_size)\n",
    "        return file.read(record_size).strip()\n",
    "\n",
    "for record_num in [0, 5, 9]:\n",
    "    record_data = read_record('records.txt', record_num, record_size)\n",
    "    print(f\"   Record {record_num}: '{record_data}'\")\n",
    "\n",
    "# Memory-mapped files (for very large files)\n",
    "print(\"\\n7. Memory-mapped files:\")\n",
    "import mmap\n",
    "\n",
    "# Create a test file\n",
    "with open('mmap_test.txt', 'w') as file:\n",
    "    file.write(\"0123456789\" * 100)  # 1000 characters\n",
    "\n",
    "# Use memory mapping\n",
    "with open('mmap_test.txt', 'r+b') as file:\n",
    "    with mmap.mmap(file.fileno(), 0) as mmapped_file:\n",
    "        print(f\"   File size: {len(mmapped_file)} bytes\")\n",
    "        print(f\"   First 20 bytes: {mmapped_file[:20]}\")\n",
    "        \n",
    "        # Modify through memory map\n",
    "        mmapped_file[0:5] = b'HELLO'\n",
    "        mmapped_file.flush()\n",
    "        \n",
    "        print(f\"   After modification: {mmapped_file[:20]}\")\n",
    "\n",
    "# Verify modification\n",
    "with open('mmap_test.txt', 'r') as file:\n",
    "    content = file.read(20)\n",
    "    print(f\"   File content after mmap: '{content}'\")\n",
    "\n",
    "print(\"\\nBinary file operations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. File System Operations\n",
    "\n",
    "Working with directories, paths, and file metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File System Operations:\n",
      "=======================\n",
      "1. Path operations:\n",
      "   Current directory: 5. Error Handling and File Operations (Topics 16-18)\n",
      "   Parent directory: python\n",
      "   Joined path: C:\\Users\\vansh\\Documents\\JYPUTER NOTEBOOK\\git repo\\python\\5. Error Handling and File Operations (Topics 16-18)\\test_folder\\subdir\\file.txt\n",
      "   Directory: C:\\Users\\vansh\\Documents\\JYPUTER NOTEBOOK\\git repo\\python\\5. Error Handling and File Operations (Topics 16-18)\\test_folder\\subdir\n",
      "   Filename: file.txt\n",
      "   Extension: .txt\n",
      "   Without extension: C:\\Users\\vansh\\Documents\\JYPUTER NOTEBOOK\\git repo\\python\\5. Error Handling and File Operations (Topics 16-18)\\test_folder\\subdir\\file\n",
      "\n",
      "2. Pathlib (modern approach):\n",
      "   Current path: 5. Error Handling and File Operations (Topics 16-18)\n",
      "   Parent: python\n",
      "   Home directory: C:\\Users\\vansh\n",
      "   Pathlib joined: test_folder\\subdir\\file.txt\n",
      "   Parts: ('test_folder', 'subdir', 'file.txt')\n",
      "   Suffix: .txt\n",
      "   Stem: file\n",
      "   Is absolute: False\n",
      "\n",
      "3. Directory operations:\n",
      "   Created: test_directory_structure\\subdir1\n",
      "   Created: test_directory_structure\\subdir2\n",
      "   Created: test_directory_structure\\subdir1\\nested\n",
      "   Created file: test_directory_structure\\file1.txt\n",
      "   Created file: test_directory_structure\\file2.py\n",
      "   Created file: test_directory_structure\\subdir1\\nested_file.txt\n",
      "   Created file: test_directory_structure\\subdir2\\another_file.py\n",
      "\n",
      "4. Directory listing:\n",
      "   os.listdir: ['file1.txt', 'file2.py', 'subdir1', 'subdir2']\n",
      "   pathlib contents:\n",
      "     FILE: file1.txt\n",
      "     FILE: file2.py\n",
      "     DIR: subdir1\n",
      "     DIR: subdir2\n",
      "\n",
      "5. Recursive directory walking:\n",
      "   os.walk:\n",
      "  test_directory_structure/\n",
      "    file1.txt\n",
      "    file2.py\n",
      "    subdir1/\n",
      "      nested_file.txt\n",
      "      nested/\n",
      "    subdir2/\n",
      "      another_file.py\n",
      "\n",
      "   pathlib recursive:\n",
      "     FILE: file1.txt\n",
      "     FILE: file2.py\n",
      "     FILE: subdir1\\nested_file.txt\n",
      "     FILE: subdir2\\another_file.py\n",
      "\n",
      "6. Pattern matching:\n",
      "   Python files: ['file2.py', 'another_file.py']\n",
      "   Text files: ['file1.txt', 'nested_file.txt']\n",
      "   All .py files in current area: 9 files\n",
      "\n",
      "7. File metadata:\n",
      "   File: file1.txt\n",
      "   Size: 20 bytes\n",
      "   Modified: 2025-08-01 18:09:10.989698\n",
      "   Is file: True\n",
      "   Is directory: False\n",
      "   Permissions: 0o100666\n",
      "   Exists (os.path): True\n",
      "   Size (os.path): 20 bytes\n",
      "   Modified (os.path): 2025-08-01 18:09:10.989698\n",
      "\n",
      "8. File permissions:\n",
      "   Current permissions: 0o100666\n",
      "   Is readable: True\n",
      "   Is writable: True\n",
      "   Is executable: True\n",
      "   Permissions changed to 644\n",
      "\n",
      "9. File operations:\n",
      "   Copied file1.txt to file1_copy.txt\n",
      "   Moved file1_copy.txt to renamed_file.txt\n",
      "\n",
      "10. Cleanup operations:\n",
      "   Deleted file: renamed_file.txt\n",
      "   Deleted directory tree: test_directory_structure\n",
      "\n",
      "11. Temporary files:\n",
      "   Created temporary file: tmpi_41bhww.txt\n",
      "   Temporary file content: 'Temporary content'\n",
      "   Temporary file deleted\n",
      "   Temporary directory: tmp145sqgib\n",
      "   Contents: [WindowsPath('C:/Users/vansh/AppData/Local/Temp/tmp145sqgib/test.txt')]\n",
      "   Temporary directory automatically deleted\n",
      "\n",
      "File system operations completed!\n"
     ]
    }
   ],
   "source": [
    "# File system operations\n",
    "import os\n",
    "import pathlib\n",
    "import glob\n",
    "import shutil\n",
    "import stat\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"File System Operations:\")\n",
    "print(\"=\" * 23)\n",
    "\n",
    "# Working with paths\n",
    "print(\"1. Path operations:\")\n",
    "\n",
    "# Using os.path (traditional)\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "print(f\"   Current directory: {os.path.basename(current_dir)}\")\n",
    "print(f\"   Parent directory: {os.path.basename(parent_dir)}\")\n",
    "\n",
    "# Join paths safely\n",
    "test_path = os.path.join(current_dir, 'test_folder', 'subdir', 'file.txt')\n",
    "print(f\"   Joined path: {test_path}\")\n",
    "\n",
    "# Path components\n",
    "print(f\"   Directory: {os.path.dirname(test_path)}\")\n",
    "print(f\"   Filename: {os.path.basename(test_path)}\")\n",
    "print(f\"   Extension: {os.path.splitext(test_path)[1]}\")\n",
    "print(f\"   Without extension: {os.path.splitext(test_path)[0]}\")\n",
    "\n",
    "# Using pathlib (modern approach)\n",
    "print(\"\\n2. Pathlib (modern approach):\")\n",
    "path = pathlib.Path.cwd()\n",
    "print(f\"   Current path: {path.name}\")\n",
    "print(f\"   Parent: {path.parent.name}\")\n",
    "print(f\"   Home directory: {pathlib.Path.home()}\")\n",
    "\n",
    "# Path operations with pathlib\n",
    "test_pathlib = pathlib.Path('test_folder') / 'subdir' / 'file.txt'\n",
    "print(f\"   Pathlib joined: {test_pathlib}\")\n",
    "print(f\"   Parts: {test_pathlib.parts}\")\n",
    "print(f\"   Suffix: {test_pathlib.suffix}\")\n",
    "print(f\"   Stem: {test_pathlib.stem}\")\n",
    "print(f\"   Is absolute: {test_pathlib.is_absolute()}\")\n",
    "\n",
    "# Directory operations\n",
    "print(\"\\n3. Directory operations:\")\n",
    "\n",
    "# Create directory structure\n",
    "test_dir = pathlib.Path('test_directory_structure')\n",
    "subdirs = ['subdir1', 'subdir2', 'subdir1/nested']\n",
    "\n",
    "for subdir in subdirs:\n",
    "    full_path = test_dir / subdir\n",
    "    full_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"   Created: {full_path}\")\n",
    "\n",
    "# Create some test files\n",
    "test_files = [\n",
    "    test_dir / 'file1.txt',\n",
    "    test_dir / 'file2.py',\n",
    "    test_dir / 'subdir1' / 'nested_file.txt',\n",
    "    test_dir / 'subdir2' / 'another_file.py'\n",
    "]\n",
    "\n",
    "for file_path in test_files:\n",
    "    file_path.write_text(f\"Content of {file_path.name}\")\n",
    "    print(f\"   Created file: {file_path}\")\n",
    "\n",
    "# List directory contents\n",
    "print(\"\\n4. Directory listing:\")\n",
    "\n",
    "# Using os.listdir\n",
    "contents = os.listdir(test_dir)\n",
    "print(f\"   os.listdir: {contents}\")\n",
    "\n",
    "# Using pathlib\n",
    "print(\"   pathlib contents:\")\n",
    "for item in test_dir.iterdir():\n",
    "    item_type = \"DIR\" if item.is_dir() else \"FILE\"\n",
    "    print(f\"     {item_type}: {item.name}\")\n",
    "\n",
    "# Recursive directory walking\n",
    "print(\"\\n5. Recursive directory walking:\")\n",
    "\n",
    "# Using os.walk\n",
    "print(\"   os.walk:\")\n",
    "for root, dirs, files in os.walk(test_dir):\n",
    "    level = root.replace(str(test_dir), '').count(os.sep)\n",
    "    indent = '  ' * (level + 1)\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = '  ' * (level + 2)\n",
    "    for file in files:\n",
    "        print(f\"{subindent}{file}\")\n",
    "\n",
    "# Using pathlib recursively\n",
    "print(\"\\n   pathlib recursive:\")\n",
    "for item in test_dir.rglob('*'):\n",
    "    if item.is_file():\n",
    "        relative = item.relative_to(test_dir)\n",
    "        print(f\"     FILE: {relative}\")\n",
    "\n",
    "# Pattern matching with glob\n",
    "print(\"\\n6. Pattern matching:\")\n",
    "\n",
    "# Find all .py files\n",
    "py_files = list(test_dir.glob('**/*.py'))\n",
    "print(f\"   Python files: {[f.name for f in py_files]}\")\n",
    "\n",
    "# Find all .txt files\n",
    "txt_files = list(test_dir.rglob('*.txt'))\n",
    "print(f\"   Text files: {[f.name for f in txt_files]}\")\n",
    "\n",
    "# Using glob module\n",
    "import glob as glob_module\n",
    "all_py_files = glob_module.glob('**/*.py', recursive=True)\n",
    "print(f\"   All .py files in current area: {len(all_py_files)} files\")\n",
    "\n",
    "# File metadata and properties\n",
    "print(\"\\n7. File metadata:\")\n",
    "\n",
    "test_file = test_dir / 'file1.txt'\n",
    "if test_file.exists():\n",
    "    # Using pathlib\n",
    "    stat_info = test_file.stat()\n",
    "    print(f\"   File: {test_file.name}\")\n",
    "    print(f\"   Size: {stat_info.st_size} bytes\")\n",
    "    print(f\"   Modified: {datetime.fromtimestamp(stat_info.st_mtime)}\")\n",
    "    print(f\"   Is file: {test_file.is_file()}\")\n",
    "    print(f\"   Is directory: {test_file.is_dir()}\")\n",
    "    print(f\"   Permissions: {oct(stat_info.st_mode)}\")\n",
    "    \n",
    "    # Using os.path\n",
    "    print(f\"   Exists (os.path): {os.path.exists(test_file)}\")\n",
    "    print(f\"   Size (os.path): {os.path.getsize(test_file)} bytes\")\n",
    "    print(f\"   Modified (os.path): {datetime.fromtimestamp(os.path.getmtime(test_file))}\")\n",
    "\n",
    "# File permissions\n",
    "print(\"\\n8. File permissions:\")\n",
    "if test_file.exists():\n",
    "    # Get current permissions\n",
    "    current_mode = test_file.stat().st_mode\n",
    "    print(f\"   Current permissions: {oct(current_mode)}\")\n",
    "    \n",
    "    # Check specific permissions\n",
    "    print(f\"   Is readable: {os.access(test_file, os.R_OK)}\")\n",
    "    print(f\"   Is writable: {os.access(test_file, os.W_OK)}\")\n",
    "    print(f\"   Is executable: {os.access(test_file, os.X_OK)}\")\n",
    "    \n",
    "    # Change permissions (be careful!)\n",
    "    try:\n",
    "        test_file.chmod(0o644)  # rw-r--r--\n",
    "        print(f\"   Permissions changed to 644\")\n",
    "    except PermissionError:\n",
    "        print(f\"   Permission denied changing file permissions\")\n",
    "\n",
    "# File operations\n",
    "print(\"\\n9. File operations:\")\n",
    "\n",
    "# Copy files\n",
    "source_file = test_dir / 'file1.txt'\n",
    "dest_file = test_dir / 'file1_copy.txt'\n",
    "\n",
    "# Using shutil\n",
    "shutil.copy2(source_file, dest_file)  # copy2 preserves metadata\n",
    "print(f\"   Copied {source_file.name} to {dest_file.name}\")\n",
    "\n",
    "# Move/rename files\n",
    "old_name = dest_file\n",
    "new_name = test_dir / 'renamed_file.txt'\n",
    "shutil.move(old_name, new_name)\n",
    "print(f\"   Moved {old_name.name} to {new_name.name}\")\n",
    "\n",
    "# Delete files and directories\n",
    "print(\"\\n10. Cleanup operations:\")\n",
    "\n",
    "# Delete a file\n",
    "if new_name.exists():\n",
    "    new_name.unlink()  # pathlib way\n",
    "    print(f\"   Deleted file: {new_name.name}\")\n",
    "\n",
    "# Delete directory and contents\n",
    "if test_dir.exists():\n",
    "    shutil.rmtree(test_dir)  # Recursive delete\n",
    "    print(f\"   Deleted directory tree: {test_dir.name}\")\n",
    "\n",
    "# Temporary files and directories\n",
    "print(\"\\n11. Temporary files:\")\n",
    "import tempfile\n",
    "\n",
    "# Temporary file\n",
    "with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as tmp:\n",
    "    tmp.write(\"Temporary content\")\n",
    "    tmp_name = tmp.name\n",
    "print(f\"   Created temporary file: {os.path.basename(tmp_name)}\")\n",
    "\n",
    "# Read from temporary file\n",
    "with open(tmp_name, 'r') as tmp:\n",
    "    content = tmp.read()\n",
    "print(f\"   Temporary file content: '{content}'\")\n",
    "\n",
    "# Clean up temporary file\n",
    "os.unlink(tmp_name)\n",
    "print(f\"   Temporary file deleted\")\n",
    "\n",
    "# Temporary directory\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    tmp_path = pathlib.Path(tmp_dir)\n",
    "    test_file = tmp_path / 'test.txt'\n",
    "    test_file.write_text('Test content')\n",
    "    print(f\"   Temporary directory: {tmp_path.name}\")\n",
    "    print(f\"   Contents: {list(tmp_path.iterdir())}\")\n",
    "\n",
    "print(f\"   Temporary directory automatically deleted\")\n",
    "\n",
    "print(\"\\nFile system operations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Working with Different File Formats\n",
    "\n",
    "Handling CSV, JSON, and other common file formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with Different File Formats:\n",
      "====================================\n",
      "1. CSV (Comma-Separated Values):\n",
      "   CSV file written with employee data\n",
      "   Read 6 rows from CSV\n",
      "   Header: ['Name', 'Age', 'Department', 'Salary']\n",
      "   First employee: ['Alice Johnson', '28', 'Engineering', '75000']\n",
      "\n",
      "   Using DictReader/DictWriter:\n",
      "   Dictionary-based CSV written\n",
      "     Frank Miller: 33 years, IT, $72000\n",
      "     Grace Lee: 27 years, Design, $68000\n",
      "     Henry Kim: 39 years, Finance, $75000\n",
      "\n",
      "2. JSON (JavaScript Object Notation):\n",
      "   JSON file written with company data\n",
      "   Company: Tech Corp\n",
      "   Number of employees: 2\n",
      "   First employee: Alice Johnson\n",
      "   JSON with custom date encoding written\n",
      "   Pretty-printed JSON sample:\n",
      "{\n",
      "  \"id\": 1,\n",
      "  \"name\": \"Alice Johnson\",\n",
      "  \"position\": \"Senior Developer\",\n",
      "  \"skills\": [\n",
      "    \"Python\",\n",
      "    \"JavaScript\",\n",
      "    \"SQL\"\n",
      "  ],\n",
      "  \"active\": true,\n",
      "  \"hire_date\": \"2020-01-15\"\n",
      "}\n",
      "\n",
      "3. Configuration Files (INI format):\n",
      "   Configuration file written\n",
      "   Database host: localhost\n",
      "   API timeout: 60\n",
      "   Default timeout: 30\n",
      "   Sections: ['database', 'api']\n",
      "   database options: ['host', 'port', 'name', 'user', 'debug', 'timeout']\n",
      "   api options: ['base_url', 'version', 'timeout', 'debug']\n",
      "\n",
      "4. Pickle (Python object serialization):\n",
      "   Complex data pickled\n",
      "   Loaded people: [Person('Alice', 28, ['reading', 'coding']), Person('Bob', 35, ['music', 'sports'])]\n",
      "   Loaded timestamp: 2025-08-01 18:09:11.171039\n",
      "   Object types preserved: <class '__main__.Person'>\n",
      "   ‚ö†Ô∏è  Warning: Only pickle files from trusted sources!\n",
      "\n",
      "5. Efficient handling of large files:\n",
      "   Created large CSV file (10,000 rows)\n",
      "   Processed 10000 rows with 10 unique categories\n",
      "\n",
      "File Format Summary:\n",
      "   CSV: Tabular data, widely supported, human-readable\n",
      "   JSON: Structured data, web APIs, human-readable\n",
      "   INI: Configuration files, simple key-value pairs\n",
      "   Pickle: Python objects, not human-readable, Python-specific\n",
      "   Binary: Raw data, images, executables, most efficient\n",
      "   Text: Plain text, logs, documents, human-readable\n",
      "\n",
      "File format operations completed!\n"
     ]
    }
   ],
   "source": [
    "# Working with different file formats\n",
    "import csv\n",
    "import json\n",
    "import configparser\n",
    "import pickle\n",
    "from datetime import datetime, date\n",
    "\n",
    "print(\"Working with Different File Formats:\")\n",
    "print(\"=\" * 36)\n",
    "\n",
    "# 1. CSV Files\n",
    "print(\"1. CSV (Comma-Separated Values):\")\n",
    "\n",
    "# Sample data\n",
    "employees = [\n",
    "    ['Name', 'Age', 'Department', 'Salary'],\n",
    "    ['Alice Johnson', 28, 'Engineering', 75000],\n",
    "    ['Bob Smith', 35, 'Marketing', 65000],\n",
    "    ['Charlie Brown', 42, 'Sales', 70000],\n",
    "    ['Diana Wilson', 31, 'Engineering', 80000],\n",
    "    ['Eve Davis', 29, 'HR', 60000]\n",
    "]\n",
    "\n",
    "# Write CSV file\n",
    "with open('employees.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(employees)\n",
    "print(\"   CSV file written with employee data\")\n",
    "\n",
    "# Read CSV file\n",
    "with open('employees.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    csv_data = list(reader)\n",
    "    print(f\"   Read {len(csv_data)} rows from CSV\")\n",
    "    print(f\"   Header: {csv_data[0]}\")\n",
    "    print(f\"   First employee: {csv_data[1]}\")\n",
    "\n",
    "# Working with CSV DictReader/DictWriter\n",
    "print(\"\\n   Using DictReader/DictWriter:\")\n",
    "\n",
    "# Write using DictWriter\n",
    "employee_dicts = [\n",
    "    {'name': 'Frank Miller', 'age': 33, 'dept': 'IT', 'salary': 72000},\n",
    "    {'name': 'Grace Lee', 'age': 27, 'dept': 'Design', 'salary': 68000},\n",
    "    {'name': 'Henry Kim', 'age': 39, 'dept': 'Finance', 'salary': 75000}\n",
    "]\n",
    "\n",
    "with open('employees_dict.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['name', 'age', 'dept', 'salary']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(employee_dicts)\n",
    "print(\"   Dictionary-based CSV written\")\n",
    "\n",
    "# Read using DictReader\n",
    "with open('employees_dict.csv', 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        print(f\"     {row['name']}: {row['age']} years, {row['dept']}, ${row['salary']}\")\n",
    "\n",
    "# 2. JSON Files\n",
    "print(\"\\n2. JSON (JavaScript Object Notation):\")\n",
    "\n",
    "# Sample data structure\n",
    "company_data = {\n",
    "    'company': 'Tech Corp',\n",
    "    'founded': 2010,\n",
    "    'employees': [\n",
    "        {\n",
    "            'id': 1,\n",
    "            'name': 'Alice Johnson',\n",
    "            'position': 'Senior Developer',\n",
    "            'skills': ['Python', 'JavaScript', 'SQL'],\n",
    "            'active': True,\n",
    "            'hire_date': '2020-01-15'\n",
    "        },\n",
    "        {\n",
    "            'id': 2,\n",
    "            'name': 'Bob Smith',\n",
    "            'position': 'Product Manager',\n",
    "            'skills': ['Project Management', 'Analytics'],\n",
    "            'active': True,\n",
    "            'hire_date': '2019-03-22'\n",
    "        }\n",
    "    ],\n",
    "    'locations': {\n",
    "        'headquarters': 'San Francisco',\n",
    "        'offices': ['New York', 'London', 'Tokyo']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write JSON file\n",
    "with open('company.json', 'w') as jsonfile:\n",
    "    json.dump(company_data, jsonfile, indent=2)\n",
    "print(\"   JSON file written with company data\")\n",
    "\n",
    "# Read JSON file\n",
    "with open('company.json', 'r') as jsonfile:\n",
    "    loaded_data = json.load(jsonfile)\n",
    "    print(f\"   Company: {loaded_data['company']}\")\n",
    "    print(f\"   Number of employees: {len(loaded_data['employees'])}\")\n",
    "    print(f\"   First employee: {loaded_data['employees'][0]['name']}\")\n",
    "\n",
    "# JSON with custom encoder (for dates)\n",
    "class DateTimeEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (datetime, date)):\n",
    "            return obj.isoformat()\n",
    "        return super().default(obj)\n",
    "\n",
    "data_with_dates = {\n",
    "    'timestamp': datetime.now(),\n",
    "    'date': date.today(),\n",
    "    'info': 'Data with dates'\n",
    "}\n",
    "\n",
    "with open('data_with_dates.json', 'w') as jsonfile:\n",
    "    json.dump(data_with_dates, jsonfile, cls=DateTimeEncoder, indent=2)\n",
    "print(\"   JSON with custom date encoding written\")\n",
    "\n",
    "# Pretty print JSON\n",
    "print(\"   Pretty-printed JSON sample:\")\n",
    "print(json.dumps(loaded_data['employees'][0], indent=2))\n",
    "\n",
    "# 3. Configuration Files (INI format)\n",
    "print(\"\\n3. Configuration Files (INI format):\")\n",
    "\n",
    "# Create configuration\n",
    "config = configparser.ConfigParser()\n",
    "config['DEFAULT'] = {\n",
    "    'debug': 'False',\n",
    "    'timeout': '30'\n",
    "}\n",
    "config['database'] = {\n",
    "    'host': 'localhost',\n",
    "    'port': '5432',\n",
    "    'name': 'myapp',\n",
    "    'user': 'admin'\n",
    "}\n",
    "config['api'] = {\n",
    "    'base_url': 'https://api.example.com',\n",
    "    'version': 'v2',\n",
    "    'timeout': '60'  # Overrides DEFAULT\n",
    "}\n",
    "\n",
    "# Write configuration file\n",
    "with open('config.ini', 'w') as configfile:\n",
    "    config.write(configfile)\n",
    "print(\"   Configuration file written\")\n",
    "\n",
    "# Read configuration file\n",
    "config_read = configparser.ConfigParser()\n",
    "config_read.read('config.ini')\n",
    "\n",
    "print(f\"   Database host: {config_read['database']['host']}\")\n",
    "print(f\"   API timeout: {config_read['api']['timeout']}\")\n",
    "print(f\"   Default timeout: {config_read['DEFAULT']['timeout']}\")\n",
    "\n",
    "# List all sections and options\n",
    "print(f\"   Sections: {config_read.sections()}\")\n",
    "for section in config_read.sections():\n",
    "    print(f\"   {section} options: {list(config_read[section].keys())}\")\n",
    "\n",
    "# 4. Pickle (Python object serialization)\n",
    "print(\"\\n4. Pickle (Python object serialization):\")\n",
    "\n",
    "# Complex Python object\n",
    "class Person:\n",
    "    def __init__(self, name, age, hobbies):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.hobbies = hobbies\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Person('{self.name}', {self.age}, {self.hobbies})\"\n",
    "\n",
    "complex_data = {\n",
    "    'people': [\n",
    "        Person('Alice', 28, ['reading', 'coding']),\n",
    "        Person('Bob', 35, ['music', 'sports'])\n",
    "    ],\n",
    "    'numbers': [1, 2, 3, 4, 5],\n",
    "    'nested': {'inner': {'deep': 'value'}},\n",
    "    'timestamp': datetime.now()\n",
    "}\n",
    "\n",
    "# Write pickle file\n",
    "with open('complex_data.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(complex_data, picklefile)\n",
    "print(\"   Complex data pickled\")\n",
    "\n",
    "# Read pickle file\n",
    "with open('complex_data.pkl', 'rb') as picklefile:\n",
    "    loaded_complex = pickle.load(picklefile)\n",
    "    print(f\"   Loaded people: {loaded_complex['people']}\")\n",
    "    print(f\"   Loaded timestamp: {loaded_complex['timestamp']}\")\n",
    "    print(f\"   Object types preserved: {type(loaded_complex['people'][0])}\")\n",
    "\n",
    "print(\"   ‚ö†Ô∏è  Warning: Only pickle files from trusted sources!\")\n",
    "\n",
    "# 5. Working with large files efficiently\n",
    "print(\"\\n5. Efficient handling of large files:\")\n",
    "\n",
    "# Create a large CSV for demonstration\n",
    "with open('large_data.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['id', 'value', 'category'])\n",
    "    for i in range(10000):\n",
    "        writer.writerow([i, f'value_{i}', f'cat_{i % 10}'])\n",
    "print(\"   Created large CSV file (10,000 rows)\")\n",
    "\n",
    "# Process large file in chunks\n",
    "def process_large_csv(filename, chunk_size=1000):\n",
    "    \"\"\"Process large CSV file in chunks\"\"\"\n",
    "    total_rows = 0\n",
    "    categories = set()\n",
    "    \n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        chunk = []\n",
    "        \n",
    "        for row in reader:\n",
    "            chunk.append(row)\n",
    "            categories.add(row['category'])\n",
    "            \n",
    "            if len(chunk) >= chunk_size:\n",
    "                # Process chunk\n",
    "                total_rows += len(chunk)\n",
    "                chunk = []\n",
    "        \n",
    "        # Process remaining chunk\n",
    "        if chunk:\n",
    "            total_rows += len(chunk)\n",
    "    \n",
    "    return total_rows, len(categories)\n",
    "\n",
    "rows, unique_cats = process_large_csv('large_data.csv')\n",
    "print(f\"   Processed {rows} rows with {unique_cats} unique categories\")\n",
    "\n",
    "# File format summary\n",
    "print(\"\\nFile Format Summary:\")\n",
    "formats = {\n",
    "    'CSV': 'Tabular data, widely supported, human-readable',\n",
    "    'JSON': 'Structured data, web APIs, human-readable',\n",
    "    'INI': 'Configuration files, simple key-value pairs',\n",
    "    'Pickle': 'Python objects, not human-readable, Python-specific',\n",
    "    'Binary': 'Raw data, images, executables, most efficient',\n",
    "    'Text': 'Plain text, logs, documents, human-readable'\n",
    "}\n",
    "\n",
    "for fmt, description in formats.items():\n",
    "    print(f\"   {fmt}: {description}\")\n",
    "\n",
    "print(\"\\nFile format operations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned about:\n",
    "\n",
    "‚úÖ **Basic File Operations**: Opening, reading, writing, and closing files  \n",
    "‚úÖ **File Modes**: Different modes and their purposes, encoding handling  \n",
    "‚úÖ **Binary Files**: Working with binary data and advanced operations  \n",
    "‚úÖ **File System**: Directory operations, paths, and metadata  \n",
    "‚úÖ **File Formats**: CSV, JSON, INI, Pickle, and format-specific operations  \n",
    "‚úÖ **Best Practices**: Context managers, error handling, and efficiency  \n",
    "\n",
    "### Key Takeaways:\n",
    "1. Always use context managers (with statement) for file operations\n",
    "2. Specify encoding explicitly when working with text files\n",
    "3. Use pathlib for modern path operations\n",
    "4. Process large files in chunks to manage memory\n",
    "5. Choose appropriate file formats for your data\n",
    "6. Handle file operations errors gracefully\n",
    "\n",
    "### Next Topic: 19_oop_basics.ipynb\n",
    "Learn about Object-Oriented Programming fundamentals in Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
