{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 30: Web Scraping for Data Collection\n",
    "\n",
    "## Overview\n",
    "Web scraping is the process of automatically extracting data from websites[7][23]. Python offers powerful libraries like Requests and Beautiful Soup that make web scraping accessible and efficient for data collection.\n",
    "\n",
    "### What You'll Learn:\n",
    "- Web scraping fundamentals and ethics\n",
    "- HTTP requests with the Requests library\n",
    "- HTML parsing with Beautiful Soup\n",
    "- Handling different data formats (JSON, XML)\n",
    "- Managing sessions, cookies, and authentication\n",
    "- Best practices and common challenges\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Web Scraping Fundamentals\n",
    "\n",
    "Understanding the basics of web scraping and HTTP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping Fundamentals:\n",
      "==========================\n",
      "1. What is Web Scraping?\n",
      "   1. Send HTTP request to target website\n",
      "   2. Download HTML content from server\n",
      "   3. Parse HTML to extract desired information\n",
      "   4. Clean and structure the extracted data\n",
      "   5. Store data in desired format (CSV, JSON, database)\n",
      "\n",
      "2. Web Scraping Ethics and Legality:\n",
      "   ✓ Check robots.txt: Always check website's robots.txt file\n",
      "   ✓ Respect rate limits: Don't overload servers with requests\n",
      "   ✓ Read Terms of Service: Understand website's usage policies\n",
      "   ✓ Public data only: Only scrape publicly available information\n",
      "   ✓ Give attribution: Credit data sources appropriately\n",
      "   ⚠️  User-Agent headers: Identify your scraper appropriately\n",
      "   ⚠️  Handle errors gracefully: Don't break on failed requests\n",
      "\n",
      "3. HTTP Basics for Web Scraping:\n",
      "   GET Request: Retrieve data from server\n",
      "   POST Request: Send data to server\n",
      "   Headers: Metadata about the request/response\n",
      "   Status Codes: 200 (OK), 404 (Not Found), 403 (Forbidden), 500 (Server Error)\n",
      "   Cookies: Store session information\n",
      "   User-Agent: Identifies the client making the request\n",
      "\n",
      "4. Making basic HTTP requests:\n",
      "   Request to https://httpbin.org/html:\n",
      "   Status Code: 503\n",
      "   Content Type: text/html\n",
      "   Response Length: 162 characters\n",
      "   First 200 characters: <html>\n",
      "<head><title>503 Service Temporarily Unavailable</title></head>\n",
      "<body>\n",
      "<center><h1>503 Service Temporarily Unavailable</h1></center>\n",
      "</body>\n",
      "</html>\n",
      "...\n",
      "\n",
      "5. HTTP Headers:\n",
      "   Request Headers (what we send):\n",
      "     User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWeb...\n",
      "     Accept: text/html,application/xhtml+xml,application/xml;q=...\n",
      "     Accept-Language: en-US,en;q=0.5\n",
      "     Accept-Encoding: gzip, deflate\n",
      "     Connection: keep-alive\n",
      "\n",
      "   Response Headers (what we receive):\n",
      "     Server: awselb/2.0\n",
      "     Date: Fri, 01 Aug 2025 12:58:37 GMT\n",
      "     Content-Type: text/html\n",
      "     Content-Length: 162\n",
      "     Connection: keep-alive\n",
      "\n",
      "6. Common Web Scraping Libraries:\n",
      "   requests: HTTP library for making web requests\n",
      "   urllib: Built-in Python URL handling\n",
      "   BeautifulSoup: HTML/XML parsing and navigation\n",
      "   lxml: Fast XML/HTML parser\n",
      "   selenium: Browser automation for JavaScript-heavy sites\n",
      "   scrapy: Framework for large-scale web scraping\n",
      "   pandas: Data manipulation and CSV/JSON handling\n",
      "\n",
      "7. Web Scraping Challenges:\n",
      "   1. JavaScript-rendered content\n",
      "   2. Anti-bot measures (CAPTCHAs, rate limiting)\n",
      "   3. Dynamic content and AJAX calls\n",
      "   4. Session management and authentication\n",
      "   5. Handling different encodings\n",
      "   6. Dealing with malformed HTML\n",
      "   7. Respecting robots.txt and rate limits\n",
      "   8. Legal and ethical considerations\n",
      "\n",
      "8. When NOT to use web scraping:\n",
      "   ✓ Official APIs are available\n",
      "   ✓ Data is available in structured formats (CSV, JSON)\n",
      "   ✓ Website prohibits scraping in Terms of Service\n",
      "   ✓ Real-time data feeds exist\n",
      "   ✓ Partner or commercial data sources available\n",
      "\n",
      "   Always prefer official APIs when available!\n"
     ]
    }
   ],
   "source": [
    "# Web Scraping Fundamentals\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Web Scraping Fundamentals:\")\n",
    "print(\"=\" * 26)\n",
    "\n",
    "print(\"1. What is Web Scraping?\")\n",
    "web_scraping_steps = [\n",
    "    \"1. Send HTTP request to target website\",\n",
    "    \"2. Download HTML content from server\", \n",
    "    \"3. Parse HTML to extract desired information\",\n",
    "    \"4. Clean and structure the extracted data\",\n",
    "    \"5. Store data in desired format (CSV, JSON, database)\"\n",
    "]\n",
    "\n",
    "for step in web_scraping_steps:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "print(f\"\\n2. Web Scraping Ethics and Legality:\")\n",
    "ethical_guidelines = {\n",
    "    \"✓ Check robots.txt\": \"Always check website's robots.txt file\",\n",
    "    \"✓ Respect rate limits\": \"Don't overload servers with requests\", \n",
    "    \"✓ Read Terms of Service\": \"Understand website's usage policies\",\n",
    "    \"✓ Public data only\": \"Only scrape publicly available information\",\n",
    "    \"✓ Give attribution\": \"Credit data sources appropriately\",\n",
    "    \"⚠️  User-Agent headers\": \"Identify your scraper appropriately\",\n",
    "    \"⚠️  Handle errors gracefully\": \"Don't break on failed requests\"\n",
    "}\n",
    "\n",
    "for guideline, description in ethical_guidelines.items():\n",
    "    print(f\"   {guideline}: {description}\")\n",
    "\n",
    "print(f\"\\n3. HTTP Basics for Web Scraping:\")\n",
    "http_concepts = {\n",
    "    \"GET Request\": \"Retrieve data from server\",\n",
    "    \"POST Request\": \"Send data to server\", \n",
    "    \"Headers\": \"Metadata about the request/response\",\n",
    "    \"Status Codes\": \"200 (OK), 404 (Not Found), 403 (Forbidden), 500 (Server Error)\",\n",
    "    \"Cookies\": \"Store session information\",\n",
    "    \"User-Agent\": \"Identifies the client making the request\"\n",
    "}\n",
    "\n",
    "for concept, description in http_concepts.items():\n",
    "    print(f\"   {concept}: {description}\")\n",
    "\n",
    "# Basic HTTP request example\n",
    "print(f\"\\n4. Making basic HTTP requests:\")\n",
    "\n",
    "# Example: Get a simple webpage (using httpbin for testing)\n",
    "test_url = \"https://httpbin.org/html\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(test_url)\n",
    "    print(f\"   Request to {test_url}:\")\n",
    "    print(f\"   Status Code: {response.status_code}\")\n",
    "    print(f\"   Content Type: {response.headers.get('content-type')}\")\n",
    "    print(f\"   Response Length: {len(response.text)} characters\")\n",
    "    print(f\"   First 200 characters: {response.text[:200]}...\")\n",
    "except requests.RequestException as e:\n",
    "    print(f\"   Error making request: {e}\")\n",
    "\n",
    "# Headers examination\n",
    "print(f\"\\n5. HTTP Headers:\")\n",
    "print(f\"   Request Headers (what we send):\")\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "for header, value in headers.items():\n",
    "    print(f\"     {header}: {value[:50]}{'...' if len(value) > 50 else ''}\")\n",
    "\n",
    "try:\n",
    "    response_with_headers = requests.get(test_url, headers=headers)\n",
    "    print(f\"\\n   Response Headers (what we receive):\")\n",
    "    for header, value in list(response_with_headers.headers.items())[:5]:\n",
    "        print(f\"     {header}: {value}\")\n",
    "except:\n",
    "    print(f\"   Could not fetch headers\")\n",
    "\n",
    "print(f\"\\n6. Common Web Scraping Libraries:\")\n",
    "libraries = {\n",
    "    \"requests\": \"HTTP library for making web requests\",\n",
    "    \"urllib\": \"Built-in Python URL handling\",\n",
    "    \"BeautifulSoup\": \"HTML/XML parsing and navigation\", \n",
    "    \"lxml\": \"Fast XML/HTML parser\",\n",
    "    \"selenium\": \"Browser automation for JavaScript-heavy sites\",\n",
    "    \"scrapy\": \"Framework for large-scale web scraping\",\n",
    "    \"pandas\": \"Data manipulation and CSV/JSON handling\"\n",
    "}\n",
    "\n",
    "for library, description in libraries.items():\n",
    "    print(f\"   {library}: {description}\")\n",
    "\n",
    "print(f\"\\n7. Web Scraping Challenges:\")\n",
    "challenges = [\n",
    "    \"JavaScript-rendered content\",\n",
    "    \"Anti-bot measures (CAPTCHAs, rate limiting)\", \n",
    "    \"Dynamic content and AJAX calls\",\n",
    "    \"Session management and authentication\",\n",
    "    \"Handling different encodings\",\n",
    "    \"Dealing with malformed HTML\",\n",
    "    \"Respecting robots.txt and rate limits\",\n",
    "    \"Legal and ethical considerations\"\n",
    "]\n",
    "\n",
    "for i, challenge in enumerate(challenges, 1):\n",
    "    print(f\"   {i}. {challenge}\")\n",
    "\n",
    "print(f\"\\n8. When NOT to use web scraping:\")\n",
    "alternatives = [\n",
    "    \"✓ Official APIs are available\",\n",
    "    \"✓ Data is available in structured formats (CSV, JSON)\", \n",
    "    \"✓ Website prohibits scraping in Terms of Service\",\n",
    "    \"✓ Real-time data feeds exist\",\n",
    "    \"✓ Partner or commercial data sources available\"\n",
    "]\n",
    "\n",
    "for alternative in alternatives:\n",
    "    print(f\"   {alternative}\")\n",
    "\n",
    "print(f\"\\n   Always prefer official APIs when available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HTML Parsing with Beautiful Soup\n",
    "\n",
    "Extracting data from HTML using Beautiful Soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Parsing with Beautiful Soup:\n",
      "=================================\n",
      "1. Creating BeautifulSoup object:\n",
      "   Parsed HTML document\n",
      "   Title: Sample E-commerce Page\n",
      "   Document type: <class 'bs4.BeautifulSoup'>\n",
      "\n",
      "2. Basic element selection:\n",
      "   Finding by tag:\n",
      "   First h1: Online Store\n",
      "   All h2 tags: ['Laptop Computer', 'Wireless Headphones', 'Coffee Maker']\n",
      "\n",
      "   Finding by class:\n",
      "   All prices: ['$999.99', '$149.99', '$79.99']\n",
      "   Product with data-id='1': Laptop Computer\n",
      "\n",
      "3. CSS Selectors:\n",
      "   Using CSS selectors:\n",
      "   Product titles: ['Laptop Computer', 'Wireless Headphones', 'Coffee Maker']\n",
      "   Categories: ['Electronics', 'Electronics', 'Appliances']\n",
      "   Electronics products found: 2 (using CSS selectors)\n",
      "   Electronics products: ['Laptop Computer', 'Wireless Headphones']\n",
      "\n",
      "4. Extracting structured data:\n",
      "   Extracted 3 products:\n",
      "   - Laptop Computer: $999.99 (4 stars, 245 reviews)\n",
      "   - Wireless Headphones: $149.99 (5 stars, 89 reviews)\n",
      "   - Coffee Maker: $79.99 (3 stars, 156 reviews)\n",
      "\n",
      "   DataFrame shape: (3, 9)\n",
      "  id                title   price                                description  \\\n",
      "0  1      Laptop Computer  999.99  High-performance laptop for professionals   \n",
      "1  2  Wireless Headphones  149.99        Premium noise-cancelling headphones   \n",
      "2  3         Coffee Maker   79.99                Automatic drip coffee maker   \n",
      "\n",
      "      category  stars  review_count         image_src            image_alt  \n",
      "0  Electronics      4           245        laptop.jpg      Laptop Computer  \n",
      "1  Electronics      5            89    headphones.jpg  Wireless Headphones  \n",
      "2   Appliances      3           156  coffee-maker.jpg         Coffee Maker  \n",
      "\n",
      "5. Advanced parsing techniques:\n",
      "   Handling missing elements:\n",
      "   Safe extraction example:\n",
      "     Title: Laptop Computer\n",
      "     Price: $999.99\n",
      "     Missing element: None\n",
      "\n",
      "6. Extracting links and navigation:\n",
      "   Found 3 links:\n",
      "     Home: /home\n",
      "     Products: /products\n",
      "     About: /about\n",
      "\n",
      "7. Extracting contact information:\n",
      "   Contact Information:\n",
      "     Email: Email: info@store.com\n",
      "     Phone: Phone: (555) 123-4567\n",
      "     Address: 123 Main StreetCity, State 12345\n",
      "\n",
      "8. Saving extracted data:\n",
      "   ✓ Saved products to 'scraped_products.csv'\n",
      "   ✓ Saved products to 'scraped_products.json'\n",
      "\n",
      "9. Beautiful Soup best practices:\n",
      "   ✓ Always specify a parser ('html.parser', 'lxml', 'html5lib')\n",
      "   ✓ Handle missing elements gracefully with try/except\n",
      "   ✓ Use CSS selectors for complex selections\n",
      "   ✓ Extract data into structured formats (dicts, DataFrames)\n",
      "   ✓ Clean and validate extracted data\n",
      "   ✓ Use get_text(strip=True) to clean whitespace\n",
      "   ✓ Check for None values before accessing attributes\n"
     ]
    }
   ],
   "source": [
    "# HTML Parsing with Beautiful Soup\n",
    "print(\"HTML Parsing with Beautiful Soup:\")\n",
    "print(\"=\" * 33)\n",
    "\n",
    "# Sample HTML for demonstration\n",
    "sample_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sample E-commerce Page</title>\n",
    "    <meta charset=\"UTF-8\">\n",
    "</head>\n",
    "<body>\n",
    "    <header>\n",
    "        <h1>Online Store</h1>\n",
    "        <nav>\n",
    "            <ul>\n",
    "                <li><a href=\"/home\">Home</a></li>\n",
    "                <li><a href=\"/products\">Products</a></li>\n",
    "                <li><a href=\"/about\">About</a></li>\n",
    "            </ul>\n",
    "        </nav>\n",
    "    </header>\n",
    "    \n",
    "    <main>\n",
    "        <div class=\"products\">\n",
    "            <div class=\"product\" data-id=\"1\">\n",
    "                <h2 class=\"product-title\">Laptop Computer</h2>\n",
    "                <p class=\"price\">$999.99</p>\n",
    "                <p class=\"description\">High-performance laptop for professionals</p>\n",
    "                <span class=\"category\">Electronics</span>\n",
    "                <div class=\"rating\">\n",
    "                    <span class=\"stars\">★★★★☆</span>\n",
    "                    <span class=\"review-count\">(245 reviews)</span>\n",
    "                </div>\n",
    "                <img src=\"laptop.jpg\" alt=\"Laptop Computer\">\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"product\" data-id=\"2\">\n",
    "                <h2 class=\"product-title\">Wireless Headphones</h2>\n",
    "                <p class=\"price\">$149.99</p>\n",
    "                <p class=\"description\">Premium noise-cancelling headphones</p>\n",
    "                <span class=\"category\">Electronics</span>\n",
    "                <div class=\"rating\">\n",
    "                    <span class=\"stars\">★★★★★</span>\n",
    "                    <span class=\"review-count\">(89 reviews)</span>\n",
    "                </div>\n",
    "                <img src=\"headphones.jpg\" alt=\"Wireless Headphones\">\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"product\" data-id=\"3\">\n",
    "                <h2 class=\"product-title\">Coffee Maker</h2>\n",
    "                <p class=\"price\">$79.99</p>\n",
    "                <p class=\"description\">Automatic drip coffee maker</p>\n",
    "                <span class=\"category\">Appliances</span>\n",
    "                <div class=\"rating\">\n",
    "                    <span class=\"stars\">★★★☆☆</span>\n",
    "                    <span class=\"review-count\">(156 reviews)</span>\n",
    "                </div>\n",
    "                <img src=\"coffee-maker.jpg\" alt=\"Coffee Maker\">\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <aside>\n",
    "            <div class=\"contact-info\">\n",
    "                <h3>Contact Us</h3>\n",
    "                <p>Email: info@store.com</p>\n",
    "                <p>Phone: (555) 123-4567</p>\n",
    "                <address>\n",
    "                    123 Main Street<br>\n",
    "                    City, State 12345\n",
    "                </address>\n",
    "            </div>\n",
    "        </aside>\n",
    "    </main>\n",
    "    \n",
    "    <footer>\n",
    "        <p>&copy; 2024 Online Store. All rights reserved.</p>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "print(\"1. Creating BeautifulSoup object:\")\n",
    "\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(sample_html, 'html.parser')\n",
    "print(f\"   Parsed HTML document\")\n",
    "print(f\"   Title: {soup.title.string}\")\n",
    "print(f\"   Document type: {type(soup)}\")\n",
    "\n",
    "print(f\"\\n2. Basic element selection:\")\n",
    "\n",
    "# Find elements by tag\n",
    "print(f\"   Finding by tag:\")\n",
    "print(f\"   First h1: {soup.find('h1').string}\")\n",
    "print(f\"   All h2 tags: {[h2.string for h2 in soup.find_all('h2')]}\")\n",
    "\n",
    "# Find by class\n",
    "print(f\"\\n   Finding by class:\")\n",
    "prices = soup.find_all(class_='price')\n",
    "print(f\"   All prices: {[price.string for price in prices]}\")\n",
    "\n",
    "# Find by ID (if existed)\n",
    "product_by_id = soup.find('div', {'data-id': '1'})\n",
    "print(f\"   Product with data-id='1': {product_by_id.find(class_='product-title').string}\")\n",
    "\n",
    "print(f\"\\n3. CSS Selectors:\")\n",
    "\n",
    "# CSS selector examples\n",
    "print(f\"   Using CSS selectors:\")\n",
    "\n",
    "# Select by class\n",
    "product_titles = soup.select('.product-title')\n",
    "print(f\"   Product titles: {[title.string for title in product_titles]}\")\n",
    "\n",
    "# Select by descendant\n",
    "categories = soup.select('.product .category')\n",
    "print(f\"   Categories: {[cat.string for cat in categories]}\")\n",
    "\n",
    "# Select by attribute\n",
    "electronics = soup.select('.product:has(.category:contains(\"Electronics\"))')\n",
    "print(f\"   Electronics products found: {len(electronics)} (using CSS selectors)\")\n",
    "\n",
    "# More practical approach for filtering\n",
    "electronics_products = []\n",
    "for product in soup.find_all('div', class_='product'):\n",
    "    category = product.find(class_='category')\n",
    "    if category and 'Electronics' in category.string:\n",
    "        title = product.find(class_='product-title').string\n",
    "        electronics_products.append(title)\n",
    "\n",
    "print(f\"   Electronics products: {electronics_products}\")\n",
    "\n",
    "print(f\"\\n4. Extracting structured data:\")\n",
    "\n",
    "# Extract all products into a structured format\n",
    "products_data = []\n",
    "\n",
    "for product in soup.find_all('div', class_='product'):\n",
    "    # Extract basic information\n",
    "    title = product.find(class_='product-title').string\n",
    "    price_text = product.find(class_='price').string\n",
    "    price = float(price_text.replace('$', '').replace(',', ''))\n",
    "    description = product.find(class_='description').string\n",
    "    category = product.find(class_='category').string\n",
    "    \n",
    "    # Extract rating information\n",
    "    stars_element = product.find(class_='stars')\n",
    "    stars_count = len([s for s in stars_element.string if s == '★']) if stars_element else 0\n",
    "    \n",
    "    review_count_element = product.find(class_='review-count')\n",
    "    review_count_text = review_count_element.string if review_count_element else '(0 reviews)'\n",
    "    review_count = int(re.search(r'\\((\\d+)', review_count_text).group(1))\n",
    "    \n",
    "    # Extract image info\n",
    "    img = product.find('img')\n",
    "    image_src = img['src'] if img else None\n",
    "    image_alt = img['alt'] if img else None\n",
    "    \n",
    "    # Get product ID\n",
    "    product_id = product.get('data-id')\n",
    "    \n",
    "    products_data.append({\n",
    "        'id': product_id,\n",
    "        'title': title,\n",
    "        'price': price,\n",
    "        'description': description,\n",
    "        'category': category,\n",
    "        'stars': stars_count,\n",
    "        'review_count': review_count,\n",
    "        'image_src': image_src,\n",
    "        'image_alt': image_alt\n",
    "    })\n",
    "\n",
    "# Display extracted data\n",
    "print(f\"   Extracted {len(products_data)} products:\")\n",
    "for product in products_data:\n",
    "    print(f\"   - {product['title']}: ${product['price']} ({product['stars']} stars, {product['review_count']} reviews)\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "products_df = pd.DataFrame(products_data)\n",
    "print(f\"\\n   DataFrame shape: {products_df.shape}\")\n",
    "print(f\"{products_df}\")\n",
    "\n",
    "print(f\"\\n5. Advanced parsing techniques:\")\n",
    "\n",
    "# Handle missing elements gracefully\n",
    "print(f\"   Handling missing elements:\")\n",
    "\n",
    "def safe_extract(element, selector, attribute=None):\n",
    "    \"\"\"Safely extract data from HTML elements\"\"\"\n",
    "    try:\n",
    "        found = element.select_one(selector)\n",
    "        if found:\n",
    "            if attribute:\n",
    "                return found.get(attribute)\n",
    "            else:\n",
    "                return found.get_text(strip=True)\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Example of safe extraction\n",
    "for product in soup.find_all('div', class_='product')[:1]:  # Just first product\n",
    "    safe_title = safe_extract(product, '.product-title')\n",
    "    safe_price = safe_extract(product, '.price')\n",
    "    safe_missing = safe_extract(product, '.nonexistent-class')\n",
    "    \n",
    "    print(f\"   Safe extraction example:\")\n",
    "    print(f\"     Title: {safe_title}\")\n",
    "    print(f\"     Price: {safe_price}\")\n",
    "    print(f\"     Missing element: {safe_missing}\")\n",
    "\n",
    "# Extract all links\n",
    "print(f\"\\n6. Extracting links and navigation:\")\n",
    "\n",
    "links = soup.find_all('a')\n",
    "print(f\"   Found {len(links)} links:\")\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "    text = link.get_text(strip=True)\n",
    "    print(f\"     {text}: {href}\")\n",
    "\n",
    "# Extract contact information\n",
    "print(f\"\\n7. Extracting contact information:\")\n",
    "contact_section = soup.find('div', class_='contact-info')\n",
    "if contact_section:\n",
    "    email = contact_section.find('p').get_text(strip=True)\n",
    "    phone = contact_section.find_all('p')[1].get_text(strip=True)\n",
    "    address = contact_section.find('address').get_text(strip=True)\n",
    "    \n",
    "    contact_info = {\n",
    "        'email': email,\n",
    "        'phone': phone, \n",
    "        'address': address.replace('\\n', ' ').strip()\n",
    "    }\n",
    "    \n",
    "    print(f\"   Contact Information:\")\n",
    "    for key, value in contact_info.items():\n",
    "        print(f\"     {key.title()}: {value}\")\n",
    "\n",
    "print(f\"\\n8. Saving extracted data:\")\n",
    "\n",
    "# Save to CSV\n",
    "products_df.to_csv('scraped_products.csv', index=False)\n",
    "print(f\"   ✓ Saved products to 'scraped_products.csv'\")\n",
    "\n",
    "# Save to JSON\n",
    "with open('scraped_products.json', 'w') as f:\n",
    "    json.dump(products_data, f, indent=2)\n",
    "print(f\"   ✓ Saved products to 'scraped_products.json'\")\n",
    "\n",
    "print(f\"\\n9. Beautiful Soup best practices:\")\n",
    "print(f\"   ✓ Always specify a parser ('html.parser', 'lxml', 'html5lib')\")\n",
    "print(f\"   ✓ Handle missing elements gracefully with try/except\")\n",
    "print(f\"   ✓ Use CSS selectors for complex selections\")\n",
    "print(f\"   ✓ Extract data into structured formats (dicts, DataFrames)\")\n",
    "print(f\"   ✓ Clean and validate extracted data\")\n",
    "print(f\"   ✓ Use get_text(strip=True) to clean whitespace\")\n",
    "print(f\"   ✓ Check for None values before accessing attributes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Real-World Scraping Examples\n",
    "\n",
    "Practical web scraping scenarios and techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real-World Web Scraping Examples:\n",
      "==================================\n",
      "1. Scraping public APIs and JSON data:\n",
      "   Successfully fetched 100 posts from API\n",
      "   First post: sunt aut facere repellat provident occaecati excepturi optio reprehenderit\n",
      "   DataFrame shape: (100, 4)\n",
      "   Columns: ['userId', 'id', 'title', 'body']\n",
      "\n",
      "   Sample posts:\n",
      "     ID 1: sunt aut facere repellat provident occaecati excep...\n",
      "     ID 2: qui est esse...\n",
      "     ID 3: ea molestias quasi exercitationem repellat qui ips...\n",
      "\n",
      "2. Handling different content types:\n",
      "   JSON Content:\n",
      "     Status: 200\n",
      "     Content-Type: application/json; charset=utf-8\n",
      "     Length: 509 characters\n",
      "     Parsed JSON keys: ['id', 'name', 'username', 'email', 'address', 'phone', 'website', 'company']\n",
      "   XML Content:\n",
      "     Status: 503\n",
      "     Content-Type: text/html\n",
      "     Length: 162 characters\n",
      "   HTML Content:\n",
      "     Status: 503\n",
      "     Content-Type: text/html\n",
      "     Length: 162 characters\n",
      "\n",
      "3. Session management and cookies:\n",
      "   Created session with default headers\n",
      "   Session headers: {'User-Agent': 'Python Web Scraper 1.0', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Accept': 'text/html,application/json', 'Connection': 'keep-alive'}\n",
      "   First request status: 503\n",
      "\n",
      "4. Handling forms and POST requests:\n",
      "\n",
      "5. Rate limiting and respectful scraping:\n",
      "   Testing respectful scraper:\n",
      "   Scraping page 1/2: https://httpbin.org/html\n",
      "     HTTP 503 on attempt 1\n",
      "     HTTP 503 on attempt 2\n",
      "   Scraping page 2/2: https://httpbin.org/delay/1\n",
      "     HTTP 503 on attempt 1\n",
      "     HTTP 503 on attempt 2\n",
      "     https://httpbin.org/html: None - Failed to fetch\n",
      "     https://httpbin.org/delay/1: None - Failed to fetch\n",
      "\n",
      "6. Error handling and robustness:\n",
      "   Testing robust error handling:\n",
      "     Attempt 1: HTTP error - HTTP 503\n",
      "     Attempt 2: HTTP error - HTTP 503\n",
      "     Attempt 3: HTTP error - HTTP 503\n",
      "   Result: {'success': False, 'error': 'All attempts failed', 'attempts': 3}\n",
      "\n",
      "7. Data export and storage:\n",
      "   Saving scraped data in multiple formats:\n",
      "     ✓ Saved to scraping_results.json\n",
      "     ✓ Saved to scraping_results.csv\n",
      "\n",
      "8. Web scraping best practices summary:\n",
      "   ✓ Always check robots.txt before scraping\n",
      "   ✓ Use appropriate delays between requests\n",
      "   ✓ Handle errors gracefully with try/except blocks\n",
      "   ✓ Use sessions for multiple requests to same site\n",
      "   ✓ Set proper User-Agent headers\n",
      "   ✓ Respect rate limits and avoid overwhelming servers\n",
      "   ✓ Cache responses when possible to reduce requests\n",
      "   ✓ Validate and clean extracted data\n",
      "   ✓ Store data in structured formats\n",
      "   ✓ Monitor for website changes that break scrapers\n"
     ]
    }
   ],
   "source": [
    "# Real-World Scraping Examples\n",
    "print(\"Real-World Web Scraping Examples:\")\n",
    "print(\"=\" * 34)\n",
    "\n",
    "print(\"1. Scraping public APIs and JSON data:\")\n",
    "\n",
    "# Example: Using a public API that returns JSON\n",
    "# JSONPlaceholder - a free fake API for testing\n",
    "api_url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "\n",
    "try:\n",
    "    api_response = requests.get(api_url)\n",
    "    \n",
    "    if api_response.status_code == 200:\n",
    "        posts_data = api_response.json()  # Parse JSON directly\n",
    "        print(f\"   Successfully fetched {len(posts_data)} posts from API\")\n",
    "        print(f\"   First post: {posts_data[0]['title']}\")\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        posts_df = pd.DataFrame(posts_data)\n",
    "        print(f\"   DataFrame shape: {posts_df.shape}\")\n",
    "        print(f\"   Columns: {list(posts_df.columns)}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\n   Sample posts:\")\n",
    "        for post in posts_data[:3]:\n",
    "            print(f\"     ID {post['id']}: {post['title'][:50]}...\")\n",
    "    else:\n",
    "        print(f\"   API request failed with status: {api_response.status_code}\")\n",
    "        \n",
    "except requests.RequestException as e:\n",
    "    print(f\"   Error accessing API: {e}\")\n",
    "\n",
    "print(f\"\\n2. Handling different content types:\")\n",
    "\n",
    "# Example of handling different response types\n",
    "test_endpoints = {\n",
    "    \"JSON\": \"https://jsonplaceholder.typicode.com/users/1\",\n",
    "    \"XML\": \"https://httpbin.org/xml\",\n",
    "    \"HTML\": \"https://httpbin.org/html\"\n",
    "}\n",
    "\n",
    "for content_type, url in test_endpoints.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        content_type_header = response.headers.get('content-type', 'unknown')\n",
    "        \n",
    "        print(f\"   {content_type} Content:\")\n",
    "        print(f\"     Status: {response.status_code}\")\n",
    "        print(f\"     Content-Type: {content_type_header}\")\n",
    "        print(f\"     Length: {len(response.text)} characters\")\n",
    "        \n",
    "        if content_type == \"JSON\" and response.status_code == 200:\n",
    "            try:\n",
    "                json_data = response.json()\n",
    "                print(f\"     Parsed JSON keys: {list(json_data.keys()) if isinstance(json_data, dict) else 'List data'}\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"     Failed to parse JSON\")\n",
    "                \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"   Error with {content_type}: {e}\")\n",
    "\n",
    "print(f\"\\n3. Session management and cookies:\")\n",
    "\n",
    "# Using sessions for persistent connections\n",
    "session = requests.Session()\n",
    "\n",
    "# Set default headers for the session\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Python Web Scraper 1.0',\n",
    "    'Accept': 'text/html,application/json'\n",
    "})\n",
    "\n",
    "print(f\"   Created session with default headers\")\n",
    "print(f\"   Session headers: {dict(session.headers)}\")\n",
    "\n",
    "# Example of making multiple requests with same session\n",
    "try:\n",
    "    # First request\n",
    "    response1 = session.get(\"https://httpbin.org/cookies/set/session_id/12345\")\n",
    "    print(f\"   First request status: {response1.status_code}\")\n",
    "    \n",
    "    # Second request - cookies should be maintained\n",
    "    response2 = session.get(\"https://httpbin.org/cookies\")\n",
    "    if response2.status_code == 200:\n",
    "        cookies_data = response2.json()\n",
    "        print(f\"   Cookies maintained: {cookies_data.get('cookies', {})}\")\n",
    "        \n",
    "except requests.RequestException as e:\n",
    "    print(f\"   Session example error: {e}\")\n",
    "\n",
    "print(f\"\\n4. Handling forms and POST requests:\")\n",
    "\n",
    "# Example of submitting form data\n",
    "form_data = {\n",
    "    'name': 'John Doe',\n",
    "    'email': 'john@example.com',\n",
    "    'message': 'Hello from Python web scraper!'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # POST request with form data\n",
    "    post_response = requests.post(\"https://httpbin.org/post\", data=form_data)\n",
    "    \n",
    "    if post_response.status_code == 200:\n",
    "        response_data = post_response.json()\n",
    "        submitted_form = response_data.get('form', {})\n",
    "        print(f\"   Form submission successful\")\n",
    "        print(f\"   Submitted data: {submitted_form}\")\n",
    "        \n",
    "except requests.RequestException as e:\n",
    "    print(f\"   Form submission error: {e}\")\n",
    "\n",
    "print(f\"\\n5. Rate limiting and respectful scraping:\")\n",
    "\n",
    "class RespectfulScraper:\n",
    "    \"\"\"A scraper that implements rate limiting and retry logic\"\"\"\n",
    "    \n",
    "    def __init__(self, delay=1.0, max_retries=3):\n",
    "        self.delay = delay\n",
    "        self.max_retries = max_retries\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Educational Python Scraper 1.0'\n",
    "        })\n",
    "        \n",
    "    def get_page(self, url):\n",
    "        \"\"\"Get a page with rate limiting and retry logic\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                # Rate limiting\n",
    "                time.sleep(self.delay)\n",
    "                \n",
    "                response = self.session.get(url, timeout=10)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    return response\n",
    "                elif response.status_code == 429:  # Too Many Requests\n",
    "                    print(f\"     Rate limited, waiting longer...\")\n",
    "                    time.sleep(self.delay * 3)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"     HTTP {response.status_code} on attempt {attempt + 1}\")\n",
    "                    \n",
    "            except requests.RequestException as e:\n",
    "                print(f\"     Request failed on attempt {attempt + 1}: {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(self.delay * 2)\n",
    "                    \n",
    "        return None\n",
    "    \n",
    "    def scrape_multiple_pages(self, urls):\n",
    "        \"\"\"Scrape multiple pages respectfully\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, url in enumerate(urls):\n",
    "            print(f\"   Scraping page {i+1}/{len(urls)}: {url}\")\n",
    "            \n",
    "            response = self.get_page(url)\n",
    "            if response:\n",
    "                results.append({\n",
    "                    'url': url,\n",
    "                    'status_code': response.status_code,\n",
    "                    'content_length': len(response.text),\n",
    "                    'title': self.extract_title(response.text)\n",
    "                })\n",
    "            else:\n",
    "                results.append({\n",
    "                    'url': url,\n",
    "                    'status_code': None,\n",
    "                    'content_length': 0,\n",
    "                    'title': 'Failed to fetch'\n",
    "                })\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def extract_title(self, html_content):\n",
    "        \"\"\"Extract page title from HTML\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            title_tag = soup.find('title')\n",
    "            return title_tag.string.strip() if title_tag else 'No title found'\n",
    "        except:\n",
    "            return 'Failed to parse title'\n",
    "\n",
    "# Demonstrate respectful scraping\n",
    "scraper = RespectfulScraper(delay=0.5, max_retries=2)\n",
    "\n",
    "# Test URLs\n",
    "test_urls = [\n",
    "    \"https://httpbin.org/html\",\n",
    "    \"https://httpbin.org/delay/1\",\n",
    "    \"https://httpbin.org/status/200\"\n",
    "]\n",
    "\n",
    "print(f\"   Testing respectful scraper:\")\n",
    "scraping_results = scraper.scrape_multiple_pages(test_urls[:2])  # Limit for demo\n",
    "\n",
    "for result in scraping_results:\n",
    "    print(f\"     {result['url']}: {result['status_code']} - {result['title']}\")\n",
    "\n",
    "print(f\"\\n6. Error handling and robustness:\")\n",
    "\n",
    "def robust_scrape(url, max_attempts=3):\n",
    "    \"\"\"Demonstrate robust error handling\"\"\"\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            \n",
    "            # Check status code\n",
    "            if response.status_code != 200:\n",
    "                raise requests.HTTPError(f\"HTTP {response.status_code}\")\n",
    "            \n",
    "            # Check content type\n",
    "            content_type = response.headers.get('content-type', '')\n",
    "            if 'text/html' not in content_type and 'application/json' not in content_type:\n",
    "                raise ValueError(f\"Unexpected content type: {content_type}\")\n",
    "            \n",
    "            # Try to parse\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'title': soup.title.string if soup.title else 'No title',\n",
    "                'text_length': len(response.text),\n",
    "                'attempt': attempt + 1\n",
    "            }\n",
    "            \n",
    "        except requests.Timeout:\n",
    "            print(f\"     Attempt {attempt + 1}: Timeout\")\n",
    "        except requests.ConnectionError:\n",
    "            print(f\"     Attempt {attempt + 1}: Connection error\")\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"     Attempt {attempt + 1}: HTTP error - {e}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"     Attempt {attempt + 1}: Content error - {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"     Attempt {attempt + 1}: Unexpected error - {e}\")\n",
    "        \n",
    "        if attempt < max_attempts - 1:\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return {\n",
    "        'success': False,\n",
    "        'error': 'All attempts failed',\n",
    "        'attempts': max_attempts\n",
    "    }\n",
    "\n",
    "# Test robust scraping\n",
    "print(f\"   Testing robust error handling:\")\n",
    "test_result = robust_scrape(\"https://httpbin.org/html\")\n",
    "print(f\"   Result: {test_result}\")\n",
    "\n",
    "print(f\"\\n7. Data export and storage:\")\n",
    "\n",
    "# Create sample scraped data\n",
    "scraped_data = {\n",
    "    'pages_scraped': len(scraping_results),\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'results': scraping_results\n",
    "}\n",
    "\n",
    "# Save in multiple formats\n",
    "print(f\"   Saving scraped data in multiple formats:\")\n",
    "\n",
    "# JSON export\n",
    "with open('scraping_results.json', 'w') as f:\n",
    "    json.dump(scraped_data, f, indent=2, default=str)\n",
    "print(f\"     ✓ Saved to scraping_results.json\")\n",
    "\n",
    "# CSV export (flattened data)\n",
    "if scraping_results:\n",
    "    results_df = pd.DataFrame(scraping_results)\n",
    "    results_df.to_csv('scraping_results.csv', index=False)\n",
    "    print(f\"     ✓ Saved to scraping_results.csv\")\n",
    "\n",
    "print(f\"\\n8. Web scraping best practices summary:\")\n",
    "best_practices = [\n",
    "    \"✓ Always check robots.txt before scraping\",\n",
    "    \"✓ Use appropriate delays between requests\", \n",
    "    \"✓ Handle errors gracefully with try/except blocks\",\n",
    "    \"✓ Use sessions for multiple requests to same site\",\n",
    "    \"✓ Set proper User-Agent headers\",\n",
    "    \"✓ Respect rate limits and avoid overwhelming servers\",\n",
    "    \"✓ Cache responses when possible to reduce requests\",\n",
    "    \"✓ Validate and clean extracted data\",\n",
    "    \"✓ Store data in structured formats\",\n",
    "    \"✓ Monitor for website changes that break scrapers\"\n",
    "]\n",
    "\n",
    "for practice in best_practices:\n",
    "    print(f\"   {practice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned about:\n",
    "\n",
    "✅ **Web Scraping Fundamentals**: HTTP requests, HTML structure, ethics[7][23]  \n",
    "✅ **Requests Library**: Making HTTP requests, handling responses, sessions  \n",
    "✅ **Beautiful Soup**: HTML parsing, element selection, data extraction[24][27]  \n",
    "✅ **Data Collection**: Structured data extraction and storage formats  \n",
    "✅ **Best Practices**: Rate limiting, error handling, respectful scraping[7][10]  \n",
    "✅ **Real-World Examples**: APIs, forms, multiple content types  \n",
    "\n",
    "### Key Takeaways:\n",
    "1. Web scraping is powerful but must be done ethically and legally[7][23]\n",
    "2. Always prefer official APIs when available over scraping\n",
    "3. Requests and Beautiful Soup provide excellent tools for most scraping needs[24]\n",
    "4. Rate limiting and error handling are crucial for robust scrapers[10]\n",
    "5. Structure extracted data for easy analysis and storage\n",
    "6. Test scrapers thoroughly and handle edge cases gracefully\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
