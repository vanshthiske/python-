{
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# Topic 26: Pandas Basics - Data Manipulation\n",
"\n",
"## Overview\n",
"Pandas is the most popular Python library for data manipulation and analysis[3][6]. It provides powerful data structures like DataFrame and Series for working with structured data[13].\n",
"\n",
"### What You'll Learn:\n",
"- Series and DataFrame structures\n",
"- Data loading and saving\n",
"- Data selection and filtering\n",
"- Data cleaning and transformation\n",
"- Grouping and aggregation\n",
"- Merging and joining data\n",
"\n",
"---"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 1. Pandas Data Structures\n",
"\n",
"Understanding Series and DataFrame:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Pandas data structures\nimport pandas as pd\nimport numpy as np\n\nprint(\"Pandas Data Structures:\")\nprint(\"=\" * 23)\n\n# Series - 1D labeled array\nprint(\"1. Pandas Series:\")\n\n# Create Series from list\nseries_from_list = pd.Series([1, 2, 3, 4, 5])\nprint(f\"   Series from list:\")\nprint(f\"{series_from_list}\")\nprint(f\"   Data type: {type(series_from_list)}\")\n\n# Series with custom index\nseries_custom_index = pd.Series([10, 20, 30, 40], index=['a', 'b', 'c', 'd'])\nprint(f\"\\n   Series with custom index:\")\nprint(f\"{series_custom_index}\")\n\n# Series from dictionary\nseries_from_dict = pd.Series({'apple': 5, 'banana': 3, 'orange': 8, 'grape': 2})\nprint(f\"\\n   Series from dictionary:\")\nprint(f\"{series_from_dict}\")\n\n# Series attributes\nprint(f\"\\n2. Series attributes:\")\nprint(f\"   Values: {series_from_dict.values}\")\nprint(f\"   Index: {series_from_dict.index.tolist()}\")\nprint(f\"   Shape: {series_from_dict.shape}\")\nprint(f\"   Size: {series_from_dict.size}\")\nprint(f\"   Data type: {series_from_dict.dtype}\")\n\n# Series operations\nprint(f\"\\n3. Series operations:\")\nprint(f\"   Original: {series_from_dict.tolist()}\")\nprint(f\"   + 2: {(series_from_dict + 2).tolist()}\")\nprint(f\"   * 3: {(series_from_dict * 3).tolist()}\")\nprint(f\"   > 5: {(series_from_dict > 5).tolist()}\")\n\n# DataFrame - 2D labeled data structure\nprint(f\"\\n4. Pandas DataFrame:\")\n\n# Create DataFrame from dictionary\ndata_dict = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n    'Age': [25, 30, 35, 28, 32],\n    'City': ['New York', 'London', 'Tokyo', 'Paris', 'Sydney'],\n    'Salary': [70000, 80000, 90000, 75000, 85000]\n}\n\ndf = pd.DataFrame(data_dict)\nprint(f\"   DataFrame from dictionary:\")\nprint(f\"{df}\")\n\n# DataFrame from list of lists\ndata_lists = [\n    ['Alice', 25, 'New York', 70000],\n    ['Bob', 30, 'London', 80000],\n    ['Charlie', 35, 'Tokyo', 90000]\n]\n\ndf_from_lists = pd.DataFrame(data_lists, \n                            columns=['Name', 'Age', 'City', 'Salary'])\nprint(f\"\\n   DataFrame from list of lists:\")\nprint(f\"{df_from_lists}\")\n\n# DataFrame attributes\nprint(f\"\\n5. DataFrame attributes:\")\nprint(f\"   Shape: {df.shape}\")\nprint(f\"   Size: {df.size}\")\nprint(f\"   Columns: {df.columns.tolist()}\")\nprint(f\"   Index: {df.index.tolist()}\")\nprint(f\"   Data types:\")\nprint(f\"{df.dtypes}\")\n\n# DataFrame info\nprint(f\"\\n6. DataFrame info:\")\nprint(f\"   Info summary:\")\ndf.info()\n\nprint(f\"\\n   Statistical description:\")\nprint(f\"{df.describe()}\")\n\n# Accessing DataFrame data\nprint(f\"\\n7. Accessing DataFrame data:\")\n\n# Select single column (returns Series)\nprint(f\"   Single column 'Name' (Series):\")\nprint(f\"{df['Name']}\")\nprint(f\"   Type: {type(df['Name'])}\")\n\n# Select multiple columns (returns DataFrame)\nprint(f\"\\n   Multiple columns ['Name', 'Age']:\")\nprint(f\"{df[['Name', 'Age']]}\")\nprint(f\"   Type: {type(df[['Name', 'Age']])}\")\n\n# Select rows by index\nprint(f\"\\n   First row:\")\nprint(f\"{df.iloc[0]}\")\n\n# Select specific rows and columns\nprint(f\"\\n   First 3 rows, columns 'Name' and 'Salary':\")\nprint(f\"{df.loc[0:2, ['Name', 'Salary']]}\")\n\n# Creating DataFrame with custom index\nprint(f\"\\n8. DataFrame with custom index:\")\ndf_custom_index = pd.DataFrame(data_dict, \n                              index=['emp001', 'emp002', 'emp003', 'emp004', 'emp005'])\nprint(f\"   DataFrame with custom index:\")\nprint(f\"{df_custom_index}\")\n\n# Working with missing data\nprint(f\"\\n9. DataFrame with missing data:\")\ndata_with_nan = {\n    'A': [1, 2, np.nan, 4, 5],\n    'B': [np.nan, 2, 3, 4, np.nan],\n    'C': [1, 2, 3, np.nan, 5]\n}\n\ndf_nan = pd.DataFrame(data_with_nan)\nprint(f\"   DataFrame with NaN values:\")\nprint(f\"{df_nan}\")\nprint(f\"\\n   Check for null values:\")\nprint(f\"{df_nan.isnull()}\")\nprint(f\"\\n   Count of null values per column:\")\nprint(f\"{df_nan.isnull().sum()}\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 2. Data Loading and Basic Operations\n",
"\n",
"Reading data from various sources and basic operations:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Data loading and basic operations\nimport pandas as pd\nimport numpy as np\nfrom io import StringIO\n\nprint(\"Data Loading and Basic Operations:\")\nprint(\"=\" * 34)\n\n# Create sample CSV data\ncsv_data = \"\"\"\nName,Age,City,Salary,Department\nAlice,25,New York,70000,Engineering\nBob,30,London,80000,Marketing\nCharlie,35,Tokyo,90000,Engineering\nDiana,28,Paris,75000,HR\nEve,32,Sydney,85000,Marketing\nFrank,29,Berlin,78000,Engineering\nGrace,31,Madrid,82000,Finance\nHenry,27,Rome,72000,HR\nIvy,33,Amsterdam,88000,Finance\nJack,26,Dublin,71000,Marketing\n\"\"\"\n\n# Simulate reading from CSV\nprint(\"1. Loading data from CSV:\")\ndf = pd.read_csv(StringIO(csv_data.strip()))\nprint(f\"   Loaded DataFrame shape: {df.shape}\")\nprint(f\"{df.head()}\")\n\n# Basic DataFrame inspection\nprint(f\"\\n2. Basic DataFrame inspection:\")\nprint(f\"   First 3 rows:\")\nprint(f\"{df.head(3)}\")\nprint(f\"\\n   Last 3 rows:\")\nprint(f\"{df.tail(3)}\")\nprint(f\"\\n   Random sample of 3 rows:\")\nprint(f\"{df.sample(3)}\")\n\n# Column operations\nprint(f\"\\n3. Column operations:\")\nprint(f\"   Column names: {df.columns.tolist()}\")\nprint(f\"   Data types:\")\nfor col, dtype in df.dtypes.items():\n    print(f\"     {col}: {dtype}\")\n\n# Add new column\ndf['Experience_Years'] = np.random.randint(1, 10, len(df))\ndf['Bonus'] = df['Salary'] * 0.1\nprint(f\"\\n   Added columns 'Experience_Years' and 'Bonus'\")\nprint(f\"{df[['Name', 'Salary', 'Experience_Years', 'Bonus']].head()}\")\n\n# Drop columns\ndf_dropped = df.drop(['Bonus'], axis=1)\nprint(f\"\\n   After dropping 'Bonus' column:\")\nprint(f\"   Columns: {df_dropped.columns.tolist()}\")\n\n# Rename columns\ndf_renamed = df.rename(columns={'Name': 'Employee_Name', 'Age': 'Employee_Age'})\nprint(f\"\\n   After renaming columns:\")\nprint(f\"   New columns: {df_renamed.columns.tolist()}\")\n\n# Row operations\nprint(f\"\\n4. Row operations:\")\n\n# Add new row\nnew_row = pd.DataFrame({\n    'Name': ['Kate'],\n    'Age': [29],\n    'City': ['Vienna'],\n    'Salary': [79000],\n    'Department': ['Engineering'],\n    'Experience_Years': [4],\n    'Bonus': [7900]\n})\n\ndf_with_new = pd.concat([df, new_row], ignore_index=True)\nprint(f\"   After adding new row, shape: {df_with_new.shape}\")\nprint(f\"{df_with_new.tail(2)}\")\n\n# Drop rows\ndf_dropped_rows = df.drop([0, 1])  # Drop first two rows\nprint(f\"\\n   After dropping first 2 rows, shape: {df_dropped_rows.shape}\")\n\n# Sorting data\nprint(f\"\\n5. Sorting data:\")\n\n# Sort by single column\ndf_sorted_age = df.sort_values('Age')\nprint(f\"   Sorted by Age (ascending):\")\nprint(f\"{df_sorted_age[['Name', 'Age', 'Salary']].head()}\")\n\n# Sort by multiple columns\ndf_sorted_multi = df.sort_values(['Department', 'Salary'], ascending=[True, False])\nprint(f\"\\n   Sorted by Department (asc), then Salary (desc):\")\nprint(f\"{df_sorted_multi[['Name', 'Department', 'Salary']].head()}\")\n\n# Filtering data\nprint(f\"\\n6. Filtering data:\")\n\n# Single condition\nengineers = df[df['Department'] == 'Engineering']\nprint(f\"   Engineers only:\")\nprint(f\"{engineers[['Name', 'Department', 'Salary']]}\")\n\n# Multiple conditions\nhigh_earners = df[(df['Salary'] > 75000) & (df['Age'] < 30)]\nprint(f\"\\n   High earners under 30:\")\nprint(f\"{high_earners[['Name', 'Age', 'Salary']]}\")\n\n# Filter with isin()\nselected_cities = df[df['City'].isin(['New York', 'London', 'Tokyo'])]\nprint(f\"\\n   Employees in major cities:\")\nprint(f\"{selected_cities[['Name', 'City']]}\")\n\n# String operations\nprint(f\"\\n7. String operations:\")\n\n# String methods\nprint(f\"   Cities starting with 'L':\")\ncities_with_l = df[df['City'].str.startswith('L')]\nprint(f\"{cities_with_l[['Name', 'City']]}\")\n\n# String contains\nprint(f\"\\n   Names containing 'a':\")\nnames_with_a = df[df['Name'].str.contains('a', case=False)]\nprint(f\"{names_with_a[['Name']]}\")\n\n# String length\ndf['Name_Length'] = df['Name'].str.len()\nprint(f\"\\n   Name lengths:\")\nprint(f\"{df[['Name', 'Name_Length']]}\")\n\n# Grouping preview\nprint(f\"\\n8. Basic grouping:\")\n\n# Group by single column\ndept_stats = df.groupby('Department')['Salary'].agg(['mean', 'min', 'max', 'count'])\nprint(f\"   Salary statistics by Department:\")\nprint(f\"{dept_stats.round(2)}\")\n\n# Value counts\nprint(f\"\\n9. Value counts:\")\nprint(f\"   Department distribution:\")\nprint(f\"{df['Department'].value_counts()}\")\n\nprint(f\"\\n   City distribution:\")\nprint(f\"{df['City'].value_counts()}\")\n\n# Basic statistics\nprint(f\"\\n10. Basic statistics:\")\nprint(f\"   Numerical columns summary:\")\nprint(f\"{df.describe()}\")\n\n# Correlation\nprint(f\"\\n   Correlation matrix:\")\ncorr_matrix = df[['Age', 'Salary', 'Experience_Years']].corr()\nprint(f\"{corr_matrix.round(3)}\")\n\nprint(f\"\\n11. Data loading formats:\")\nprint(f\"   ✓ CSV: pd.read_csv('file.csv')\")\nprint(f\"   ✓ Excel: pd.read_excel('file.xlsx')\")\nprint(f\"   ✓ JSON: pd.read_json('file.json')\")\nprint(f\"   ✓ SQL: pd.read_sql('query', connection)\")\nprint(f\"   ✓ HTML: pd.read_html('url')\")\n\nprint(f\"\\n12. Data saving formats:\")\nprint(f\"   ✓ CSV: df.to_csv('file.csv', index=False)\")\nprint(f\"   ✓ Excel: df.to_excel('file.xlsx', index=False)\")\nprint(f\"   ✓ JSON: df.to_json('file.json')\")\nprint(f\"   ✓ SQL: df.to_sql('table', connection)\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## 3. Data Cleaning and Transformation\n",
"\n",
"Handling missing data and transforming datasets:"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Data cleaning and transformation\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\nprint(\"Data Cleaning and Transformation:\")\nprint(\"=\" * 33)\n\n# Create messy dataset for cleaning\nnp.random.seed(42)\nmessy_data = {\n    'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    'Name': ['Alice Johnson', ' bob smith ', 'CHARLIE BROWN', 'diana davis', 'Eve Wilson', None, 'frank miller', 'Grace LEE', 'henry CLARK', ''],\n    'Age': [25, 30, np.nan, 28, 32, 45, 29, np.nan, 27, 35],\n    'Email': ['alice@email.com', 'BOB@EMAIL.COM', 'charlie@email.com', 'invalid-email', 'eve@email.com', 'frank@email.com', None, 'grace@email.com', 'henry@email.com', 'duplicate@email.com'],\n    'Salary': [70000, 80000, 90000, np.nan, 85000, 95000, 78000, 82000, np.nan, 88000],\n    'Join_Date': ['2021-01-15', '2020-03-22', '2019-07-10', '2022-01-05', 'invalid-date', '2020-11-18', '2021-06-30', '2019-12-01', '2022-03-15', '2020-08-25'],\n    'Department': ['Engineering', 'marketing', 'ENGINEERING', 'hr', 'Marketing', 'engineering', 'HR', 'Marketing', 'Engineering', 'HR']\n}\n\ndf_messy = pd.DataFrame(messy_data)\nprint(\"1. Original messy dataset:\")\nprint(f\"{df_messy}\")\n\n# Data quality assessment\nprint(f\"\\n2. Data quality assessment:\")\nprint(f\"   Dataset shape: {df_messy.shape}\")\nprint(f\"   Missing values per column:\")\nprint(f\"{df_messy.isnull().sum()}\")\nprint(f\"\\n   Data types:\")\nprint(f\"{df_messy.dtypes}\")\n\n# Handling missing values\nprint(f\"\\n3. Handling missing values:\")\n\n# Check for different types of missing data\nprint(f\"   Empty strings count: {(df_messy['Name'] == '').sum()}\")\nprint(f\"   None values count: {df_messy['Name'].isnull().sum()}\")\n\n# Replace empty strings with NaN\ndf_cleaned = df_messy.copy()\ndf_cleaned = df_cleaned.replace('', np.nan)\nprint(f\"\\n   After replacing empty strings with NaN:\")\nprint(f\"{df_cleaned.isnull().sum()}\")\n\n# Drop rows with too many missing values\nprint(f\"\\n4. Removing rows with excessive missing data:\")\n# Keep rows with at least 6 non-null values\ndf_cleaned = df_cleaned.dropna(thresh=6)\nprint(f\"   Shape after removing rows with >4 missing values: {df_cleaned.shape}\")\n\n# Fill missing values\nprint(f\"\\n5. Filling missing values:\")\n\n# Fill Age with median\nmedian_age = df_cleaned['Age'].median()\ndf_cleaned['Age'] = df_cleaned['Age'].fillna(median_age)\nprint(f\"   Filled missing Age with median: {median_age}\")\n\n# Fill Salary with mean by Department\nprint(f\"   Filling Salary with department mean:\")\nfor dept in df_cleaned['Department'].unique():\n    if pd.isna(dept):\n        continue\n    dept_mask = df_cleaned['Department'] == dept\n    dept_mean_salary = df_cleaned.loc[dept_mask, 'Salary'].mean()\n    df_cleaned.loc[dept_mask, 'Salary'] = df_cleaned.loc[dept_mask, 'Salary'].fillna(dept_mean_salary)\n\n# Forward fill for remaining missing values\ndf_cleaned['Salary'] = df_cleaned['Salary'].fillna(method='ffill')\n\nprint(f\"   Missing values after filling:\")\nprint(f\"{df_cleaned.isnull().sum()}\")\n\n# String cleaning\nprint(f\"\\n6. String cleaning:\")\n\n# Clean Name column\ndf_cleaned['Name'] = df_cleaned['Name'].str.strip()  # Remove whitespace\ndf_cleaned['Name'] = df_cleaned['Name'].str.title()  # Title case\nprint(f\"   Cleaned Names:\")\nprint(f\"{df_cleaned['Name'].tolist()}\")\n\n# Clean Email column\ndf_cleaned['Email'] = df_cleaned['Email'].str.lower()  # Lowercase emails\nprint(f\"   Cleaned Emails:\")\nprint(f\"{df_cleaned['Email'].tolist()}\")\n\n# Standardize Department names\ndept_mapping = {\n    'engineering': 'Engineering',\n    'marketing': 'Marketing', \n    'hr': 'HR'\n}\ndf_cleaned['Department'] = df_cleaned['Department'].str.lower().map(dept_mapping)\nprint(f\"   Standardized Departments:\")\nprint(f\"{df_cleaned['Department'].value_counts()}\")\n\n# Data type conversion\nprint(f\"\\n7. Data type conversion:\")\n\n# Convert Join_Date to datetime\nprint(f\"   Converting Join_Date to datetime:\")\ndf_cleaned['Join_Date'] = pd.to_datetime(df_cleaned['Join_Date'], errors='coerce')\nprint(f\"   Join_Date type: {df_cleaned['Join_Date'].dtype}\")\nprint(f\"   Invalid dates (NaT): {df_cleaned['Join_Date'].isnull().sum()}\")\n\n# Fill invalid dates with a reasonable default\ndefault_date = pd.to_datetime('2021-01-01')\ndf_cleaned['Join_Date'] = df_cleaned['Join_Date'].fillna(default_date)\n\n# Convert numeric columns\ndf_cleaned['Age'] = df_cleaned['Age'].astype(int)\ndf_cleaned['Salary'] = df_cleaned['Salary'].astype(int)\n\nprint(f\"   Final data types:\")\nprint(f\"{df_cleaned.dtypes}\")\n\n# Remove duplicates\nprint(f\"\\n8. Handling duplicates:\")\nprint(f\"   Original shape: {df_cleaned.shape}\")\n\n# Check for duplicate emails\nduplicate_emails = df_cleaned['Email'].duplicated().sum()\nprint(f\"   Duplicate emails: {duplicate_emails}\")\n\n# Remove rows with duplicate emails (keep first)\ndf_cleaned = df_cleaned.drop_duplicates(subset=['Email'], keep='first')\nprint(f\"   Shape after removing duplicate emails: {df_cleaned.shape}\")\n\n# Data validation\nprint(f\"\\n9. Data validation:\")\n\n# Validate email format\nemail_pattern = r'^[\\w\\.-]+@[\\w\\.-]+\\.[\\w]+$'\nvalid_emails = df_cleaned['Email'].str.match(email_pattern, na=False)\nprint(f\"   Valid email addresses: {valid_emails.sum()}/{len(df_cleaned)}\")\nprint(f\"   Invalid emails:\")\ninvalid_email_mask = ~valid_emails & df_cleaned['Email'].notna()\nif invalid_email_mask.any():\n    print(f\"{df_cleaned.loc[invalid_email_mask, ['Name', 'Email']]}\")\n\n# Validate age range\nvalid_ages = (df_cleaned['Age'] >= 18) & (df_cleaned['Age'] <= 65)\nprint(f\"   Valid ages (18-65): {valid_ages.sum()}/{len(df_cleaned)}\")\n\n# Validate salary range\nvalid_salaries = (df_cleaned['Salary'] >= 30000) & (df_cleaned['Salary'] <= 200000)\nprint(f\"   Valid salaries (30k-200k): {valid_salaries.sum()}/{len(df_cleaned)}\")\n\n# Feature engineering\nprint(f\"\\n10. Feature engineering:\")\n\n# Calculate years since joining\ncurrent_date = datetime.now()\ndf_cleaned['Years_Since_Join'] = (current_date - df_cleaned['Join_Date']).dt.days / 365.25\ndf_cleaned['Years_Since_Join'] = df_cleaned['Years_Since_Join'].round(1)\n\n# Create age categories\ndef categorize_age(age):\n    if age < 25:\n        return 'Young'\n    elif age < 35:\n        return 'Mid-Career'\n    else:\n        return 'Senior'\n\ndf_cleaned['Age_Category'] = df_cleaned['Age'].apply(categorize_age)\n\n# Create salary bands\ndf_cleaned['Salary_Band'] = pd.cut(df_cleaned['Salary'], \n                                  bins=[0, 75000, 85000, float('inf')], \n                                  labels=['Low', 'Medium', 'High'])\n\nprint(f\"   New features created:\")\nprint(f\"{df_cleaned[['Name', 'Age', 'Age_Category', 'Salary', 'Salary_Band', 'Years_Since_Join']]}\")\n\n# Outlier detection\nprint(f\"\\n11. Outlier detection:\")\n\n# Using IQR method for salary\nQ1 = df_cleaned['Salary'].quantile(0.25)\nQ3 = df_cleaned['Salary'].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\noutliers = (df_cleaned['Salary'] < lower_bound) | (df_cleaned['Salary'] > upper_bound)\nprint(f\"   Salary outliers (IQR method): {outliers.sum()}\")\nprint(f\"   Outlier bounds: ${lower_bound:.0f} - ${upper_bound:.0f}\")\n\nif outliers.any():\n    print(f\"   Outlier records:\")\n    print(f\"{df_cleaned.loc[outliers, ['Name', 'Salary']]}\")\n\n# Final cleaned dataset\nprint(f\"\\n12. Final cleaned dataset:\")\nprint(f\"   Shape: {df_cleaned.shape}\")\nprint(f\"   Missing values: {df_cleaned.isnull().sum().sum()}\")\nprint(f\"\\n   Sample of cleaned data:\")\nprint(f\"{df_cleaned.head()}\")\n\n# Data cleaning summary\nprint(f\"\\n13. Data cleaning checklist:\")\nprint(f\"   ✓ Handled missing values (fill, drop, impute)\")\nprint(f\"   ✓ Standardized string formats (case, whitespace)\")\nprint(f\"   ✓ Converted data types appropriately\")\nprint(f\"   ✓ Removed duplicates\")\nprint(f\"   ✓ Validated data ranges and formats\")\nprint(f\"   ✓ Created derived features\")\nprint(f\"   ✓ Detected outliers\")\nprint(f\"   ✓ Documented cleaning process\")\n\n# Export cleaned data\nprint(f\"\\n14. Exporting cleaned data:\")\nprint(f\"   # Save cleaned dataset\")\nprint(f\"   df_cleaned.to_csv('cleaned_data.csv', index=False)\")\nprint(f\"   # Create data dictionary\")\nprint(f\"   data_dict = df_cleaned.dtypes.to_dict()\")\nprint(f\"   print('Data Dictionary:', data_dict)\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Summary\n",
"\n",
"In this notebook, you learned about:\n",
"\n",
"✅ **Pandas Data Structures**: Series and DataFrame for structured data[3][13]  \n",
"✅ **Data Loading**: Reading from CSV, Excel, JSON and other formats[3]  \n",
"✅ **Data Selection**: Indexing, filtering, and querying data effectively  \n",
"✅ **Data Cleaning**: Handling missing values, duplicates, and data validation  \n",
"✅ **Data Transformation**: Type conversion, feature engineering, and standardization  \n",
"✅ **Basic Operations**: Sorting, grouping, and statistical summaries[6]  \n",
"\n",
"### Key Takeaways:\n",
"1. Pandas is built on NumPy and provides high-level data structures[7][13]\n",
"2. DataFrame is like a spreadsheet or SQL table with labeled rows and columns\n",
"3. Always inspect data quality before analysis (missing values, duplicates, outliers)\n",
"4. String methods (.str) provide powerful text processing capabilities\n",
"5. Proper data cleaning is crucial for accurate analysis results[6]\n",
"6. Pandas integrates seamlessly with NumPy and Matplotlib[7]\n",
"\n",
"### Next Topic: 27_matplotlib_basics.ipynb\n",
"Learn about data visualization with Matplotlib for creating plots and charts."
]
}
],
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.8.5"
}
},
"nbformat": 4,
"nbformat_minor": 4
}
