{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 26: Pandas Basics - Data Manipulation\n",
    "\n",
    "## Overview\n",
    "Pandas is the most popular Python library for data manipulation and analysis[3][6]. It provides powerful data structures like DataFrame and Series for working with structured data[13].\n",
    "\n",
    "### What You'll Learn:\n",
    "- Series and DataFrame structures\n",
    "- Data loading and saving\n",
    "- Data selection and filtering\n",
    "- Data cleaning and transformation\n",
    "- Grouping and aggregation\n",
    "- Merging and joining data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pandas Data Structures\n",
    "\n",
    "Understanding Series and DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Data Structures:\n",
      "=======================\n",
      "1. Pandas Series:\n",
      "   Series from list:\n",
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "4    5\n",
      "dtype: int64\n",
      "   Data type: <class 'pandas.core.series.Series'>\n",
      "\n",
      "   Series with custom index:\n",
      "a    10\n",
      "b    20\n",
      "c    30\n",
      "d    40\n",
      "dtype: int64\n",
      "\n",
      "   Series from dictionary:\n",
      "apple     5\n",
      "banana    3\n",
      "orange    8\n",
      "grape     2\n",
      "dtype: int64\n",
      "\n",
      "2. Series attributes:\n",
      "   Values: [5 3 8 2]\n",
      "   Index: ['apple', 'banana', 'orange', 'grape']\n",
      "   Shape: (4,)\n",
      "   Size: 4\n",
      "   Data type: int64\n",
      "\n",
      "3. Series operations:\n",
      "   Original: [5, 3, 8, 2]\n",
      "   + 2: [7, 5, 10, 4]\n",
      "   * 3: [15, 9, 24, 6]\n",
      "   > 5: [False, False, True, False]\n",
      "\n",
      "4. Pandas DataFrame:\n",
      "   DataFrame from dictionary:\n",
      "      Name  Age      City  Salary\n",
      "0    Alice   25  New York   70000\n",
      "1      Bob   30    London   80000\n",
      "2  Charlie   35     Tokyo   90000\n",
      "3    Diana   28     Paris   75000\n",
      "4      Eve   32    Sydney   85000\n",
      "\n",
      "   DataFrame from list of lists:\n",
      "      Name  Age      City  Salary\n",
      "0    Alice   25  New York   70000\n",
      "1      Bob   30    London   80000\n",
      "2  Charlie   35     Tokyo   90000\n",
      "\n",
      "5. DataFrame attributes:\n",
      "   Shape: (5, 4)\n",
      "   Size: 20\n",
      "   Columns: ['Name', 'Age', 'City', 'Salary']\n",
      "   Index: [0, 1, 2, 3, 4]\n",
      "   Data types:\n",
      "Name      object\n",
      "Age        int64\n",
      "City      object\n",
      "Salary     int64\n",
      "dtype: object\n",
      "\n",
      "6. DataFrame info:\n",
      "   Info summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Name    5 non-null      object\n",
      " 1   Age     5 non-null      int64 \n",
      " 2   City    5 non-null      object\n",
      " 3   Salary  5 non-null      int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 292.0+ bytes\n",
      "\n",
      "   Statistical description:\n",
      "             Age       Salary\n",
      "count   5.000000      5.00000\n",
      "mean   30.000000  80000.00000\n",
      "std     3.807887   7905.69415\n",
      "min    25.000000  70000.00000\n",
      "25%    28.000000  75000.00000\n",
      "50%    30.000000  80000.00000\n",
      "75%    32.000000  85000.00000\n",
      "max    35.000000  90000.00000\n",
      "\n",
      "7. Accessing DataFrame data:\n",
      "   Single column 'Name' (Series):\n",
      "0      Alice\n",
      "1        Bob\n",
      "2    Charlie\n",
      "3      Diana\n",
      "4        Eve\n",
      "Name: Name, dtype: object\n",
      "   Type: <class 'pandas.core.series.Series'>\n",
      "\n",
      "   Multiple columns ['Name', 'Age']:\n",
      "      Name  Age\n",
      "0    Alice   25\n",
      "1      Bob   30\n",
      "2  Charlie   35\n",
      "3    Diana   28\n",
      "4      Eve   32\n",
      "   Type: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "   First row:\n",
      "Name         Alice\n",
      "Age             25\n",
      "City      New York\n",
      "Salary       70000\n",
      "Name: 0, dtype: object\n",
      "\n",
      "   First 3 rows, columns 'Name' and 'Salary':\n",
      "      Name  Salary\n",
      "0    Alice   70000\n",
      "1      Bob   80000\n",
      "2  Charlie   90000\n",
      "\n",
      "8. DataFrame with custom index:\n",
      "   DataFrame with custom index:\n",
      "           Name  Age      City  Salary\n",
      "emp001    Alice   25  New York   70000\n",
      "emp002      Bob   30    London   80000\n",
      "emp003  Charlie   35     Tokyo   90000\n",
      "emp004    Diana   28     Paris   75000\n",
      "emp005      Eve   32    Sydney   85000\n",
      "\n",
      "9. DataFrame with missing data:\n",
      "   DataFrame with NaN values:\n",
      "     A    B    C\n",
      "0  1.0  NaN  1.0\n",
      "1  2.0  2.0  2.0\n",
      "2  NaN  3.0  3.0\n",
      "3  4.0  4.0  NaN\n",
      "4  5.0  NaN  5.0\n",
      "\n",
      "   Check for null values:\n",
      "       A      B      C\n",
      "0  False   True  False\n",
      "1  False  False  False\n",
      "2   True  False  False\n",
      "3  False  False   True\n",
      "4  False   True  False\n",
      "\n",
      "   Count of null values per column:\n",
      "A    1\n",
      "B    2\n",
      "C    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Pandas data structures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Pandas Data Structures:\")\n",
    "print(\"=\" * 23)\n",
    "\n",
    "# Series - 1D labeled array\n",
    "print(\"1. Pandas Series:\")\n",
    "\n",
    "# Create Series from list\n",
    "series_from_list = pd.Series([1, 2, 3, 4, 5])\n",
    "print(f\"   Series from list:\")\n",
    "print(f\"{series_from_list}\")\n",
    "print(f\"   Data type: {type(series_from_list)}\")\n",
    "\n",
    "# Series with custom index\n",
    "series_custom_index = pd.Series([10, 20, 30, 40], index=['a', 'b', 'c', 'd'])\n",
    "print(f\"\\n   Series with custom index:\")\n",
    "print(f\"{series_custom_index}\")\n",
    "\n",
    "# Series from dictionary\n",
    "series_from_dict = pd.Series({'apple': 5, 'banana': 3, 'orange': 8, 'grape': 2})\n",
    "print(f\"\\n   Series from dictionary:\")\n",
    "print(f\"{series_from_dict}\")\n",
    "\n",
    "# Series attributes\n",
    "print(f\"\\n2. Series attributes:\")\n",
    "print(f\"   Values: {series_from_dict.values}\")\n",
    "print(f\"   Index: {series_from_dict.index.tolist()}\")\n",
    "print(f\"   Shape: {series_from_dict.shape}\")\n",
    "print(f\"   Size: {series_from_dict.size}\")\n",
    "print(f\"   Data type: {series_from_dict.dtype}\")\n",
    "\n",
    "# Series operations\n",
    "print(f\"\\n3. Series operations:\")\n",
    "print(f\"   Original: {series_from_dict.tolist()}\")\n",
    "print(f\"   + 2: {(series_from_dict + 2).tolist()}\")\n",
    "print(f\"   * 3: {(series_from_dict * 3).tolist()}\")\n",
    "print(f\"   > 5: {(series_from_dict > 5).tolist()}\")\n",
    "\n",
    "# DataFrame - 2D labeled data structure\n",
    "print(f\"\\n4. Pandas DataFrame:\")\n",
    "\n",
    "# Create DataFrame from dictionary\n",
    "data_dict = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'City': ['New York', 'London', 'Tokyo', 'Paris', 'Sydney'],\n",
    "    'Salary': [70000, 80000, 90000, 75000, 85000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "print(f\"   DataFrame from dictionary:\")\n",
    "print(f\"{df}\")\n",
    "\n",
    "# DataFrame from list of lists\n",
    "data_lists = [\n",
    "    ['Alice', 25, 'New York', 70000],\n",
    "    ['Bob', 30, 'London', 80000],\n",
    "    ['Charlie', 35, 'Tokyo', 90000]\n",
    "]\n",
    "\n",
    "df_from_lists = pd.DataFrame(data_lists, \n",
    "                            columns=['Name', 'Age', 'City', 'Salary'])\n",
    "print(f\"\\n   DataFrame from list of lists:\")\n",
    "print(f\"{df_from_lists}\")\n",
    "\n",
    "# DataFrame attributes\n",
    "print(f\"\\n5. DataFrame attributes:\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Size: {df.size}\")\n",
    "print(f\"   Columns: {df.columns.tolist()}\")\n",
    "print(f\"   Index: {df.index.tolist()}\")\n",
    "print(f\"   Data types:\")\n",
    "print(f\"{df.dtypes}\")\n",
    "\n",
    "# DataFrame info\n",
    "print(f\"\\n6. DataFrame info:\")\n",
    "print(f\"   Info summary:\")\n",
    "df.info()\n",
    "\n",
    "print(f\"\\n   Statistical description:\")\n",
    "print(f\"{df.describe()}\")\n",
    "\n",
    "# Accessing DataFrame data\n",
    "print(f\"\\n7. Accessing DataFrame data:\")\n",
    "\n",
    "# Select single column (returns Series)\n",
    "print(f\"   Single column 'Name' (Series):\")\n",
    "print(f\"{df['Name']}\")\n",
    "print(f\"   Type: {type(df['Name'])}\")\n",
    "\n",
    "# Select multiple columns (returns DataFrame)\n",
    "print(f\"\\n   Multiple columns ['Name', 'Age']:\")\n",
    "print(f\"{df[['Name', 'Age']]}\")\n",
    "print(f\"   Type: {type(df[['Name', 'Age']])}\")\n",
    "\n",
    "# Select rows by index\n",
    "print(f\"\\n   First row:\")\n",
    "print(f\"{df.iloc[0]}\")\n",
    "\n",
    "# Select specific rows and columns\n",
    "print(f\"\\n   First 3 rows, columns 'Name' and 'Salary':\")\n",
    "print(f\"{df.loc[0:2, ['Name', 'Salary']]}\")\n",
    "\n",
    "# Creating DataFrame with custom index\n",
    "print(f\"\\n8. DataFrame with custom index:\")\n",
    "df_custom_index = pd.DataFrame(data_dict, \n",
    "                              index=['emp001', 'emp002', 'emp003', 'emp004', 'emp005'])\n",
    "print(f\"   DataFrame with custom index:\")\n",
    "print(f\"{df_custom_index}\")\n",
    "\n",
    "# Working with missing data\n",
    "print(f\"\\n9. DataFrame with missing data:\")\n",
    "data_with_nan = {\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 2, 3, 4, np.nan],\n",
    "    'C': [1, 2, 3, np.nan, 5]\n",
    "}\n",
    "\n",
    "df_nan = pd.DataFrame(data_with_nan)\n",
    "print(f\"   DataFrame with NaN values:\")\n",
    "print(f\"{df_nan}\")\n",
    "print(f\"\\n   Check for null values:\")\n",
    "print(f\"{df_nan.isnull()}\")\n",
    "print(f\"\\n   Count of null values per column:\")\n",
    "print(f\"{df_nan.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Basic Operations\n",
    "\n",
    "Reading data from various sources and basic operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading and Basic Operations:\n",
      "==================================\n",
      "1. Loading data from CSV:\n",
      "   Loaded DataFrame shape: (10, 5)\n",
      "      Name  Age      City  Salary   Department\n",
      "0    Alice   25  New York   70000  Engineering\n",
      "1      Bob   30    London   80000    Marketing\n",
      "2  Charlie   35     Tokyo   90000  Engineering\n",
      "3    Diana   28     Paris   75000           HR\n",
      "4      Eve   32    Sydney   85000    Marketing\n",
      "\n",
      "2. Basic DataFrame inspection:\n",
      "   First 3 rows:\n",
      "      Name  Age      City  Salary   Department\n",
      "0    Alice   25  New York   70000  Engineering\n",
      "1      Bob   30    London   80000    Marketing\n",
      "2  Charlie   35     Tokyo   90000  Engineering\n",
      "\n",
      "   Last 3 rows:\n",
      "    Name  Age       City  Salary Department\n",
      "7  Henry   27       Rome   72000         HR\n",
      "8    Ivy   33  Amsterdam   88000    Finance\n",
      "9   Jack   26     Dublin   71000  Marketing\n",
      "\n",
      "   Random sample of 3 rows:\n",
      "    Name  Age      City  Salary   Department\n",
      "7  Henry   27      Rome   72000           HR\n",
      "1    Bob   30    London   80000    Marketing\n",
      "0  Alice   25  New York   70000  Engineering\n",
      "\n",
      "3. Column operations:\n",
      "   Column names: ['Name', 'Age', 'City', 'Salary', 'Department']\n",
      "   Data types:\n",
      "     Name: object\n",
      "     Age: int64\n",
      "     City: object\n",
      "     Salary: int64\n",
      "     Department: object\n",
      "\n",
      "   Added columns 'Experience_Years' and 'Bonus'\n",
      "      Name  Salary  Experience_Years   Bonus\n",
      "0    Alice   70000                 9  7000.0\n",
      "1      Bob   80000                 5  8000.0\n",
      "2  Charlie   90000                 8  9000.0\n",
      "3    Diana   75000                 1  7500.0\n",
      "4      Eve   85000                 9  8500.0\n",
      "\n",
      "   After dropping 'Bonus' column:\n",
      "   Columns: ['Name', 'Age', 'City', 'Salary', 'Department', 'Experience_Years']\n",
      "\n",
      "   After renaming columns:\n",
      "   New columns: ['Employee_Name', 'Employee_Age', 'City', 'Salary', 'Department', 'Experience_Years', 'Bonus']\n",
      "\n",
      "4. Row operations:\n",
      "   After adding new row, shape: (11, 7)\n",
      "    Name  Age    City  Salary   Department  Experience_Years   Bonus\n",
      "9   Jack   26  Dublin   71000    Marketing                 8  7100.0\n",
      "10  Kate   29  Vienna   79000  Engineering                 4  7900.0\n",
      "\n",
      "   After dropping first 2 rows, shape: (8, 7)\n",
      "\n",
      "5. Sorting data:\n",
      "   Sorted by Age (ascending):\n",
      "    Name  Age  Salary\n",
      "0  Alice   25   70000\n",
      "9   Jack   26   71000\n",
      "7  Henry   27   72000\n",
      "3  Diana   28   75000\n",
      "5  Frank   29   78000\n",
      "\n",
      "   Sorted by Department (asc), then Salary (desc):\n",
      "      Name   Department  Salary\n",
      "2  Charlie  Engineering   90000\n",
      "5    Frank  Engineering   78000\n",
      "0    Alice  Engineering   70000\n",
      "8      Ivy      Finance   88000\n",
      "6    Grace      Finance   82000\n",
      "\n",
      "6. Filtering data:\n",
      "   Engineers only:\n",
      "      Name   Department  Salary\n",
      "0    Alice  Engineering   70000\n",
      "2  Charlie  Engineering   90000\n",
      "5    Frank  Engineering   78000\n",
      "\n",
      "   High earners under 30:\n",
      "    Name  Age  Salary\n",
      "5  Frank   29   78000\n",
      "\n",
      "   Employees in major cities:\n",
      "      Name      City\n",
      "0    Alice  New York\n",
      "1      Bob    London\n",
      "2  Charlie     Tokyo\n",
      "\n",
      "7. String operations:\n",
      "   Cities starting with 'L':\n",
      "  Name    City\n",
      "1  Bob  London\n",
      "\n",
      "   Names containing 'a':\n",
      "      Name\n",
      "0    Alice\n",
      "2  Charlie\n",
      "3    Diana\n",
      "5    Frank\n",
      "6    Grace\n",
      "9     Jack\n",
      "\n",
      "   Name lengths:\n",
      "      Name  Name_Length\n",
      "0    Alice            5\n",
      "1      Bob            3\n",
      "2  Charlie            7\n",
      "3    Diana            5\n",
      "4      Eve            3\n",
      "5    Frank            5\n",
      "6    Grace            5\n",
      "7    Henry            5\n",
      "8      Ivy            3\n",
      "9     Jack            4\n",
      "\n",
      "8. Basic grouping:\n",
      "   Salary statistics by Department:\n",
      "                 mean    min    max  count\n",
      "Department                                \n",
      "Engineering  79333.33  70000  90000      3\n",
      "Finance      85000.00  82000  88000      2\n",
      "HR           73500.00  72000  75000      2\n",
      "Marketing    78666.67  71000  85000      3\n",
      "\n",
      "9. Value counts:\n",
      "   Department distribution:\n",
      "Department\n",
      "Engineering    3\n",
      "Marketing      3\n",
      "HR             2\n",
      "Finance        2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   City distribution:\n",
      "City\n",
      "New York     1\n",
      "London       1\n",
      "Tokyo        1\n",
      "Paris        1\n",
      "Sydney       1\n",
      "Berlin       1\n",
      "Madrid       1\n",
      "Rome         1\n",
      "Amsterdam    1\n",
      "Dublin       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "10. Basic statistics:\n",
      "   Numerical columns summary:\n",
      "             Age        Salary  Experience_Years        Bonus  Name_Length\n",
      "count  10.000000     10.000000         10.000000    10.000000    10.000000\n",
      "mean   29.600000  79100.000000          5.500000  7910.000000     4.500000\n",
      "std     3.204164   7140.650453          2.915476   714.065045     1.269296\n",
      "min    25.000000  70000.000000          1.000000  7000.000000     3.000000\n",
      "25%    27.250000  72750.000000          3.000000  7275.000000     3.250000\n",
      "50%    29.500000  79000.000000          5.500000  7900.000000     5.000000\n",
      "75%    31.750000  84250.000000          8.000000  8425.000000     5.000000\n",
      "max    35.000000  90000.000000          9.000000  9000.000000     7.000000\n",
      "\n",
      "   Correlation matrix:\n",
      "                    Age  Salary  Experience_Years\n",
      "Age               1.000   0.993             0.048\n",
      "Salary            0.993   1.000             0.072\n",
      "Experience_Years  0.048   0.072             1.000\n",
      "\n",
      "11. Data loading formats:\n",
      "   ✓ CSV: pd.read_csv('file.csv')\n",
      "   ✓ Excel: pd.read_excel('file.xlsx')\n",
      "   ✓ JSON: pd.read_json('file.json')\n",
      "   ✓ SQL: pd.read_sql('query', connection)\n",
      "   ✓ HTML: pd.read_html('url')\n",
      "\n",
      "12. Data saving formats:\n",
      "   ✓ CSV: df.to_csv('file.csv', index=False)\n",
      "   ✓ Excel: df.to_excel('file.xlsx', index=False)\n",
      "   ✓ JSON: df.to_json('file.json')\n",
      "   ✓ SQL: df.to_sql('table', connection)\n"
     ]
    }
   ],
   "source": [
    "# Data loading and basic operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "print(\"Data Loading and Basic Operations:\")\n",
    "print(\"=\" * 34)\n",
    "\n",
    "# Create sample CSV data\n",
    "csv_data = \"\"\"\n",
    "Name,Age,City,Salary,Department\n",
    "Alice,25,New York,70000,Engineering\n",
    "Bob,30,London,80000,Marketing\n",
    "Charlie,35,Tokyo,90000,Engineering\n",
    "Diana,28,Paris,75000,HR\n",
    "Eve,32,Sydney,85000,Marketing\n",
    "Frank,29,Berlin,78000,Engineering\n",
    "Grace,31,Madrid,82000,Finance\n",
    "Henry,27,Rome,72000,HR\n",
    "Ivy,33,Amsterdam,88000,Finance\n",
    "Jack,26,Dublin,71000,Marketing\n",
    "\"\"\"\n",
    "\n",
    "# Simulate reading from CSV\n",
    "print(\"1. Loading data from CSV:\")\n",
    "df = pd.read_csv(StringIO(csv_data.strip()))\n",
    "print(f\"   Loaded DataFrame shape: {df.shape}\")\n",
    "print(f\"{df.head()}\")\n",
    "\n",
    "# Basic DataFrame inspection\n",
    "print(f\"\\n2. Basic DataFrame inspection:\")\n",
    "print(f\"   First 3 rows:\")\n",
    "print(f\"{df.head(3)}\")\n",
    "print(f\"\\n   Last 3 rows:\")\n",
    "print(f\"{df.tail(3)}\")\n",
    "print(f\"\\n   Random sample of 3 rows:\")\n",
    "print(f\"{df.sample(3)}\")\n",
    "\n",
    "# Column operations\n",
    "print(f\"\\n3. Column operations:\")\n",
    "print(f\"   Column names: {df.columns.tolist()}\")\n",
    "print(f\"   Data types:\")\n",
    "for col, dtype in df.dtypes.items():\n",
    "    print(f\"     {col}: {dtype}\")\n",
    "\n",
    "# Add new column\n",
    "df['Experience_Years'] = np.random.randint(1, 10, len(df))\n",
    "df['Bonus'] = df['Salary'] * 0.1\n",
    "print(f\"\\n   Added columns 'Experience_Years' and 'Bonus'\")\n",
    "print(f\"{df[['Name', 'Salary', 'Experience_Years', 'Bonus']].head()}\")\n",
    "\n",
    "# Drop columns\n",
    "df_dropped = df.drop(['Bonus'], axis=1)\n",
    "print(f\"\\n   After dropping 'Bonus' column:\")\n",
    "print(f\"   Columns: {df_dropped.columns.tolist()}\")\n",
    "\n",
    "# Rename columns\n",
    "df_renamed = df.rename(columns={'Name': 'Employee_Name', 'Age': 'Employee_Age'})\n",
    "print(f\"\\n   After renaming columns:\")\n",
    "print(f\"   New columns: {df_renamed.columns.tolist()}\")\n",
    "\n",
    "# Row operations\n",
    "print(f\"\\n4. Row operations:\")\n",
    "\n",
    "# Add new row\n",
    "new_row = pd.DataFrame({\n",
    "    'Name': ['Kate'],\n",
    "    'Age': [29],\n",
    "    'City': ['Vienna'],\n",
    "    'Salary': [79000],\n",
    "    'Department': ['Engineering'],\n",
    "    'Experience_Years': [4],\n",
    "    'Bonus': [7900]\n",
    "})\n",
    "\n",
    "df_with_new = pd.concat([df, new_row], ignore_index=True)\n",
    "print(f\"   After adding new row, shape: {df_with_new.shape}\")\n",
    "print(f\"{df_with_new.tail(2)}\")\n",
    "\n",
    "# Drop rows\n",
    "df_dropped_rows = df.drop([0, 1])  # Drop first two rows\n",
    "print(f\"\\n   After dropping first 2 rows, shape: {df_dropped_rows.shape}\")\n",
    "\n",
    "# Sorting data\n",
    "print(f\"\\n5. Sorting data:\")\n",
    "\n",
    "# Sort by single column\n",
    "df_sorted_age = df.sort_values('Age')\n",
    "print(f\"   Sorted by Age (ascending):\")\n",
    "print(f\"{df_sorted_age[['Name', 'Age', 'Salary']].head()}\")\n",
    "\n",
    "# Sort by multiple columns\n",
    "df_sorted_multi = df.sort_values(['Department', 'Salary'], ascending=[True, False])\n",
    "print(f\"\\n   Sorted by Department (asc), then Salary (desc):\")\n",
    "print(f\"{df_sorted_multi[['Name', 'Department', 'Salary']].head()}\")\n",
    "\n",
    "# Filtering data\n",
    "print(f\"\\n6. Filtering data:\")\n",
    "\n",
    "# Single condition\n",
    "engineers = df[df['Department'] == 'Engineering']\n",
    "print(f\"   Engineers only:\")\n",
    "print(f\"{engineers[['Name', 'Department', 'Salary']]}\")\n",
    "\n",
    "# Multiple conditions\n",
    "high_earners = df[(df['Salary'] > 75000) & (df['Age'] < 30)]\n",
    "print(f\"\\n   High earners under 30:\")\n",
    "print(f\"{high_earners[['Name', 'Age', 'Salary']]}\")\n",
    "\n",
    "# Filter with isin()\n",
    "selected_cities = df[df['City'].isin(['New York', 'London', 'Tokyo'])]\n",
    "print(f\"\\n   Employees in major cities:\")\n",
    "print(f\"{selected_cities[['Name', 'City']]}\")\n",
    "\n",
    "# String operations\n",
    "print(f\"\\n7. String operations:\")\n",
    "\n",
    "# String methods\n",
    "print(f\"   Cities starting with 'L':\")\n",
    "cities_with_l = df[df['City'].str.startswith('L')]\n",
    "print(f\"{cities_with_l[['Name', 'City']]}\")\n",
    "\n",
    "# String contains\n",
    "print(f\"\\n   Names containing 'a':\")\n",
    "names_with_a = df[df['Name'].str.contains('a', case=False)]\n",
    "print(f\"{names_with_a[['Name']]}\")\n",
    "\n",
    "# String length\n",
    "df['Name_Length'] = df['Name'].str.len()\n",
    "print(f\"\\n   Name lengths:\")\n",
    "print(f\"{df[['Name', 'Name_Length']]}\")\n",
    "\n",
    "# Grouping preview\n",
    "print(f\"\\n8. Basic grouping:\")\n",
    "\n",
    "# Group by single column\n",
    "dept_stats = df.groupby('Department')['Salary'].agg(['mean', 'min', 'max', 'count'])\n",
    "print(f\"   Salary statistics by Department:\")\n",
    "print(f\"{dept_stats.round(2)}\")\n",
    "\n",
    "# Value counts\n",
    "print(f\"\\n9. Value counts:\")\n",
    "print(f\"   Department distribution:\")\n",
    "print(f\"{df['Department'].value_counts()}\")\n",
    "\n",
    "print(f\"\\n   City distribution:\")\n",
    "print(f\"{df['City'].value_counts()}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\n10. Basic statistics:\")\n",
    "print(f\"   Numerical columns summary:\")\n",
    "print(f\"{df.describe()}\")\n",
    "\n",
    "# Correlation\n",
    "print(f\"\\n   Correlation matrix:\")\n",
    "corr_matrix = df[['Age', 'Salary', 'Experience_Years']].corr()\n",
    "print(f\"{corr_matrix.round(3)}\")\n",
    "\n",
    "print(f\"\\n11. Data loading formats:\")\n",
    "print(f\"   ✓ CSV: pd.read_csv('file.csv')\")\n",
    "print(f\"   ✓ Excel: pd.read_excel('file.xlsx')\")\n",
    "print(f\"   ✓ JSON: pd.read_json('file.json')\")\n",
    "print(f\"   ✓ SQL: pd.read_sql('query', connection)\")\n",
    "print(f\"   ✓ HTML: pd.read_html('url')\")\n",
    "\n",
    "print(f\"\\n12. Data saving formats:\")\n",
    "print(f\"   ✓ CSV: df.to_csv('file.csv', index=False)\")\n",
    "print(f\"   ✓ Excel: df.to_excel('file.xlsx', index=False)\")\n",
    "print(f\"   ✓ JSON: df.to_json('file.json')\")\n",
    "print(f\"   ✓ SQL: df.to_sql('table', connection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Transformation\n",
    "\n",
    "Handling missing data and transforming datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaning and Transformation:\n",
      "=================================\n",
      "1. Original messy dataset:\n",
      "   ID           Name   Age                Email   Salary     Join_Date  \\\n",
      "0   1  Alice Johnson  25.0      alice@email.com  70000.0    2021-01-15   \n",
      "1   2     bob smith   30.0        BOB@EMAIL.COM  80000.0    2020-03-22   \n",
      "2   3  CHARLIE BROWN   NaN    charlie@email.com  90000.0    2019-07-10   \n",
      "3   4    diana davis  28.0        invalid-email      NaN    2022-01-05   \n",
      "4   5     Eve Wilson  32.0        eve@email.com  85000.0  invalid-date   \n",
      "5   6           None  45.0      frank@email.com  95000.0    2020-11-18   \n",
      "6   7   frank miller  29.0                 None  78000.0    2021-06-30   \n",
      "7   8      Grace LEE   NaN      grace@email.com  82000.0    2019-12-01   \n",
      "8   9    henry CLARK  27.0      henry@email.com      NaN    2022-03-15   \n",
      "9  10                 35.0  duplicate@email.com  88000.0    2020-08-25   \n",
      "\n",
      "    Department  \n",
      "0  Engineering  \n",
      "1    marketing  \n",
      "2  ENGINEERING  \n",
      "3           hr  \n",
      "4    Marketing  \n",
      "5  engineering  \n",
      "6           HR  \n",
      "7    Marketing  \n",
      "8  Engineering  \n",
      "9           HR  \n",
      "\n",
      "2. Data quality assessment:\n",
      "   Dataset shape: (10, 7)\n",
      "   Missing values per column:\n",
      "ID            0\n",
      "Name          1\n",
      "Age           2\n",
      "Email         1\n",
      "Salary        2\n",
      "Join_Date     0\n",
      "Department    0\n",
      "dtype: int64\n",
      "\n",
      "   Data types:\n",
      "ID              int64\n",
      "Name           object\n",
      "Age           float64\n",
      "Email          object\n",
      "Salary        float64\n",
      "Join_Date      object\n",
      "Department     object\n",
      "dtype: object\n",
      "\n",
      "3. Handling missing values:\n",
      "   Empty strings count: 1\n",
      "   None values count: 1\n",
      "\n",
      "   After replacing empty strings with NaN:\n",
      "ID            0\n",
      "Name          2\n",
      "Age           2\n",
      "Email         1\n",
      "Salary        2\n",
      "Join_Date     0\n",
      "Department    0\n",
      "dtype: int64\n",
      "\n",
      "4. Removing rows with excessive missing data:\n",
      "   Shape after removing rows with >4 missing values: (10, 7)\n",
      "\n",
      "5. Filling missing values:\n",
      "   Filled missing Age with median: 29.5\n",
      "   Filling Salary with department mean:\n",
      "   Missing values after filling:\n",
      "ID            0\n",
      "Name          2\n",
      "Age           0\n",
      "Email         1\n",
      "Salary        0\n",
      "Join_Date     0\n",
      "Department    0\n",
      "dtype: int64\n",
      "\n",
      "6. String cleaning:\n",
      "   Cleaned Names:\n",
      "['Alice Johnson', 'Bob Smith', 'Charlie Brown', 'Diana Davis', 'Eve Wilson', None, 'Frank Miller', 'Grace Lee', 'Henry Clark', nan]\n",
      "   Cleaned Emails:\n",
      "['alice@email.com', 'bob@email.com', 'charlie@email.com', 'invalid-email', 'eve@email.com', 'frank@email.com', None, 'grace@email.com', 'henry@email.com', 'duplicate@email.com']\n",
      "   Standardized Departments:\n",
      "Department\n",
      "Engineering    4\n",
      "Marketing      3\n",
      "HR             3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "7. Data type conversion:\n",
      "   Converting Join_Date to datetime:\n",
      "   Join_Date type: datetime64[ns]\n",
      "   Invalid dates (NaT): 1\n",
      "   Final data types:\n",
      "ID                     int64\n",
      "Name                  object\n",
      "Age                    int32\n",
      "Email                 object\n",
      "Salary                 int32\n",
      "Join_Date     datetime64[ns]\n",
      "Department            object\n",
      "dtype: object\n",
      "\n",
      "8. Handling duplicates:\n",
      "   Original shape: (10, 7)\n",
      "   Duplicate emails: 0\n",
      "   Shape after removing duplicate emails: (10, 7)\n",
      "\n",
      "9. Data validation:\n",
      "   Valid email addresses: 8/10\n",
      "   Invalid emails:\n",
      "          Name          Email\n",
      "3  Diana Davis  invalid-email\n",
      "   Valid ages (18-65): 10/10\n",
      "   Valid salaries (30k-200k): 10/10\n",
      "\n",
      "10. Feature engineering:\n",
      "   New features created:\n",
      "            Name  Age Age_Category  Salary Salary_Band  Years_Since_Join\n",
      "0  Alice Johnson   25   Mid-Career   70000         Low               4.5\n",
      "1      Bob Smith   30   Mid-Career   80000      Medium               5.4\n",
      "2  Charlie Brown   29   Mid-Career   90000        High               6.1\n",
      "3    Diana Davis   28   Mid-Career   90000        High               3.6\n",
      "4     Eve Wilson   32   Mid-Career   85000      Medium               4.6\n",
      "5           None   45       Senior   95000        High               4.7\n",
      "6   Frank Miller   29   Mid-Career   78000      Medium               4.1\n",
      "7      Grace Lee   29   Mid-Career   82000      Medium               5.7\n",
      "8    Henry Clark   27   Mid-Career   70000         Low               3.4\n",
      "9            NaN   35       Senior   88000        High               4.9\n",
      "\n",
      "11. Outlier detection:\n",
      "   Salary outliers (IQR method): 0\n",
      "   Outlier bounds: $62000 - $106000\n",
      "\n",
      "12. Final cleaned dataset:\n",
      "   Shape: (10, 10)\n",
      "   Missing values: 3\n",
      "\n",
      "   Sample of cleaned data:\n",
      "   ID           Name  Age              Email  Salary  Join_Date   Department  \\\n",
      "0   1  Alice Johnson   25    alice@email.com   70000 2021-01-15  Engineering   \n",
      "1   2      Bob Smith   30      bob@email.com   80000 2020-03-22    Marketing   \n",
      "2   3  Charlie Brown   29  charlie@email.com   90000 2019-07-10  Engineering   \n",
      "3   4    Diana Davis   28      invalid-email   90000 2022-01-05           HR   \n",
      "4   5     Eve Wilson   32      eve@email.com   85000 2021-01-01    Marketing   \n",
      "\n",
      "   Years_Since_Join Age_Category Salary_Band  \n",
      "0               4.5   Mid-Career         Low  \n",
      "1               5.4   Mid-Career      Medium  \n",
      "2               6.1   Mid-Career        High  \n",
      "3               3.6   Mid-Career        High  \n",
      "4               4.6   Mid-Career      Medium  \n",
      "\n",
      "13. Data cleaning checklist:\n",
      "   ✓ Handled missing values (fill, drop, impute)\n",
      "   ✓ Standardized string formats (case, whitespace)\n",
      "   ✓ Converted data types appropriately\n",
      "   ✓ Removed duplicates\n",
      "   ✓ Validated data ranges and formats\n",
      "   ✓ Created derived features\n",
      "   ✓ Detected outliers\n",
      "   ✓ Documented cleaning process\n",
      "\n",
      "14. Exporting cleaned data:\n",
      "   # Save cleaned dataset\n",
      "   df_cleaned.to_csv('cleaned_data.csv', index=False)\n",
      "   # Create data dictionary\n",
      "   data_dict = df_cleaned.dtypes.to_dict()\n",
      "   print('Data Dictionary:', data_dict)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vansh\\AppData\\Local\\Temp\\ipykernel_24932\\2971994611.py:70: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_cleaned['Salary'] = df_cleaned['Salary'].fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning and transformation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"Data Cleaning and Transformation:\")\n",
    "print(\"=\" * 33)\n",
    "\n",
    "# Create messy dataset for cleaning\n",
    "np.random.seed(42)\n",
    "messy_data = {\n",
    "    'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Name': ['Alice Johnson', ' bob smith ', 'CHARLIE BROWN', 'diana davis', 'Eve Wilson', None, 'frank miller', 'Grace LEE', 'henry CLARK', ''],\n",
    "    'Age': [25, 30, np.nan, 28, 32, 45, 29, np.nan, 27, 35],\n",
    "    'Email': ['alice@email.com', 'BOB@EMAIL.COM', 'charlie@email.com', 'invalid-email', 'eve@email.com', 'frank@email.com', None, 'grace@email.com', 'henry@email.com', 'duplicate@email.com'],\n",
    "    'Salary': [70000, 80000, 90000, np.nan, 85000, 95000, 78000, 82000, np.nan, 88000],\n",
    "    'Join_Date': ['2021-01-15', '2020-03-22', '2019-07-10', '2022-01-05', 'invalid-date', '2020-11-18', '2021-06-30', '2019-12-01', '2022-03-15', '2020-08-25'],\n",
    "    'Department': ['Engineering', 'marketing', 'ENGINEERING', 'hr', 'Marketing', 'engineering', 'HR', 'Marketing', 'Engineering', 'HR']\n",
    "}\n",
    "\n",
    "df_messy = pd.DataFrame(messy_data)\n",
    "print(\"1. Original messy dataset:\")\n",
    "print(f\"{df_messy}\")\n",
    "\n",
    "# Data quality assessment\n",
    "print(f\"\\n2. Data quality assessment:\")\n",
    "print(f\"   Dataset shape: {df_messy.shape}\")\n",
    "print(f\"   Missing values per column:\")\n",
    "print(f\"{df_messy.isnull().sum()}\")\n",
    "print(f\"\\n   Data types:\")\n",
    "print(f\"{df_messy.dtypes}\")\n",
    "\n",
    "# Handling missing values\n",
    "print(f\"\\n3. Handling missing values:\")\n",
    "\n",
    "# Check for different types of missing data\n",
    "print(f\"   Empty strings count: {(df_messy['Name'] == '').sum()}\")\n",
    "print(f\"   None values count: {df_messy['Name'].isnull().sum()}\")\n",
    "\n",
    "# Replace empty strings with NaN\n",
    "df_cleaned = df_messy.copy()\n",
    "df_cleaned = df_cleaned.replace('', np.nan)\n",
    "print(f\"\\n   After replacing empty strings with NaN:\")\n",
    "print(f\"{df_cleaned.isnull().sum()}\")\n",
    "\n",
    "# Drop rows with too many missing values\n",
    "print(f\"\\n4. Removing rows with excessive missing data:\")\n",
    "# Keep rows with at least 6 non-null values\n",
    "df_cleaned = df_cleaned.dropna(thresh=6)\n",
    "print(f\"   Shape after removing rows with >4 missing values: {df_cleaned.shape}\")\n",
    "\n",
    "# Fill missing values\n",
    "print(f\"\\n5. Filling missing values:\")\n",
    "\n",
    "# Fill Age with median\n",
    "median_age = df_cleaned['Age'].median()\n",
    "df_cleaned['Age'] = df_cleaned['Age'].fillna(median_age)\n",
    "print(f\"   Filled missing Age with median: {median_age}\")\n",
    "\n",
    "# Fill Salary with mean by Department\n",
    "print(f\"   Filling Salary with department mean:\")\n",
    "for dept in df_cleaned['Department'].unique():\n",
    "    if pd.isna(dept):\n",
    "        continue\n",
    "    dept_mask = df_cleaned['Department'] == dept\n",
    "    dept_mean_salary = df_cleaned.loc[dept_mask, 'Salary'].mean()\n",
    "    df_cleaned.loc[dept_mask, 'Salary'] = df_cleaned.loc[dept_mask, 'Salary'].fillna(dept_mean_salary)\n",
    "\n",
    "# Forward fill for remaining missing values\n",
    "df_cleaned['Salary'] = df_cleaned['Salary'].fillna(method='ffill')\n",
    "\n",
    "print(f\"   Missing values after filling:\")\n",
    "print(f\"{df_cleaned.isnull().sum()}\")\n",
    "\n",
    "# String cleaning\n",
    "print(f\"\\n6. String cleaning:\")\n",
    "\n",
    "# Clean Name column\n",
    "df_cleaned['Name'] = df_cleaned['Name'].str.strip()  # Remove whitespace\n",
    "df_cleaned['Name'] = df_cleaned['Name'].str.title()  # Title case\n",
    "print(f\"   Cleaned Names:\")\n",
    "print(f\"{df_cleaned['Name'].tolist()}\")\n",
    "\n",
    "# Clean Email column\n",
    "df_cleaned['Email'] = df_cleaned['Email'].str.lower()  # Lowercase emails\n",
    "print(f\"   Cleaned Emails:\")\n",
    "print(f\"{df_cleaned['Email'].tolist()}\")\n",
    "\n",
    "# Standardize Department names\n",
    "dept_mapping = {\n",
    "    'engineering': 'Engineering',\n",
    "    'marketing': 'Marketing', \n",
    "    'hr': 'HR'\n",
    "}\n",
    "df_cleaned['Department'] = df_cleaned['Department'].str.lower().map(dept_mapping)\n",
    "print(f\"   Standardized Departments:\")\n",
    "print(f\"{df_cleaned['Department'].value_counts()}\")\n",
    "\n",
    "# Data type conversion\n",
    "print(f\"\\n7. Data type conversion:\")\n",
    "\n",
    "# Convert Join_Date to datetime\n",
    "print(f\"   Converting Join_Date to datetime:\")\n",
    "df_cleaned['Join_Date'] = pd.to_datetime(df_cleaned['Join_Date'], errors='coerce')\n",
    "print(f\"   Join_Date type: {df_cleaned['Join_Date'].dtype}\")\n",
    "print(f\"   Invalid dates (NaT): {df_cleaned['Join_Date'].isnull().sum()}\")\n",
    "\n",
    "# Fill invalid dates with a reasonable default\n",
    "default_date = pd.to_datetime('2021-01-01')\n",
    "df_cleaned['Join_Date'] = df_cleaned['Join_Date'].fillna(default_date)\n",
    "\n",
    "# Convert numeric columns\n",
    "df_cleaned['Age'] = df_cleaned['Age'].astype(int)\n",
    "df_cleaned['Salary'] = df_cleaned['Salary'].astype(int)\n",
    "\n",
    "print(f\"   Final data types:\")\n",
    "print(f\"{df_cleaned.dtypes}\")\n",
    "\n",
    "# Remove duplicates\n",
    "print(f\"\\n8. Handling duplicates:\")\n",
    "print(f\"   Original shape: {df_cleaned.shape}\")\n",
    "\n",
    "# Check for duplicate emails\n",
    "duplicate_emails = df_cleaned['Email'].duplicated().sum()\n",
    "print(f\"   Duplicate emails: {duplicate_emails}\")\n",
    "\n",
    "# Remove rows with duplicate emails (keep first)\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['Email'], keep='first')\n",
    "print(f\"   Shape after removing duplicate emails: {df_cleaned.shape}\")\n",
    "\n",
    "# Data validation\n",
    "print(f\"\\n9. Data validation:\")\n",
    "\n",
    "# Validate email format\n",
    "email_pattern = r'^[\\w\\.-]+@[\\w\\.-]+\\.[\\w]+$'\n",
    "valid_emails = df_cleaned['Email'].str.match(email_pattern, na=False)\n",
    "print(f\"   Valid email addresses: {valid_emails.sum()}/{len(df_cleaned)}\")\n",
    "print(f\"   Invalid emails:\")\n",
    "invalid_email_mask = ~valid_emails & df_cleaned['Email'].notna()\n",
    "if invalid_email_mask.any():\n",
    "    print(f\"{df_cleaned.loc[invalid_email_mask, ['Name', 'Email']]}\")\n",
    "\n",
    "# Validate age range\n",
    "valid_ages = (df_cleaned['Age'] >= 18) & (df_cleaned['Age'] <= 65)\n",
    "print(f\"   Valid ages (18-65): {valid_ages.sum()}/{len(df_cleaned)}\")\n",
    "\n",
    "# Validate salary range\n",
    "valid_salaries = (df_cleaned['Salary'] >= 30000) & (df_cleaned['Salary'] <= 200000)\n",
    "print(f\"   Valid salaries (30k-200k): {valid_salaries.sum()}/{len(df_cleaned)}\")\n",
    "\n",
    "# Feature engineering\n",
    "print(f\"\\n10. Feature engineering:\")\n",
    "\n",
    "# Calculate years since joining\n",
    "current_date = datetime.now()\n",
    "df_cleaned['Years_Since_Join'] = (current_date - df_cleaned['Join_Date']).dt.days / 365.25\n",
    "df_cleaned['Years_Since_Join'] = df_cleaned['Years_Since_Join'].round(1)\n",
    "\n",
    "# Create age categories\n",
    "def categorize_age(age):\n",
    "    if age < 25:\n",
    "        return 'Young'\n",
    "    elif age < 35:\n",
    "        return 'Mid-Career'\n",
    "    else:\n",
    "        return 'Senior'\n",
    "\n",
    "df_cleaned['Age_Category'] = df_cleaned['Age'].apply(categorize_age)\n",
    "\n",
    "# Create salary bands\n",
    "df_cleaned['Salary_Band'] = pd.cut(df_cleaned['Salary'], \n",
    "                                  bins=[0, 75000, 85000, float('inf')], \n",
    "                                  labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(f\"   New features created:\")\n",
    "print(f\"{df_cleaned[['Name', 'Age', 'Age_Category', 'Salary', 'Salary_Band', 'Years_Since_Join']]}\")\n",
    "\n",
    "# Outlier detection\n",
    "print(f\"\\n11. Outlier detection:\")\n",
    "\n",
    "# Using IQR method for salary\n",
    "Q1 = df_cleaned['Salary'].quantile(0.25)\n",
    "Q3 = df_cleaned['Salary'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = (df_cleaned['Salary'] < lower_bound) | (df_cleaned['Salary'] > upper_bound)\n",
    "print(f\"   Salary outliers (IQR method): {outliers.sum()}\")\n",
    "print(f\"   Outlier bounds: ${lower_bound:.0f} - ${upper_bound:.0f}\")\n",
    "\n",
    "if outliers.any():\n",
    "    print(f\"   Outlier records:\")\n",
    "    print(f\"{df_cleaned.loc[outliers, ['Name', 'Salary']]}\")\n",
    "\n",
    "# Final cleaned dataset\n",
    "print(f\"\\n12. Final cleaned dataset:\")\n",
    "print(f\"   Shape: {df_cleaned.shape}\")\n",
    "print(f\"   Missing values: {df_cleaned.isnull().sum().sum()}\")\n",
    "print(f\"\\n   Sample of cleaned data:\")\n",
    "print(f\"{df_cleaned.head()}\")\n",
    "\n",
    "# Data cleaning summary\n",
    "print(f\"\\n13. Data cleaning checklist:\")\n",
    "print(f\"   ✓ Handled missing values (fill, drop, impute)\")\n",
    "print(f\"   ✓ Standardized string formats (case, whitespace)\")\n",
    "print(f\"   ✓ Converted data types appropriately\")\n",
    "print(f\"   ✓ Removed duplicates\")\n",
    "print(f\"   ✓ Validated data ranges and formats\")\n",
    "print(f\"   ✓ Created derived features\")\n",
    "print(f\"   ✓ Detected outliers\")\n",
    "print(f\"   ✓ Documented cleaning process\")\n",
    "\n",
    "# Export cleaned data\n",
    "print(f\"\\n14. Exporting cleaned data:\")\n",
    "print(f\"   # Save cleaned dataset\")\n",
    "print(f\"   df_cleaned.to_csv('cleaned_data.csv', index=False)\")\n",
    "print(f\"   # Create data dictionary\")\n",
    "print(f\"   data_dict = df_cleaned.dtypes.to_dict()\")\n",
    "print(f\"   print('Data Dictionary:', data_dict)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned about:\n",
    "\n",
    "✅ **Pandas Data Structures**: Series and DataFrame for structured data[3][13]  \n",
    "✅ **Data Loading**: Reading from CSV, Excel, JSON and other formats[3]  \n",
    "✅ **Data Selection**: Indexing, filtering, and querying data effectively  \n",
    "✅ **Data Cleaning**: Handling missing values, duplicates, and data validation  \n",
    "✅ **Data Transformation**: Type conversion, feature engineering, and standardization  \n",
    "✅ **Basic Operations**: Sorting, grouping, and statistical summaries[6]  \n",
    "\n",
    "### Key Takeaways:\n",
    "1. Pandas is built on NumPy and provides high-level data structures[7][13]\n",
    "2. DataFrame is like a spreadsheet or SQL table with labeled rows and columns\n",
    "3. Always inspect data quality before analysis (missing values, duplicates, outliers)\n",
    "4. String methods (.str) provide powerful text processing capabilities\n",
    "5. Proper data cleaning is crucial for accurate analysis results[6]\n",
    "6. Pandas integrates seamlessly with NumPy and Matplotlib[7]\n",
    "\n",
    "### Next Topic: 27_matplotlib_basics.ipynb\n",
    "Learn about data visualization with Matplotlib for creating plots and charts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
